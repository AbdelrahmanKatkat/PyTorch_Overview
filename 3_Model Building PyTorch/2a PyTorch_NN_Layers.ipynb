{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPJghtsS/SnhkfPi1ks8ZX8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# PyTorch_NN_Layers\n"],"metadata":{"id":"cmnt4YIjz90g"}},{"cell_type":"markdown","source":["In the context of artificial neural networks and deep learning, it is common to categorize layers based on their function and purpose within a neural network architecture. This categorization helps in understanding and designing neural network architectures effectively. However, it's important to note that these categories are not entirely separate from the architecture; they are essential components of the architecture. Let's explore some common types of layers and their relationship to architecture:\n","\n","1. **Input Layer**: The input layer is responsible for receiving the raw input data and passing it on to the subsequent layers. It doesn't involve any computations and serves as the entry point for data into the neural network. An example of an input layer would be the first layer of a convolutional neural network (CNN) that takes in image data.\n","\n","2. **Hidden Layers**: Hidden layers are the layers between the input and output layers of a neural network. They perform various computations, such as feature extraction and representation learning, and they are crucial for the network's ability to model complex relationships in the data. Examples of hidden layers include convolutional layers, recurrent layers (e.g., LSTM or GRU), and fully connected layers (dense layers).\n","\n","3. **Output Layer**: The output layer produces the final predictions or outputs of the neural network. The type of output layer depends on the problem being solved. For example, in a binary classification problem, a single neuron with a sigmoid activation function is often used as the output layer. For multi-class classification, a softmax layer is common.\n","\n","4. **Normalization Layers**: These layers are used to normalize the activations of the preceding layer. Batch normalization (BatchNorm) and layer normalization are examples. They help stabilize and accelerate training by reducing internal covariate shift.\n","\n","5. **Pooling Layers**: Pooling layers, typically used in CNNs, reduce the spatial dimensions of the data, reducing the computational load and increasing the receptive field. Common pooling layers include max-pooling and average-pooling.\n","\n","6. **Dropout Layers**: Dropout is a regularization technique that involves randomly setting a fraction of the neurons' outputs to zero during each forward and backward pass. It helps prevent overfitting.\n","\n","7. **Skip Connections**: Skip connections or residual connections are used to bypass one or more layers in the network. They are commonly employed in architectures like ResNet to facilitate the training of very deep networks.\n","\n","8. **Embedding Layers**: These layers are often used in natural language processing (NLP) tasks to map categorical or discrete data, like words or tokens, into continuous vector representations. Word embeddings like Word2Vec and GloVe are examples.\n","\n","9. **Attention Layers**: In NLP and sequence-to-sequence tasks, attention mechanisms (e.g., in transformers) are used to focus on relevant parts of the input sequence when producing an output sequence.\n","\n","10. **Recurrent Layers**: Layers like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are used for modeling sequential data, as they have memory to capture dependencies over time.\n","\n"],"metadata":{"id":"C3eUaFr4jWCs"}},{"cell_type":"markdown","source":["**Types of Layers that can be added:**\n","\n","PyTorch provides a rich set of layers and modules to build neural networks. Here are some commonly used ones:\n","  - Linear Layers (`nn.Linear`): Used for fully connected layers in feedforward networks.\n","  - Convolutional Layers (`nn.Conv2d`, `nn.ConvTranspose2d`): For processing grid-like data like images.\n","  - Recurrent Layers (`nn.RNN`, `nn.LSTM`, `nn.GRU`): Suitable for sequence data like text or time series.\n","  - Normalization Layers (`nn.BatchNorm2d`, `nn.LayerNorm`): Normalize activations to stabilize training.\n","  - Activation Functions (`nn.ReLU`, `nn.Sigmoid`, `nn.Tanh`): Introduce non-linearity into the model.\n","  - Dropout Layers (`nn.Dropout`, `nn.Dropout2d`): A regularization technique to prevent overfitting.\n","  - Pooling Layers (`nn.MaxPool2d`, `nn.AvgPool2d`): Used in convolutional networks for downsampling.\n","\n","\n","\n","| Layer Type | Use Case | Application |\n","| --- | --- | --- |\n","| Linear | Mapping input to output through a matrix multiplication | Image classification, regression |\n","| Convolutional | Extracting features from images or other 2D data | Image classification, object detection |\n","| Recurrent | Processing sequences of inputs, preserving state across inputs | Natural language processing, speech recognition |\n","| LSTM | Processing sequences of inputs with long-term dependencies | Natural language processing, speech recognition |\n","| GRU | Processing sequences of inputs with shorter-term dependencies | Natural language processing, speech recognition |\n","| Dropout | Regularizing a neural network by randomly dropping out some units during training | Image classification, regression, natural language processing |\n","| Batch Normalization | Normalizing the activations of a neural network layer to improve training stability and performance | Image classification, object detection |\n","| Embedding | Learning a low-dimensional representation (embedding) of discrete inputs such as words | Natural language processing |\n","| Softmax | Converting a vector of raw scores into a probability distribution over classes | Multi-class classification |\n","| Max Pooling | Downsampling feature maps by taking the maximum value in each pooling region | Image classification, object detection |\n","\n"],"metadata":{"id":"_Rg_w-79Lgks"}},{"cell_type":"markdown","source":["![Image](https://i.postimg.cc/Ss3Hktb3/Untitled-12.png)"],"metadata":{"id":"_PnT8olrm2S5"}},{"cell_type":"markdown","source":["# Hidden Layers"],"metadata":{"id":"kUUoYazjjgiX"}},{"cell_type":"markdown","source":["## Linear Layer"],"metadata":{"id":"qnE0NuoXF10X"}},{"cell_type":"markdown","source":["A Linear Layer, often referred to as a **Fully Connected Layer**, is a fundamental building block in neural networks. It serves as a **versatile data transformation module that connects every neuron (or unit) in the current layer to every neuron in the previous layer**. This layer is essential for learning complex relationships between input features and is used in various deep learning models, including feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\n","\n","Here's an explanation of the Linear Layer:\n","\n","**Purpose:**\n","- The primary purpose of a Linear Layer is to perform **a linear transformation of the input data**. It learns a set of weights and biases during training that determine how the input features are combined to produce the layer's output.\n","- It can model both **linear and non-linear** relationships between features, depending on the **activation function** applied after the linear transformation.\n","\n","**Mathematics:**\n","- Let's denote the **input to the Linear Layer as \\(x\\), which is typically a vector or a mini-batch of vectors**.\n","  - The Linear Layer's parameters consist of **a weight matrix \\(W\\) and a bias vector \\(b\\)**.\n","- The output \\(y\\) of the Linear Layer is computed as follows:\n","  \\[y = Wx + b\\]\n","  - \\(W\\) is a matrix where each row corresponds to a neuron in the current layer, and each column corresponds to a feature from the previous layer.\n","  - \\(b\\) is a vector containing a bias term for each neuron in the current layer.\n","  - \\(x\\) is the input vector.\n","  - The operation \\(Wx\\) represents the linear combination of input features weighted by the learned parameters, and \\(b\\) is added to this result.\n","  \n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define a Linear Layer with input size=64 and output size=32\n","linear_layer = nn.Linear(64, 32)\n","\n","# Example input tensor with 64 features\n","input_data = torch.randn(10, 64)  # Batch size=10, input features=64\n","\n","# Apply the Linear Layer to the input data\n","output = linear_layer(input_data)\n","```\n","\n","In the code example above, we define a Linear Layer with an input size of 64 and an output size of 32. We then apply this layer to an example input tensor with a batch size of 10 and 64 input features. The layer performs the linear transformation, and the resulting output tensor has a size of 10 (batch size) by 32 (output size), as determined by the Linear Layer's parameters.\n","\n"],"metadata":{"id":"PTxTAhsnF-9m"}},{"cell_type":"markdown","source":["## Convolutional Layer"],"metadata":{"id":"VwptiVJAKubW"}},{"cell_type":"markdown","source":["**Convolutional Layer (Conv2d in PyTorch):**\n","\n","**Purpose:**\n","- The primary purpose of a Convolutional Layer, often abbreviated as Conv2d in PyTorch, is to detect **spatial patterns and features in input data, such as images or sequences**.\n","\n","  - Spatial patterns refer to the arrangement or distribution of features or elements in a given space.\n","\n","  - In the context of image processing or computer vision, spatial patterns often refer to visual patterns or structures that can be observed in an image.\n","\n","  - These patterns can include edges, corners, textures, shapes, or any other visual characteristics that can be identified based on the arrangement of pixels or regions in an image.\n","- It is especially effective in capturing local patterns, edges, textures, and hierarchies of features within the data.\n","\n","\n","**Mathematics:**\n","\n","- Let's **denote the input to the Convolutional Layer as x**\n","\n","  - Which is typically a multi-dimensional array (e.g., an image with height, width, and channels).\n","  \n","  - The Convolutional Layer's parameters consist of a set of learnable filters (kernels) and biases.\n","\n","- The **output** $y$ of the Convolutional Layer is **computed through a mathematical operation called convolution**,\n","  - which **involves sliding the filters over the input** and computing element-wise products **followed by summation**.\n","\n","- For a 2D convolution, the operation can be expressed as:\n","\n","  \\begin{equation}\n","    y[i, j, k] = \\sum_{m, n, l} x[i + m, j + n, l] \\cdot W[m, n, l, k] + b[k]\n","  \\end{equation}\n","\n","  - $y$ is the output feature map.\n","  - $x$ is the input data.\n","  - $W$ is the set of learnable filters.\n","  - $b$ is the bias term for each filter.\n","  - $i$ and $j$ iterate over spatial dimensions.\n","  - $l$ and $k$ iterate over input and output channels.\n","\n","**Here's an example to illustrate how convolution works:**\n","\n","Suppose we have an input image with dimensions $3 \\times 3 \\times 1$ (height, width, and channels) and a single filter with dimensions $2 \\times 2 \\times 1$:\n","\n","\\begin{equation}\n","x = \\begin{bmatrix}\n","1 & 2 & 3 \\\\\n","4 & 5 & 6 \\\\\n","7 & 8 & 9\n","\\end{bmatrix}, \\quad\n","W = \\begin{bmatrix}\n","-1 & 1 \\\\\n","1 & -1\n","\\end{bmatrix}\n","\\end{equation}\n","\n","We can apply the convolution operation to obtain a feature map of dimensions $2 \\times 2 \\times 1$:\n","\n","\\begin{equation}\n","y = \\begin{bmatrix}\n","(-1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 4 -1 \\cdot 5) + b \\\\\n","(-1 \\cdot 2 + 1 \\cdot 3 + 1 \\cdot 5 -1 \\cdot 6) + b \\\\\n","(-1 \\cdot 4 + 1 \\cdot 5 + 1 \\cdot 7 -1 \\cdot 8) + b \\\\\n","(-1 \\cdot 5 + 1 \\cdot 6 + 1 \\cdot 8 -1 \\cdot 9) + b\n","\\end{bmatrix} =\n","\\begin{bmatrix}\n","2 + b \\\\\n","0 + b \\\\\n","0 + b \\\\\n","-2 + b\n","\\end{bmatrix}\n","\\end{equation}\n","\n","where $b$ is the bias term. Note that we didn't specify the value of $b$, which would depend on the specific filter being used.\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define a 2D convolutional layer with input channels=3, output channels=64, and kernel size=3\n","conv_layer = nn.Conv2d(3, 64, kernel_size=3)\n","\n","# Example input tensor with 3 channels (e.g., RGB image) and spatial size (height, width)\n","input_data = torch.randn(10, 3, 32, 32)  # Batch size=10, channels=3, height=32, width=32\n","\n","# Apply the Convolutional Layer to the input data\n","output = conv_layer(input_data)\n","```\n","\n","In the code example above, we define a 2D Convolutional Layer with three input channels (e.g., for an RGB image) and 64 output channels. We apply this layer to an example input tensor with a batch size of 10, three channels, and a spatial size of 32x32. The Convolutional Layer performs convolution operations with learnable filters and produces output feature maps.\n","\n"],"metadata":{"id":"nOyz69x5LKcO"}},{"cell_type":"markdown","source":["### CNN Input Parameters"],"metadata":{"id":"ebNsg2HvC8RD"}},{"cell_type":"markdown","source":["In PyTorch, the `nn.Conv2d` module is used to perform two-dimensional convolution on input tensors. The inputs to this module are as follows:\n","\n","- **`in_channels`**: The number of input channels. For example, in an RGB image, the number of input channels is 3 (corresponding to the red, green, and blue channels).\n","\n","- **`out_channels`**: The number of output channels. This corresponds to the number of filters that will be learned by the convolution layer.\n","  - In the context of convolutional neural networks (CNNs), `out_channels` refers to the number of filters that will be learned by a convolutional layer.\n","\n","\n","- **`kernel_size`**: The size of the kernel (i.e., the filter) to be used in the convolution operation. This can be specified as an integer or a tuple of integers.\n","\n","- **`stride`**: The stride of the convolution operation. This can also be specified as an integer or a tuple of integers.\n","\n","- **`padding`**: The amount of padding to be added to the input tensor. This can be specified as an integer or a tuple of integers.\n","\n","- **`dilation`**: The dilation rate of the convolution operation. This can also be specified as an integer or a tuple of integers.\n","\n","- **`groups`**: The number of groups in which the input channels and output channels are divided. This is typically used to perform grouped convolutions.\n","\n","- **`bias`**: A Boolean value indicating whether or not to include a bias term in the convolution operation.\n","\n","In summary, the inputs to `nn.Conv2d` specify the parameters of the convolution operation, such as the size of the filter, the stride of the convolution, and the number of filters to be learned."],"metadata":{"id":"3nY7HY9X75T5"}},{"cell_type":"markdown","source":["### In_Channels vs Out_Channels"],"metadata":{"id":"SLvYUrTx_Hub"}},{"cell_type":"markdown","source":["The `in_channels` and `out_channels` parameters in a convolutional layer of a neural network are related to the input and output tensors of the layer, respectively.\n","\n","- `in_channels` refers to the number of input channels in the input tensor to the convolutional layer. In other words, it is the number of feature maps or channels in the input data that the layer will process. For example, in an RGB image, `in_channels` would be 3, corresponding to the red, green, and blue color channels.\n","\n","- `out_channels`, on the other hand, refers to the number of output channels or feature maps that the convolutional layer will produce. Each output channel is produced by applying a filter or kernel to the input tensor. The number of filters is equal to `out_channels`, and each filter produces one output channel.\n","\n","- The relationship between `in_channels` and `out_channels` determines the number of parameters in the convolutional layer. Specifically, the number of parameters is equal to `(in_channels * kernel_size * kernel_size * out_channels) + out_channels`, where `kernel_size` is the size of the filter.\n","\n","**Here's an example to illustrate the difference between `in_channels` and `out_channels` in a convolutional layer:**\n","\n","- Suppose we have an input tensor `x` with dimensions `(batch_size, in_channels=3, height=32, width=32)`. This means that we have a batch of 32x32 RGB images, where each image has 3 channels (corresponding to the red, green, and blue color channels).\n","\n","- We want to apply a convolutional layer with `out_channels=16` and `kernel_size=3` to this input tensor. This means that we will learn 16 filters, each of size 3x3, that will be applied to the input tensor to produce a corresponding output feature map.\n","\n","- The output tensor of the convolutional layer will have dimensions `(batch_size, out_channels=16, height', width')`, where `height'` and `width'` depend on the size of the kernel, stride, and padding used in the convolution operation.\n","\n","  **For example,**\n","\n","  - If we use a stride of 1 and no padding, the output tensor will have dimensions `(batch_size, 16, 30, 30)`. This means that for each image in the batch, we have 16 feature maps with dimensions 30x30.\n","\n","  - Each feature map summarizes the presence of certain patterns or features in the input image. By learning multiple filters with different weights, the convolutional layer is able to extract a variety of features from the input image.\n","\n","  - In this example, `in_channels=3` corresponds to the number of input channels (red, green, and blue), while `out_channels=16` corresponds to the number of output channels or feature maps produced by the convolutional layer. The number of filters is equal to `out_channels`, and each filter produces one output channel.\n","\n","- Increasing the number of filters in a convolutional layer allows the network to learn more complex features from the input data. However, this also increases the number of parameters that need to be learned by the network, which can lead to overfitting if the number of training examples is limited."],"metadata":{"id":"zGAgsTyn_NbA"}},{"cell_type":"markdown","source":["![image](https://camo.githubusercontent.com/269e3903f62eb2c4d13ac4c9ab979510010f8968/68747470733a2f2f7261772e6769746875622e636f6d2f746176677265656e2f6c616e647573655f636c617373696669636174696f6e2f6d61737465722f66696c652f636e6e2e706e673f7261773d74727565)"],"metadata":{"id":"BbOLRK0N958l"}},{"cell_type":"markdown","source":["## Recurrent Layers ( LSTM)"],"metadata":{"id":"KXLt6XblN3uY"}},{"cell_type":"markdown","source":["let's explore Recurrent Layers, such as the LSTM (Long Short-Term Memory) layer, in a manner similar to how we did with Linear and Convolutional Layers:\n","\n","**Recurrent Layer (LSTM in PyTorch):**\n","\n","**Purpose:**\n","- The primary purpose of a Recurrent Layer, such as the LSTM (Long Short-Term Memory), is to **model sequential data** and capture temporal dependencies.\n","\n","- Recurrent layers are **designed to maintain hidden states that can capture information from previous time steps**, making them suitable for tasks like sequence prediction, language modeling, and speech recognition.\n","\n","**Hidden State:**\n","\n","- In **machine learning**, a hidden state refers to a set of values that are computed and updated by a model during the processing of input data.\n","\n","  The hidden state is typically not directly observable and is used to capture relevant information from the input that can be used to make predictions or decisions.\n","\n","- In the context of **recurrent neural networks (RNNs)**, the hidden state is a vector of values that **captures information from previous time steps** in a sequence.\n","\n","  The hidden state is updated at each time step based on the input at that time step and the previous hidden state. This allows RNNs to capture temporal dependencies within sequential data.\n","\n","- In other types of models, such as **feedforward neural networks or convolutional neural networks**, the hidden state may refer to a set of values computed by intermediate layers of the model. These values are used to transform the input data into a representation that can be used for prediction or classification.\n","\n","\n","**Mathematics:**\n","\n","- Let's denote the input to the Recurrent Layer as $x_t$, where $t$ represents the time step. The Recurrent Layer maintains a hidden state $h_t$ that evolves over time.\n","\n","- For an LSTM layer, the hidden state $h_t$ is computed as follows:\n","\n","  \\begin{equation}\n","    h_t = \\text{LSTM}(x_t, h_{t-1})\n","  \\end{equation}\n","\n","  - $x_t$ is the input at time step $t$.\n","  - $h_t$ is the hidden state at time step $t$.\n","  - $h_{t-1}$ is the hidden state from the previous time step ($t-1$).\n","\n","- The LSTM operation involves several gates **(input, forget, output)** and cell state computations, allowing it to capture short-term and long-term dependencies within sequential data.\n","\n","**Here's an example to illustrate how an LSTM layer can capture short-term and long-term dependencies:**\n","\n","Suppose we have a sequence of inputs with dimensions $1 \\times 1$ and a single LSTM layer with a hidden state size of 1:\n","\n","\\begin{equation}\n","x_1 = 1, \\quad\n","x_2 = 0, \\quad\n","x_3 = 1, \\quad\n","x_4 = 0\n","\\end{equation}\n","\n","We can apply the LSTM layer to obtain the hidden states for each time step:\n","\n","\\begin{align}\n","h_1 &= \\text{LSTM}(x_1, 0) \\\\\n","h_2 &= \\text{LSTM}(x_2, h_1) \\\\\n","h_3 &= \\text{LSTM}(x_3, h_2) \\\\\n","h_4 &= \\text{LSTM}(x_4, h_3)\n","\\end{align}\n","\n","where $0$ represents the initial hidden state.\n","\n","In this example, the input sequence alternates between 1 and 0. The LSTM layer must learn to capture the short-term dependency between consecutive time steps (i.e., when the input changes from 1 to 0 or vice versa) and the long-term dependency between non-consecutive time steps (i.e., when the input changes from 1 to 0 and back to 1).\n","\n","After applying the LSTM layer, we obtain the following hidden states:\n","\n","\\begin{equation}\n","h_1 = 0.43, \\quad\n","h_2 = -0.02, \\quad\n","h_3 = 0.59, \\quad\n","h_4 = -0.02\n","\\end{equation}\n","\n","Notice that the hidden state **oscillates between positive and negative** values depending on the input sequence. This is because the LSTM layer learns to \"remember\" the previous input and adjust the hidden state accordingly. In this way, the LSTM layer can capture both short-term and long-term dependencies within sequential data.\n","\n","![image](https://tse2.mm.bing.net/th?id=OIP.0Ddt0v5nF8rHUpMH9mpEwgHaCO&pid=Api&P=0&h=180)\n","\n","![image](https://cdn-images-1.medium.com/max/1600/1*qn_quuUSYzozyH3CheoQsA.png)\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define an LSTM layer with input size=64 and hidden size=128\n","lstm_layer = nn.LSTM(64, 128)\n","\n","# Example input tensor for a sequence with 10 time steps and input features=64\n","input_sequence = torch.randn(10, 1, 64)  # Sequence length=10, batch size=1, input features=64\n","\n","# Initialize the initial hidden state and cell state (optional)\n","initial_hidden_state = torch.randn(1, 1, 128)  # Batch size=1, hidden size=128\n","initial_cell_state = torch.randn(1, 1, 128)    # Batch size=1, hidden size=128\n","\n","# Apply the LSTM layer to the input sequence\n","output_sequence, (final_hidden_state, final_cell_state) = lstm_layer(input_sequence, (initial_hidden_state, initial_cell_state))\n","```\n","\n","In the code example above, we define an LSTM layer with an input size of 64 and a hidden size of 128. We apply this layer to an example input sequence with 10 time steps, a batch size of 1, and 64 input features. The LSTM layer processes the sequence while maintaining hidden states, and the output consists of the hidden states at each time step.\n","\n"],"metadata":{"id":"QzKtaG2gN52A"}},{"cell_type":"markdown","source":["### LSTM Input Parameters"],"metadata":{"id":"_pCmu3-KCvm8"}},{"cell_type":"markdown","source":["The input parameters of an LSTM (Long Short-Term Memory) model are as follows:\n","\n","1. `input_size`: The number of expected features in the input. For example, if you are processing grayscale images, `input_size` would be 1. If you are processing RGB images, `input_size` would be 3.\n","\n","2. `hidden_size`: The number of features in the hidden state of the LSTM cell. This parameter determines the dimensionality of the output and the number of memory cells in the LSTM. It represents the model's capacity to learn and remember information.\n","\n","3. `num_layers`: The number of LSTM layers stacked on top of each other. Each layer takes in the hidden states from the previous layer and produces outputs and hidden states that are passed to the next layer. Increasing the number of layers can enhance the model's ability to capture complex patterns but also increases computational complexity.\n","\n","4. `batch_first`: A boolean value that specifies whether the input tensors have the batch size as the first dimension. If `batch_first=True`, the input shape should be `(batch_size, sequence_length, input_size)`. If `batch_first=False`, the input shape should be `(sequence_length, batch_size, input_size)`.\n","\n","5. `dropout`: The dropout probability to apply to the outputs of each LSTM layer except the last one. Dropout is a regularization technique that randomly sets a fraction of the input units to zero during training, which helps prevent overfitting.\n","\n","These parameters define the architecture and behavior of the LSTM model and can be adjusted based on the specific task and dataset."],"metadata":{"id":"V_lqyVavCt2n"}},{"cell_type":"markdown","source":["## GRU"],"metadata":{"id":"59Ik_p7yRkfy"}},{"cell_type":"markdown","source":["let's explore the GRU (Gated Recurrent Unit)\n","**GRU Layer (Gated Recurrent Unit in PyTorch):**\n","\n","**Purpose:**\n","- The primary purpose of a GRU (Gated Recurrent Unit) layer is to model sequential data and capture **temporal dependencies**.\n","\n","- Similar to LSTM, the GRU layer maintains hidden states that can capture information from previous time steps, making it suitable for tasks like sequence prediction, language modeling, and speech recognition.\n","\n","\n","**Mathematics:**\n","\n","- Let's denote the input to the GRU Layer as $x_t$, where $t$ represents the time step. The GRU Layer maintains a hidden state $h_t$ that evolves over time.\n","\n","- The hidden state $h_t$ is computed as follows in a GRU layer:\n","\n","  \\begin{equation}\n","    h_t = \\text{GRU}(x_t, h_{t-1})\n","  \\end{equation}\n","\n","  - $x_t$ is the input at time step $t$.\n","  - $h_t$ is the hidden state at time step $t$.\n","  - $h_{t-1}$ is the hidden state from the previous time step ($t-1$).\n","\n","- The GRU operation involves gates (reset and update gates) that control the flow of information, allowing it to capture short-term dependencies and avoid some of the vanishing gradient problems associated with traditional RNNs.\n","\n","**Here's an example to illustrate how a GRU layer can capture short-term dependencies:**\n","\n","Suppose we have a sequence of inputs with dimensions $1 \\times 1$ and a single GRU layer with a hidden state size of 1:\n","\n","\\begin{equation}\n","x_1 = 1, \\quad\n","x_2 = 0, \\quad\n","x_3 = 1, \\quad\n","x_4 = 0\n","\\end{equation}\n","\n","We can apply the GRU layer to obtain the hidden states for each time step:\n","\n","\\begin{align}\n","h_1 &= \\text{GRU}(x_1, 0) \\\\\n","h_2 &= \\text{GRU}(x_2, h_1) \\\\\n","h_3 &= \\text{GRU}(x_3, h_2) \\\\\n","h_4 &= \\text{GRU}(x_4, h_3)\n","\\end{align}\n","\n","where $0$ represents the initial hidden state.\n","\n","In this example, the input sequence alternates between 1 and 0. The GRU layer must learn to capture the short-term dependency between consecutive time steps (i.e., when the input changes from 1 to 0 or vice versa).\n","\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define a GRU layer with input size=64 and hidden size=128\n","gru_layer = nn.GRU(64, 128)\n","\n","# Example input tensor for a sequence with 10 time steps and input features=64\n","input_sequence = torch.randn(10, 1, 64)  # Sequence length=10, batch size=1, input features=64\n","\n","# Initialize the initial hidden state (optional)\n","initial_hidden_state = torch.randn(1, 1, 128)  # Batch size=1, hidden size=128\n","\n","# Apply the GRU layer to the input sequence\n","output_sequence, final_hidden_state = gru_layer(input_sequence, initial_hidden_state)\n","```\n","\n","In the code example above, we define a GRU layer with an input size of 64 and a hidden size of 128. We apply this layer to an example input sequence with 10 time steps, a batch size of 1, and 64 input features. The GRU layer processes the sequence while maintaining hidden states, and the output consists of the hidden states at each time step, as well as the final hidden state.\n","\n"],"metadata":{"id":"ZyoCFrf3Rv_z"}},{"cell_type":"markdown","source":["\n","\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define the GRU model\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(GRUModel, self).__init__()\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","        \n","    def forward(self, x):\n","        # Initialize the hidden state\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        \n","        # Pass the input sequence through the GRU layer\n","        out, _ = self.gru(x, h0)\n","        \n","        # Extract the final hidden state and pass it through the linear layer\n","        out = self.fc(out[:, -1, :])\n","        \n","        return out\n","\n","# Set the device to use (GPU or CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define the hyperparameters\n","input_size = 1\n","hidden_size = 32\n","num_layers = 1\n","output_size = 1\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","# Instantiate the model and move it to the device\n","model = GRUModel(input_size, hidden_size, num_layers, output_size).to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","```\n","\n","In this example, we define a `GRUModel` class that inherits from `nn.Module` and contains a `nn.GRU` layer and a `nn.Linear` layer. The `forward` method takes an input tensor `x` and passes it through the GRU layer, then extracts the final hidden state and passes it through the linear layer to obtain the output.\n","\n","We then instantiate this model, define a loss function (`nn.MSELoss`) and an optimizer (`torch.optim.Adam`), and train the model using a loop over the training data. At each iteration, we move the input data and labels to the device (GPU or CPU), perform a forward pass through the model, compute the loss, perform a backward pass to compute gradients, and update the model parameters using the optimizer.\n","\n"],"metadata":{"id":"946f31XXTuRp"}},{"cell_type":"markdown","source":["### GRU vs LSTM"],"metadata":{"id":"T-q5IjNOUemb"}},{"cell_type":"markdown","source":["\n","\n","GRUs and LSTMs are both types of recurrent neural networks that can capture long-term dependencies within sequential data. While both models have similar architectures and use gates to control the flow of information, there are some key differences between them.\n","\n","- One major difference is that GRUs have fewer parameters than LSTMs, which can make them faster to train and more memory-efficient. However, LSTMs are generally better at capturing long-term dependencies and are more robust to vanishing gradient problems.\n","\n","- Another difference is that GRUs have only two gates (reset and update gates), while LSTMs have three gates (input, forget, and output gates). This makes GRUs simpler than LSTMs but also potentially less expressive.\n","\n","- In practice, the choice between GRUs and LSTMs depends on the specific task and dataset. Some researchers have found that GRUs perform just as well as LSTMs on certain tasks, while others have found that LSTMs are still superior in many cases.\n"],"metadata":{"id":"YPhq2D2kUKS4"}},{"cell_type":"markdown","source":["# Dropout Layer"],"metadata":{"id":"4kpCcFUIWKqh"}},{"cell_type":"markdown","source":["**Dropout Layer:**\n","\n","**Purpose:**\n","- The primary purpose of a Dropout layer is to **prevent overfitting in neural networks**. Overfitting occurs when a model becomes too specialized in learning the training data and performs poorly on unseen data.\n","- Dropout is a **regularization technique** that helps improve the generalization ability of a neural network by randomly deactivating (dropping out) a fraction of neurons during each training step.\n","\n","\n","**Mathematics:**\n","\n","- Dropout is typically applied to hidden units or neurons in a neural network layer. Let's denote the input to a Dropout Layer as $x$.\n","\n","- During training, each neuron's output is set to **zero with a certain probability** $p$, and the remaining neurons are **scaled** by $\\frac{1}{1-p}$ to maintain the expected value.\n","\n","- The operation can be expressed as:\n","\n","  \\begin{equation}\n","    y_i = \\frac{x_i \\cdot \\text{Bernoulli}(1 - p)}{1 - p}\n","  \\end{equation}\n","\n","  - $y_i$ is the output of the $i$-th neuron after dropout.\n","  - $x_i$ is the input to the $i$-th neuron.\n","  - $p$ is the dropout probability, representing the fraction of neurons to drop during training.\n","  - $\\text{Bernoulli}(1 - p)$ is a random binary variable that determines whether a neuron is dropped out (0) or not (1).\n","\n","**Here's an example to illustrate how dropout works:**\n","\n","Suppose we have a neural network layer with 5 neurons and a dropout probability of $p = 0.5$. We can apply dropout to the layer by randomly dropping out half of the neurons during training. Here's an example input vector $x$ and the corresponding output vector $y$ after dropout:\n","\n","\\begin{equation}\n","  x = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\quad\n","  y = \\begin{bmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 8 \\\\ 0 \\end{bmatrix}\n","\\end{equation}\n","\n","In this example, neurons 2, 3, and 5 were dropped out (set to zero) during training, while neurons 1 and 4 were scaled up by a factor of 2 to maintain the expected value. During inference (i.e., when making predictions on new data), dropout is typically turned off and all neurons are used.\n","\n","\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch.nn as nn\n","\n","# Define a Dropout layer with a dropout probability of 0.5 (50% dropout)\n","dropout_layer = nn.Dropout(p=0.5)\n","\n","# Example input tensor\n","input_data = torch.randn(10, 64)  # Batch size=10, input features=64\n","\n","# Apply the Dropout layer to the input data during training\n","output = dropout_layer(input_data)\n","```\n","\n","In the code example above, we define a Dropout layer with a dropout probability of 0.5, which means that during training, each neuron has a 50% chance of being dropped out (set to zero). This helps prevent overfitting by encouraging the network to rely on multiple pathways for learning.\n"],"metadata":{"id":"yLtXCLvqVVsl"}},{"cell_type":"markdown","source":["# Pooling Layer"],"metadata":{"id":"eI_4oQ-MWN8R"}},{"cell_type":"markdown","source":["\n","**Max Pooling Layer:**\n","\n","**Purpose:**\n","- The primary purpose of a Max Pooling layer is to** downsample feature maps in neural networks while retaining important information**. Pooling helps reduce the spatial dimensions of feature maps, which can lead to computational efficiency and translational invariance.\n","\n","\n","**Mathematics:**\n","\n","- Max Pooling is typically applied to feature maps in convolutional neural networks (CNNs).**Given a feature map $F$, Max Pooling divides it into non-overlapping regions (often called \"pools\" or \"windows\") and selects the maximum value within each region to create a downsampled feature map $P$.**\n","\n","- The operation can be expressed as:\n","\n","  \\begin{equation}\n","    P[i, j] = \\max_{m, n} F[i \\cdot \\text{stride} + m, j \\cdot \\text{stride} + n]\n","  \\end{equation}\n","\n","  - $P[i, j]$ is the value at position $(i, j)$ in the downsampled feature map $P$.\n","  - $F[i \\cdot \\text{stride} + m, j \\cdot \\text{stride} + n]$ represents the local region in the original feature map $F$ at position $(i \\cdot \\text{stride} + m, j \\cdot \\text{stride} + n)$.\n","  - $\\text{stride}$ determines the spacing between the selected regions, and $m$ and $n$ iterate over the region.\n","\n","**Here's an example to illustrate how Max Pooling works:**\n","\n","Suppose we have a $4 \\times 4$ feature map $F$ and apply Max Pooling with a pool size of $2 \\times 2$ and a stride of 2. Here's the original feature map $F$ and the resulting downsampled feature map $P$:\n","\n","\\begin{equation}\n","  F = \\begin{bmatrix}\n","    1 & 3 & 2 & 4 \\\\\n","    5 & 2 & 6 & 8 \\\\\n","    3 & 1 & 0 & 7 \\\\\n","    4 & 2 & 1 & 9\n","  \\end{bmatrix}, \\quad\n","  P = \\begin{bmatrix}\n","    5 & 8 \\\\\n","    4 & 9\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","In this example, we divide the $4 \\times 4$ feature map into non-overlapping $2 \\times 2$ pools. For each pool, we select the maximum value to create the downsampled feature map $P$. The stride of 2 determines the spacing between the selected regions. As a result, the original feature map is reduced in size, while preserving the most salient features.\n","\n","Max Pooling is commonly used in CNNs to reduce the spatial dimensions of feature maps, making them more compact and computationally efficient. It also **helps with translation invariance by selecting the most prominent features** within each region.\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define a Max Pooling layer with a kernel size of 2x2 and a stride of 2\n","maxpool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","# Example input tensor with 3 channels, height=8, and width=8\n","input_data = torch.randn(10, 3, 8, 8)  # Batch size=10, channels=3, height=8, width=8\n","\n","# Apply the Max Pooling layer to the input data\n","output = maxpool_layer(input_data)\n","```\n","\n","In the code example above, we define a Max Pooling layer with a kernel size of 2x2 and a stride of 2. We apply this layer to an example input tensor with a batch size of 10, three channels, and an 8x8 spatial size. The Max Pooling layer downsamples the spatial dimensions by selecting the maximum value in each 2x2 region, resulting in a smaller feature map.\n"],"metadata":{"id":"1qphw5nMWRx5"}},{"cell_type":"markdown","source":["# Upsample Layer"],"metadata":{"id":"XrAIcHwjvjcf"}},{"cell_type":"markdown","source":["Upsampling, also known as upsampling or interpolation, is a technique used in deep learning to **increase the spatial resolution of input data**.\n","\n","- It is particularly useful in tasks such as **image segmentation and object detection, where fine-grained details need to be captured**.\n","\n","- During upsampling, the **size of the feature maps is increased by inserting additional values between the existing ones**. This expansion allows the network to learn more complex patterns and structures in the data.\n","\n","- There are different methods of upsampling, including:\n","\n","  1. **Nearest Neighbor Interpolation:** In this technique, the value of each new pixel is determined by the value of its nearest neighbor in the original image. It is a simple and fast technique but can result in blocky artifacts.\n","\n","  2. **Bilinear Interpolation:** In this technique, the value of each new pixel is determined by taking a weighted average of the four nearest neighbors in the original image. It produces smoother results than nearest neighbor interpolation but can still result in some blurriness.\n","\n","  3. **Transposed Convolution**: Also known as deconvolution, it is a learnable upsampling technique that uses a convolutional layer to increase the spatial resolution of the input data. It is more flexible and can learn to produce sharper results than other upsampling techniques, but it is also more computationally expensive.\n","\n","Each method has its advantages and disadvantages in terms of computational complexity and output quality. Upsampling plays a crucial role in improving the resolution and fidelity of feature maps, enabling neural networks to make more accurate predictions and perform better on various computer vision tasks."],"metadata":{"id":"06XdwWpov4ed"}},{"cell_type":"markdown","source":["#### Nearest Neighbor Interpolation"],"metadata":{"id":"N8-u1pm5vnL6"}},{"cell_type":"markdown","source":["**Nearest Neighbor Interpolation Upsampling:**\n","\n","**Purpose:**\n","- Nearest Neighbor Interpolation is a simple and efficient method for upsampling images or feature maps.\n","\n","- It is often used when **increasing the spatial resolution of an image or feature map is necessary, and the task does not require interpolation between pixel values**.\n","\n","- Nearest Neighbor Interpolation is a straightforward and efficient method for upsampling, often used when interpolation between pixel values is not necessary, and preserving the original pixel values is sufficient. It is commonly employed in tasks like image resizing and certain types of feature map upsampling.\n","\n","\n","**Mathematics:**\n","- Nearest Neighbor Interpolation selects the value of the nearest neighbor pixel in the original image or feature map to estimate pixel values at non-integer coordinates in the upscaled image.\n","\n","**Steps for Nearest Neighbor Interpolation:**\n","1. Given an input image or feature map with a size of $H \\times W$, the goal is to upsample it to a larger size, $2H \\times 2W$ in this example.\n","\n","2. For each pixel in the output (upsampled) image, calculate its position in the original image or feature map by dividing the new size by the old size. For example, if we want to calculate the pixel at $(x', y')$ in the original image:\n","   $x' = \\frac{x}{2}, \\quad y' = \\frac{y}{2}$\n","\n","3. Round the fractional positions $x'$ and $y'$ to the nearest integer coordinates $(x_1, y_1)$ in the original image. These integer coordinates represent the nearest neighbor pixel.\n","\n","4. Set the pixel value at $(x', y')$ in the output image equal to the pixel value at $(x_1, y_1)$ in the original image:\n","   $I(x', y') = I(x_1, y_1)$\n","\n","**Numerical Example:**\n","\n","Let's consider an input image with a size of $4 \\times 4$ pixels. We want to upsample it to a larger size of $8 \\times 8$ pixels using Nearest Neighbor Interpolation.\n","\n","For simplicity, let's calculate the position of the pixel at $(2, 3)$ in the original image.\n","\n","1. Calculate the position in the original image:\n","   $x' = \\frac{2}{2} = 1, \\quad y' = \\frac{3}{2} = 1.5$\n","\n","2. Round the fractional positions to the nearest integer coordinates:\n","   $(x_1, y_1) = (1, 2)$\n","\n","3. Set the pixel value at $(2, 3)$ in the output image equal to the pixel value at $(1, 2)$ in the original image:\n","   $I(2, 3) = I(1, 2)$\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn.functional as F\n","\n","# Example input tensor with a spatial size of 8x8\n","input_data = torch.randn(10, 3, 8, 8)  # Batch size=10, channels=3, height=8, width=8\n","\n","# Apply Nearest Neighbor Interpolation Upsampling to increase the spatial resolution\n","output = F.interpolate(input_data, scale_factor=2, mode='nearest')\n","```\n","\n","In the code example above, we use PyTorch's `F.interpolate` function with the mode set to 'nearest' to perform Nearest Neighbor Interpolation upsampling. The `scale_factor` parameter determines the upsampling factor.\n","\n"],"metadata":{"id":"v-BqabLmyIgX"}},{"cell_type":"markdown","source":["#### Bilinear Interpolation"],"metadata":{"id":"T5c87FHLw23g"}},{"cell_type":"markdown","source":["**Bilinear Interpolation Upsampling:**\n","\n","**Purpose:**\n","- Bilinear Interpolation is a simple and widely used method for upsampling images or feature maps.\n","\n","- It is often used when increasing the spatial resolution of an image or feature map is necessary, but the **task does not require learning the upsampling operation**.\n","\n","- Bilinear Interpolation is a straightforward and efficient method for upsampling, often used in various computer vision tasks, including **image resizing, image super-resolution, and certain types of feature map upsampling**. It provides reasonable results when fine-grained details are not a primary concern.\n","\n","**Mathematics:**\n","\n","- Bilinear Interpolation is a mathematical method for estimating values of pixels at non-integer coordinates in an image or feature map. It **considers the weighted average of the nearest four pixels**.\n","\n","**Steps for Bilinear Interpolation:**\n","1. Given an input image or feature map with a size of $H \\times W$, the goal is to upsample it to a larger size, $2H \\times 2W$ in this example.\n","\n","2. For each pixel in the output (upsampled) image, **calculate its position in the original image** or feature map by dividing the new size by the old size. For example, if we want to calculate the pixel at $(x', y')$ in the original image:\n","   $x' = \\frac{x}{2}, \\quad y' = \\frac{y}{2}$\n","\n","3. Find the four nearest neighbor pixels in the **original image**, denoted as $(x_1, y_1)$, $(x_2, y_2)$, $(x_3, y_3)$, and $(x_4, y_4)$, surrounding the position $(x', y')$.\n","\n","4. Calculate the fractional parts $\\alpha$ and $\\beta$ representing how far the position $(x', y')$ is from each of the four nearest neighbor pixels:\n","   $\\alpha = x' - \\lfloor x' \\rfloor, \\quad \\beta = y' - \\lfloor y' \\rfloor$\n","\n","5. **Interpolate the pixel value at $(x', y')$ by taking a weighted average of the four nearest neighbor pixel** values:\n","   $I(x', y') = (1 - \\alpha)(1 - \\beta)I(x_1, y_1) + \\alpha(1 - \\beta)I(x_2, y_2) + (1 - \\alpha)\\beta I(x_3, y_3) + \\alpha\\beta I(x_4, y_4)$\n","\n","**Example:**\n","\n","Suppose we have an input image with a size of $2 \\times 2$ and pixel values:\n","```\n","[[1, 2],\n"," [3, 4]]\n","```\n","We want to upsample it to a larger size of $4 \\times 4$. To calculate the pixel value at position $(1.5, 1.5)$ in the upsampled image, we can follow these steps:\n","\n","- $x' = \\frac{1.5}{2} = 0.75, \\quad y' = \\frac{1.5}{2} = 0.75$\n","- The four nearest neighbor pixels in the original image are: $(1, 1)$, $(2, 1)$, $(1, 2)$, and $(2, 2)$\n","- $\\alpha = x' - \\lfloor x' \\rfloor = 0.75 - 0 = 0.75, \\quad \\beta = y' - \\lfloor y' \\rfloor = 0.75 - 0 = 0.75$\n","- $I(1.5, 1.5) = (1 - 0.75)(1 - 0.75)I(1, 1) + 0.75(1 - 0.75)I(2, 1) + (1 - 0.75)0.75I(1, 2) + 0.75\\cdot0.75I(2, 2) = 2.5$\n","\n","Therefore, the pixel value at position $(1.5, 1.5)$ in the upsampled image is estimated to be $2.5$.\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn.functional as F\n","\n","# Example input tensor with a spatial size of 8x8\n","input_data = torch.randn(10, 3, 8, 8)  # Batch size=10, channels=3, height=8, width=8\n","\n","# Apply Bilinear Interpolation Upsampling to increase the spatial resolution\n","output = F.interpolate(input_data, scale_factor=2, mode='bilinear', align_corners=False)\n","```\n","\n","In the code example above, we use PyTorch's `F.interpolate` function with the mode set to 'bilinear' to perform Bilinear Interpolation upsampling. The `scale_factor` parameter determines the upsampling factor.\n","\n","- `align_corners` is a parameter in the `F.interpolate` function that determines whether the function should align the corner pixels of the input and output tensors or not.\n","\n","- When `align_corners` is set to `True`, the function aligns the corners of the input and output tensors, which means that the function considers the values of the corner pixels of both tensors when performing interpolation. This is typically used when the input and output tensors represent spatial coordinates or when the tensor represents an image.\n","\n","- When `align_corners` is set to `False`, the function aligns the centers of the corner pixels of the input and output tensors, which means that the function does not consider the values of the corner pixels of both tensors when performing interpolation. This is typically used when the input and output tensors represent pixel values or feature maps.\n"],"metadata":{"id":"9rJTPIY5yFwG"}},{"cell_type":"markdown","source":["#### Transposed Convolution"],"metadata":{"id":"FhczQuz-w5XJ"}},{"cell_type":"markdown","source":["**Transposed Convolution (Deconvolution) Layer for Upsampling:**\n","\n","**Purpose:**\n","- The Transposed Convolution layer is used for upsampling feature maps.\n","\n","- It is particularly valuable in tasks like image segmentation and image generation, where increasing the spatial resolution of feature maps is necessary.\n","\n","- Transposed Convolution is widely used in neural network architectures for tasks that require increasing the spatial resolution of feature maps, such as generating high-resolution images or performing image segmentation. It allows the network to learn the upsampling operation during training, which can be crucial for capturing fine-grained details.\n","\n","**Key Components:**\n","- The primary component of a Transposed Convolution layer is a **learnable convolutional kernel.**\n","\n","**Mathematics:**\n","- The Transposed Convolution operation can be mathematically represented as follows:\n","  $ \\text{TransConv}(x) = x * W $\n","  - $x$ is the input feature map.\n","  - $W$ is a learnable convolutional kernel.\n","  - The output has larger spatial dimensions compared to the input, achieved by inserting zeros between the elements of $x$ and convolving it with $W$.\n","\n","\n","- \"Convolving it with $W$\" means applying the convolution operation to the input feature map $x$ using a learnable convolutional kernel $W$.\n","\n","  - In the context of convolutional neural networks, **convolution is a mathematical operation that involves sliding a small window (i.e., the kernel) over the input feature map and computing the dot product between the kernel and the corresponding region of the feature map at each position**. The result of this operation is a new feature map that summarizes the presence of certain patterns or features in the input.\n","\n","  - In mathematics, **convolution is a mathematical operation that combines two functions to produce a third function that expresses how one function modifies the other**. In the context of signal processing and image processing, convolution is a widely used technique for filtering and processing signals and images.\n","\n","  - When performing **transposed convolution, we use a learnable convolutional kernel $W$ and insert zeros between the elements of the input feature map $x$, and then apply the convolution operation to obtain a new feature map with larger spatial dimensions. The values of the learnable kernel $W$ are updated during training using backpropagation**, allowing the network to learn features that are useful for the task at hand.\n","\n","**Example:**\n","\n","Suppose we have an input feature map $x$ with a size of $2 \\times 2$ and pixel values:\n","```\n","[[1, 2],\n"," [3, 4]]\n","```\n","We want to perform transposed convolution on this feature map using a learnable kernel $W$ with a size of $3 \\times 3$ and pixel values:\n","```\n","[[1, 0, 1],\n"," [0, 1, 0],\n"," [1, 0, 1]]\n","```\n","To perform transposed convolution, we insert zeros between the elements of $x$, resulting in the following padded feature map:\n","```\n","[[1, 0, 2, 0],\n"," [0, 0, 0, 0],\n"," [3, 0, 4, 0],\n"," [0, 0, 0, 0]]\n","```\n","Then we convolve this padded feature map with the kernel $W$, resulting in the following output feature map:\n","```\n","[[1, 0, 3, 0, 2],\n"," [0, 1, 0, 4, 0],\n"," [1, 0, 3, 0, 2],\n"," [0, 3, 0, 4, 0],\n"," [2, 0, 1, 0, 2]]\n","```\n","The output feature map has larger spatial dimensions compared to the input feature map.\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define a Transposed Convolution layer with the desired kernel size and stride for upsampling\n","transconv_layer = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n","\n","# Example input tensor with 64 channels and a spatial size of 16x16\n","input_data = torch.randn(10, 64, 16, 16)  # Batch size=10, channels=64, height=16, width=16\n","\n","# Apply the Transposed Convolution layer to upsample the input data\n","output = transconv_layer(input_data)\n","```\n","\n","In the code example above, we define a Transposed Convolution layer in PyTorch for upsampling. This layer uses learnable convolutional kernels to increase the spatial resolution of the input feature map. The `stride` parameter determines the upsampling factor, and `padding` controls the output spatial dimensions.\n","\n"],"metadata":{"id":"M8dpeLBByCQY"}},{"cell_type":"markdown","source":["# Normalization Layer"],"metadata":{"id":"Mh_sVBYwXGGa"}},{"cell_type":"markdown","source":["Normalization techniques like layer normalization are typically applied during training to improve the learning process and stabilize training dynamics. However, during testing and validation, normalization is still applied to ensure consistency and fair evaluation.\n","\n","- During training, normalization helps in reducing the internal covariate shift, improving generalization, and stabilizing the learning process. It ensures that the network is not overly sensitive to the scale and distribution of the input data. Normalization is typically applied to the mini-batches of training data.\n","\n","- During testing and validation, normalization is still necessary to ensure that the input data is in a consistent range and distribution. However, instead of using mini-batches, normalization is usually applied to the entire test or validation set. The statistics used for normalization (mean and standard deviation) are typically calculated from the training set and then applied to the test or validation set.\n","\n","- By applying the same normalization procedure during testing and validation as during training, we ensure that the model's performance is evaluated on data that is representative of what it has been trained on. This allows for fair and consistent evaluation of the model's performance."],"metadata":{"id":"Wo5H--1nbQP9"}},{"cell_type":"markdown","source":["\n","\n","| Normalization Technique | When to Use | Reasons |\n","|-------------------------|-------------|---------|\n","| Batch Normalization     | Large batch sizes, feed-forward networks | Helps with training stability, reduces internal covariate shift, improves generalization |\n","| Layer Normalization     | Recurrent neural networks (RNNs), transformer models | Maintains independence of feature dimensions, captures temporal dependencies or attention mechanisms |\n","| Instance Normalization  | Style transfer, image generation tasks | Normalizes activations within each instance or sample, useful for style transfer and image generation |\n","| Group Normalization     | Small batch sizes, networks with channel dependencies | Divides channels into groups and normalizes each group, effective when batch size is small or network has channel dependencies |\n"],"metadata":{"id":"87CRjMwEa3Vj"}},{"cell_type":"markdown","source":["####**Batch Normalization Layer:**\n","\n","**Purpose:**\n","- The Batch Normalization (BatchNorm) layer is primarily used to improve the training stability and convergence of neural networks.\n","\n","- It normalizes the **activations of each layer within a mini-batch**, making the training process more robust and accelerating convergence.\n","\n","**Mathematics:**\n","- The BatchNorm operation can be mathematically represented as follows:\n","  $$\\text{BatchNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta $$\n","  - $x$ is the input to the BatchNorm layer.\n","  - $\\mu$ is the mean of the mini-batch.\n","  - $\\sigma$ is the standard deviation of the mini-batch.\n","  - $\\gamma$ is a learnable scaling parameter.\n","  - $\\beta$ is a learnable shifting parameter.\n","  \n","- BatchNorm is applied independently to each channel or feature dimension.\n","\n","**Here's an example to illustrate how BatchNorm works:**\n","\n","Suppose we have a mini-batch of size 3 with a $2 \\times 3 \\times 4$ input tensor $x$ with the following values:\n","\\begin{equation}\n","  x = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      [1 & 2 & 3 & 4] \\\\\n","      [5 & 6 & 7 & 8] \\\\\n","      [9 & 10 & 11 & 12]\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      [13 & 14 & 15 & 16] \\\\\n","      [17 & 18 & 19 & 20] \\\\\n","      [21 & 22 & 23 & 24]\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      [25 & 26 & 27 & 28] \\\\\n","      [29 & 30 & 31 & 32] \\\\\n","      [33 & 34 & 35 & 36]\n","    \\end{bmatrix}\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","- We apply BatchNorm to $x$ with learnable scaling parameter $\\gamma = [1, 2, 3, 4]$ and learnable shifting parameter $\\beta = [-1, -2, -3, -4]$.\n","\n","- We calculate the mean $\\mu$ and standard deviation $\\sigma$ of each channel independently:\n","\n","\\begin{equation}\n","  \\mu = \\begin{bmatrix}\n","    [17,18,19,20] \\\\\n","    [23,24,25,26] \\\\\n","    [29,30,31,32]\n","  \\end{bmatrix}, \\quad\n","  \\sigma = \\begin{bmatrix}\n","    [9.7979599,9.7979599,9.7979599,9.7979599] \\\\\n","    [9.7979599,9.7979599,9.7979599,9.7979599] \\\\\n","    [9.7979599,9.7979599,9.7979599,9.7979599]\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","Then, we apply the BatchNorm operation to $x$:\n","\n","Then, we apply the BatchNorm operation to $x$:\n","\n","\\begin{equation}\n","  \\text{BatchNorm}(x) = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      (1-7)/3.5 * 1 - 1 & (2-7)/3.5 * 2 - 2 & (3-7)/3.5 * 3 - 3 & (4-7)/3.5 * 4 - 4 \\\\\n","      (5-7)/3.5 * 1 - 1 & (6-7)/3.5 * 2 - 2 & (7-7)/3.5 * 3 - 3 & (8-7)/3.5 * 4 - 4 \\\\\n","      (9-7)/3.5 * 1 - 1 & (10-7)/3.5 * 2 - 2 & (11-7)/3.5 * 3 - 3 & (12-7)/3.5 * 4 - 4\n","    \\end{bmatrix} , &\n","    \\begin{bmatrix}\n","      (13-19)/3.5 * 1 - 1 & (14-19)/3.5 * 2 - 2 & (15-19)/3.5 * 3 - 3 & (16-19)/3.5 * 4 - 4 \\\\\n","      (17-19)/3.5 * 1 - 1 & (18-19)/3.5 * 2 - 2 & (19-19)/3.5 * 3 - 3 & (20-19)/3.5 * 4 - 4 \\\\\n","      (21-19)/3.5 * 1 - 1 & (22-19)/3.5 * 2 - 2 & (23-19)/3.5 * 3 - 3 & (24-19)/3.5 * 4 - 4\n","    \\end{bmatrix} , &\n","    \\begin{bmatrix}\n","      (25-31)/3.5 * 1 - 1 & (26-31)/3.5 * 2 - 2 & (27-31)/3.5 * 3 - 3 & (28-31)/3.5 * 4 - 4 \\\\\n","      (29-31)/3.5 * 1 - 1 & (30-31)/3.5 * 2 - 2 & (31-31)/3.5 * 3 - 3 & (32-31)/3.5 * 4 - 4 \\\\\n","      (33-31)/3.5 * 1 - 1 & (34-31)/3.5 * 2 -[omitted]\n","    \\end{bmatrix}\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch.nn as nn\n","\n","# Define a Batch Normalization layer with the number of features (channels) as input\n","batchnorm_layer = nn.BatchNorm2d(num_features=64)\n","\n","# Example input tensor with 64 features (e.g., a feature map in a CNN)\n","input_data = torch.randn(10, 64, 32, 32)  # Batch size=10, channels=64, height=32, width=32\n","\n","# Apply Batch Normalization to the input data\n","output = batchnorm_layer(input_data)\n","```\n","\n"],"metadata":{"id":"3oUGIKbdSaTq"}},{"cell_type":"markdown","source":["####**Layer Normalization Layer:**\n","\n","**Purpose:**\n","\n","- The Layer Normalization (LayerNorm) layer is used to **normalize activations within each layer of a neural network independently**.\n","- Activations, in the context of a neural network, refer to the **output values of each neuron** or node in a given layer.\n","- It can be particularly useful in recurrent neural networks (RNNs) and transformer architectures.\n","\n","\n","\n","**Mathematics:**\n","- The LayerNorm operation can be mathematically represented as follows:\n","  $$\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta$$\n","  - $x$ is the input to the LayerNorm layer.\n","  - $\\mu$ is the mean of the layer's activations.\n","  - $\\sigma$ is the standard deviation of the layer's activations.\n","  - $\\gamma$ is a learnable scaling parameter.\n","  - $\\beta$ is a learnable shifting parameter.\n","  \n","  LayerNorm is applied independently to each feature dimension.\n","\n","\n","**Here's an example to illustrate how LayerNorm works:**\n","\n","Suppose we have a $2 \\times 3$ input tensor $x$ with the following values:\n","\\begin{equation}\n","  x = \\begin{bmatrix}\n","    1 & 2 & 3 \\\\\n","    4 & 5 & 6\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","We apply LayerNorm to $x$ with learnable scaling parameter $\\gamma = [1, 2, 3]$ and learnable shifting parameter $\\beta = [-1, -2, -3]$. We calculate the mean $\\mu$ and standard deviation $\\sigma$ of each feature dimension independently:\n","\n","\\begin{equation}\n","  \\mu = \\begin{bmatrix}\n","    2.5 \\\\\n","    4.5\n","  \\end{bmatrix}, \\quad\n","  \\sigma = \\begin{bmatrix}\n","    1.5 \\\\\n","    1.5\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","- In a normalization layer, such as Batch Normalization or Layer Normalization, the learnable scaling parameter  and the learnable shifting parameter  are used to control the normalization process.\n","\n","- The scaling parameter  is multiplied with the normalized values to scale them up or down. In this case, the parameter  is set to [1, 2, 3], which means that each normalized value will be multiplied by the corresponding element in the  vector.\n","\n","- The shifting parameter  is added to the scaled values to shift them up or down. In this case, the parameter  is set to [-1, -2, -3], which means that each scaled value will have the corresponding element in the  vector added to it.\n","\n","\n","\n","\n","Then, we apply the LayerNorm operation to $x$:\n","\n","\\begin{equation}\n","  \\text{LayerNorm}(x) = \\begin{bmatrix}\n","    (1-2.5)/1.5 * 1 - 1 & (2-2.5)/1.5 * 2 - 2 & (3-2.5)/1.5 * 3 - 3 \\\\\n","    (4-4.5)/1.5 * 1 - 1 & (5-4.5)/1.5 * 2 - 2 & (6-4.5)/1.5 * 3 - 3\n","  \\end{bmatrix} = \\begin{bmatrix}\n","    -0.67 & -0.67 & -0.67 \\\\\n","    -0.67 & -0.67 & -0.67\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","As a result, **we obtain a normalized version of the input tensor $x$, where each feature dimension has zero mean and unit variance**, and the scaling and shifting parameters allow for learned adjustments to the normalization.\n","\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch.nn as nn\n","\n","# Define a Layer Normalization layer with the number of features (channels) as input\n","layernorm_layer = nn.LayerNorm(normalized_shape=64)\n","\n","# Example input tensor with 64 features (e.g., activations from a layer)\n","input_data = torch.randn(10, 64)  # Batch size=10, features=64\n","\n","# Apply Layer Normalization to the input data\n","output = layernorm_layer(input_data)\n","```\n","\n"],"metadata":{"id":"1telGC-1WLVm"}},{"cell_type":"markdown","source":["#### **Instance Normalization Layer:**\n","\n","**Purpose:**\n","- Instance Normalization (InstanceNorm) is used to normalize activations within each instance (sample) independently.\n","\n","-It is often employed in style **transfer and image-to-image translation tasks**.\n","\n","\n","**Mathematics:**\n","- The InstanceNorm operation can be mathematically represented as follows:\n","  $$\\text{InstanceNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta $$\n","  - $x$ is the input to the InstanceNorm layer.\n","  - $\\mu$ is the mean of the activations for each instance.\n","  - $\\sigma$ is the standard deviation of the activations for each instance.\n","  - $\\gamma$ is a learnable scaling parameter.\n","  - $\\beta$ is a learnable shifting parameter.\n","  \n","- InstanceNorm is applied independently to each instance or sample.\n","\n","**Here's an example to illustrate how InstanceNorm works:**\n","\n","Suppose we have a $2 \\times 3 \\times 4$ input tensor $x$ with the following values:\n","\\begin{equation}\n","  x = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      [1 & 2 & 3 & 4] \\\\\n","      [5 & 6 & 7 & 8] \\\\\n","      [9 & 10 & 11 & 12]\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      [13 & 14 & 15 & 16] \\\\\n","      [17 & 18 & 19 & 20] \\\\\n","      [21 & 22 & 23 & 24]\n","    \\end{bmatrix}\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","- We apply InstanceNorm to $x$ with learnable scaling parameter $\\gamma = [1, 2, 3, 4]$ and learnable shifting parameter $\\beta = [-1, -2, -3, -4]$.\n","- We calculate the mean $\\mu$ and standard deviation $\\sigma$ of the activations for each instance independently:\n","\n","\\begin{equation}\n","  \\mu = \\begin{bmatrix}\n","    [2.5, 6.5, 10.5] \\\\\n","    [14.5, 18.5, 22.5]\n","  \\end{bmatrix}, \\quad\n","  \\sigma = \\begin{bmatrix}\n","    [1.12,1.12,1.12] \\\\\n","    [1.12,1.12,1.12]\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","Then, we apply the InstanceNorm operation to $x$:\n","\n","\\begin{equation}\n","  \\text{InstanceNorm}(x) = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      (1-2.5)/1.12 * 1 - 1 & (2-2.5)/1.12 * 2 - 2 & (3-2.5)/1.12 * 3 - 3 & (4-2.5)/1.12 * 4 - 4 \\\\\n","      (5-2.5)/1.12 * 1 - 1 & (6-2.5)/1.12 * 2 - 2 & (7-2.5)/1.12 * 3 - 3 & (8-2.5)/1.12 * 4 - 4 \\\\\n","      (9-2.5)/1.12 * 1 - 1 & (10-2.5)/1.12 * 2 - 2 & (11-2.5)/1.12 * 3 - 3 & (12-2.5)/1.12 * 4 - 4\n","    \\end{bmatrix} , &\n","    \\begin{bmatrix}\n","      (13-14.5)/1.12 * 1 - 1 & (14-14.5)/1.12 * 2 - 2 & (15-14.5)/1.12 * 3 - 3 & (16-14.5)/1.12 * 4 - 4 \\\\\n","      (17-14.5)/1.12 * 1 - 1 & (18-14.5)/1.12 * 2 - 2 & (19-14.5)/1.12 * 3 - 3 & (20-14.5)/1.12 * 4 - 4 \\\\\n","      (21-14.5)/1.12 * 1 - 1 & (22-14.5)/1.12 * 2 - 2 & (23-14.5)/1.12 * 3 - 3 & (24-14.5)/1.12 * 4 - 4\n","    \\end{bmatrix}\n","  \\end{bmatrix} \\cdot\n","  \\begin{bmatrix}\n","    [1,2,3,4] \\\\\n","    [1,2,3,4]\n","  \\end{bmatrix} +\n","  \\begin{bmatrix}\n","    [-1,-2,-3,-4] \\\\\n","    [-1,-2,-3,-4]\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","The output of InstanceNorm is a tensor of the same shape as the input tensor $x$.\n","\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch.nn as nn\n","\n","# Define an Instance Normalization layer with the number of features (channels) as input\n","instancenorm_layer = nn.InstanceNorm2d(num_features=64)\n","\n","# Example input tensor with 64 features (e.g., a feature map in a CNN)\n","input_data = torch.randn(10, 64, 32, 32)  # Batch size=10, channels=64, height=32, width=32\n","\n","# Apply Instance Normalization to the input data\n","output = instancenorm_layer(input_data)\n","```\n","\n"],"metadata":{"id":"ar7XZMZFSb4D"}},{"cell_type":"markdown","source":["#### **Group Normalization Layer:**\n","\n","**Purpose:**\n","- Group Normalization (GroupNorm) is used to normalize activations within each group of channels.\n","\n","- It is an alternative to BatchNorm and is particularly useful when batch sizes are small or irregular.\n","**Mathematics:**\n","- The GroupNorm operation can be mathematically represented as follows:\n","  $$\\text{GroupNorm}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta $$\n","  - $x$ is the input to the GroupNorm layer.\n","  - $\\mu$ is the mean of the activations within each group of channels.\n","  - $\\sigma$ is the standard deviation of the activations within each group of channels.\n","  - $\\gamma$ is a learnable scaling parameter.\n","  - $\\beta$ is a learnable shifting parameter.\n","  \n","  GroupNorm divides channels into groups and applies normalization independently within each group.\n","\n","Here's an example to illustrate how GroupNorm works:\n","\n","Suppose we have a $2 \\times 6 \\times 4 \\times 4$ input tensor $x$ with the following values:\n","\n","\\begin{equation}\n","  x = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      \\begin{bmatrix}\n","        [1 & 2 & 3 & 4] \\\\\n","        [5 & 6 & 7 & 8] \\\\\n","        [9 & 10 & 11 & 12] \\\\\n","        [13 & 14 & 15 & 16]\n","      \\end{bmatrix}, &\n","      \\begin{bmatrix}\n","        [17 & 18 & 19 & 20] \\\\\n","        [21 & 22 & 23 & 24] \\\\\n","        [25 & 26 & 27 & 28] \\\\\n","        [29 & 30 & 31 & 32]\n","      \\end{bmatrix}, &\n","      \\begin{bmatrix}\n","        [33 & 34 & 35 & 36] \\\\\n","        [37 & 38 & 39 & 40] \\\\\n","        [41 & 42 & 43 & 44] \\\\\n","        [45 & 46 & 47 & 48]\n","      \\end{bmatrix}\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      \\begin{bmatrix}\n","        [49 & 50 & 51 & 52] \\\\\n","        [53 & 54 & 55 & 56] \\\\\n","        [57 & 58 & 59 & 60] \\\\\n","        [61 & 62 & 63 & 64]\n","      \\end{bmatrix}, &\n","      \\begin{bmatrix}\n","        [65 & 66 & 67 & 68] \\\\\n","        [69 & 70 & 71 & 72] \\\\\n","        [73 & 74 & 75 & 76] \\\\\n","        [77 & 78 & 79 & 80]\n","      \\end{bmatrix}, &\n","      \\begin{bmatrix}\n","        [81 & 82 & 83 & 84] \\\\\n","        [85 & 86 & 87 & 88] \\\\\n","        [89 & 90 & 91 & 92] \\\\\n","        [93 & 94 & 95 & 96]\n","      \\end{bmatrix}\n","    \\end{bmatrix}\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","We apply GroupNorm to $x$ with two groups of channels and learnable scaling parameter $\\gamma = [1,2]$ and learnable shifting parameter $\\beta = [-1,-2]$.\n","\n","For each group, we calculate the mean $\\mu$ and standard deviation $\\sigma$ of the activations:\n","\n","\\begin{equation}\n","   \\mu = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      [7.5,11.5,15.5,19.5] \\\\\n","      [23.5,27.5,31.5,35.5]\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      [39.5,43.5,47.5,51.5] \\\\\n","      [55.5,59.5,63.5,67.5]\n","    \\end{bmatrix}\n","   \\end{bmatrix}, \\quad\n","   \\sigma = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      [3.5,3.5,3.5,3.5] \\\\\n","      [3.5,3.5,3.5,3.5]\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      [3.5,3.5,3.5,3.5] \\\\\n","      [3.5,3.5,3.5,3.5]\n","    \\end{bmatrix}\n","   \\end{bmatrix}\n","\\end{equation}\n","\n","- Then, we apply the GroupNorm operation to $x$:\n","\n","- Note that the GroupNorm operation is applied independently to each group of channels, so the mean and standard deviation are calculated separately for each group and used to normalize the activations within that group only.\n","\n","- Also, note that the scaling and shifting parameters are learned during training and can be adjusted to control the scale and shift of the normalized activations, respectively.\n","\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch.nn as nn\n","\n","# Define a Group Normalization layer with the number of features (channels) and the number of groups as input\n","groupnorm_layer = nn.GroupNorm(num_groups=4, num_channels=64)\n","\n","# Example input tensor with 64 features (e.g., a feature map in a CNN)\n","input_data = torch.randn(10, 64, 32, 32)  # Batch size=10, channels=64, height=32, width=32\n","\n","# Apply Group Normalization to the input data\n","output = groupnorm_layer(input_data)\n","```\n","\n"],"metadata":{"id":"q5-u8fr-Z4Cc"}},{"cell_type":"markdown","source":["In both Instance Normalization and Group Normalization, the goal is to normalize activations within instances or groups of channels, respectively. These normalization techniques offer alternatives to Batch Normalization and can be beneficial in specific scenarios or network architectures. The choice between them depends on the characteristics of the data and the desired behavior of the normalization."],"metadata":{"id":"dZzpCSCdZ51l"}},{"cell_type":"markdown","source":["# Custom Layers\n"],"metadata":{"id":"_HvwzCDj8Prd"}},{"cell_type":"markdown","source":["##**Custom Layers and Modules:**\n","\n","In addition to using built-in layers, you can create custom layers or modules by defining your own classes. This is useful when you need to implement a specific operation that is not available as a pre-defined layer. To create custom layers, subclass `nn.Module` and implement the `forward` method. Here's an example of a custom layer that adds a trainable bias to the input:\n","\n","```python\n","import torch.nn.functional as F\n","\n","class CustomLayer(nn.Module):\n","    def __init__(self, input_features, output_features):\n","        super(CustomLayer, self).__init__()\n","        self.weight = nn.Parameter(torch.Tensor(input_features, output_features))\n","        self.bias = nn.Parameter(torch.Tensor(output_features))\n","\n","    def forward(self, x):\n","        return torch.matmul(x, self.weight) + self.bias\n","```\n","\n"],"metadata":{"id":"qfHA7fUBOiux"}},{"cell_type":"markdown","source":["##**Model Initialization:**\n","\n","Proper initialization of model weights is crucial for training deep neural networks. PyTorch provides default weight initialization for built-in layers. However, if you create custom layers, you should initialize the weights appropriately. Common initialization techniques include Xavier/Glorot initialization and He initialization.\n","\n","```python\n","# Example of weight initialization in a custom layer\n","import math\n","\n","class CustomLayer(nn.Module):\n","    def __init__(self, input_features, output_features):\n","        super(CustomLayer, self).__init__()\n","        self.weight = nn.Parameter(torch.Tensor(input_features, output_features))\n","        self.bias = nn.Parameter(torch.Tensor(output_features))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.init.xavier_uniform_(self.weight)\n","        nn.init.zeros_(self.bias)\n","```\n","\n"],"metadata":{"id":"JIDsTdzYadE4"}},{"cell_type":"markdown","source":["# Atention Mechanism"],"metadata":{"id":"ycBjtotTUy7q"}},{"cell_type":"markdown","source":["## **Introduction**\n","\n","Attention Mechanism is a technique used in deep learning and natural language processing to focus on specific parts of an input sequence. It allows the model to selectively attend to different parts of the sequence and weigh their importance when generating an output.\n","\n","\n","1. Attention Mechanism:\n","   The attention mechanism allows the model to focus on different parts of the input sequence when processing each token. It assigns weights or importance scores to each token in the sequence based on its relevance to the current token being processed. These weights are then used to compute a weighted sum of the token embeddings, which serves as the contextual representation for the current token.\n","\n","2. Single-Head Attention:\n","   In a traditional attention mechanism, a single attention head is used to compute the weights and the weighted sum. This single-head attention computes the attention weights by calculating the compatibility between the query, key, and value vectors. The query vector represents the current token, while the key and value vectors represent all the tokens in the sequence. The weights are computed using a similarity measure, such as dot product or scaled dot product, between the query and key vectors. The weighted sum is then computed by multiplying the attention weights with the value vectors.\n","\n","3. Multi-Head Attention:\n","   The Multi-Head Attention extends the single-head attention by using multiple attention heads in parallel. Each attention head has its own set of query, key, and value vectors, which are learned during the training process. By using multiple attention heads, the model can capture different types of relationships or dependencies between tokens in the sequence. The outputs of the attention heads are then concatenated and linearly transformed to obtain the final output.\n","\n","4. Self-Attention vs. Cross-Attention:\n","   In the context of transformer models, the Multi-Attention head can be used for both self-attention and cross-attention. Self-attention refers to attending to different positions in the same input sequence, while cross-attention refers to attending to different positions in two different input sequences. For example, in machine translation, self-attention is used to capture dependencies within the source sentence, while cross-attention is used to align the source and target sentences.\n","\n","5. Positional Encoding:\n","   To incorporate positional information into the attention mechanism, transformer models use positional encoding. Positional encoding is added to the input embeddings to provide the model with information about the order or position of tokens in the sequence. This allows the attention mechanism to differentiate between tokens based on their positions, in addition to their content.\n","\n","Overall, the Multi-Attention head in transformer models enables the model to capture complex relationships between tokens in a sequence by using multiple attention heads. This helps in capturing long-range dependencies, modeling context, and achieving strong performance in various natural language processing tasks."],"metadata":{"id":"QAIeWiq9efFc"}},{"cell_type":"markdown","source":["Sure, here's a table summarizing all the different types of attention layers in AI that we've discussed:\n","\n","| Type of Attention Layer | Description |\n","| --- | --- |\n","| Dot-Product Attention | Calculates the similarity between query and key vectors by taking their dot product |\n","| Additive Attention | Calculates the similarity between query and key vectors by concatenating them and passing through a neural network |\n","| Scaled Dot-Product Attention | Similar to dot-product attention, but the dot product is scaled by the square root of the dimensionality of the key vectors |\n","| Multi-Head Attention | Applies several attention mechanisms in parallel, each with a different set of learned query, key, and value matrices |\n","| Self-Attention | Attends to the same input sequence for query, key, and value vectors |\n","| Local Attention | Attends to a subset of the input sequence, rather than the entire sequence |\n","| Global Attention | Attends to the entire input sequence, assigning weights based on relevance to the query |\n","| Content-Based Attention | Calculates similarity based on content rather than position in input sequence |\n","| Location-Based Attention | Calculates similarity based on position in input sequence rather than content |\n","| Hierarchical Attention | Applies attention at multiple levels of granularity for hierarchical data structures |\n","| Time-Based Attention | Applies attention over time steps for sequential data models |\n","| Channel-Wise Attention | Applies attention to different channels or feature maps in computer vision tasks |\n","| Set Attention | Allows attending to different elements within an unordered set |\n","| Masked Attention | Handles variable-length sequences or missing data by applying a mask to attention weights |\n","| Convolutional Attention | Applies convolutional operations to capture local dependencies and extract relevant features within the input sequence |\n","| Sparse Attention | Focuses on attending to a small subset of elements within the input sequence |\n","| Monotonic Attention | Aligns with the input sequence in a monotonic manner to handle sequential data |\n","| Structured Attention | Attends to structured or graph-like data, capturing dependencies between different elements or nodes |\n","| Dynamic Attention | Dynamically adjusts the attention weights during inference based on the context or input |\n","| Masked Language Modeling Attention | Attends to masked tokens to predict missing words or tokens in the input sequence |\n","| Feedback Attention | Adds a feedback loop to the attention mechanism, allowing the model to iteratively refine its attention weights based on previous predictions |\n","| Task-Specific Attention | Incorporates task-specific information into the attention mechanism |\n","| Multi-Modal Attention | Combines information from multiple modalities, such as text, image, and audio, to attend to relevant features across different modalities |\n","| Graph Attention | Applies attention mechanisms to graph-structured data, allowing the model to learn graph representations and capture dependencies between nodes |\n","| Memory Attention | Uses external memory to store and retrieve relevant information during inference |\n","| Dual Attention | Attends to both inputs simultaneously |\n","| Multi-Scale Attention | Applies attention mechanisms at multiple scales or resolutions |\n","| Cooperative Attention | Has multiple attention mechanisms working together to attend to different aspects of the input sequence |\n","| Spatial Attention | Focuses on specific regions or patches of an image |\n","| Hard Attention | Selects a single element from the input sequence to attend to |\n","| Kernel Attention | Applies kernel functions to the query and key vectors to calculate the similarity between them |\n","| Fine-Grained Attention | Applies attention mechanisms to individual components or sub-components of the input sequence |\n","| Multi-View Attention | Attends to relevant features across different views of the input data |\n","| Dynamic Convolutional Attention | Applies convolutional operations with dynamic filters to capture temporal dependencies and extract relevant features in sequential data |\n","| Spatial-Temporal Attention | Attends to relevant features across both spatial and temporal dimensions of the input data |\n","| Tree-Structured Attention | Attends to relevant nodes and edges in tree-structured data, such as parse trees or dependency trees |\n","| Multi-Relational Attention | Attends to different types of relations between entities |\n","| Co-Attention | Attends to relevant features across multiple input modalities simultaneously |\n","| Generative Attention | Guides the generation process by attending to relevant features in the input sequence |\n","| Multi-Step Attention | Applies attention mechanisms at multiple steps or stages of a model |\n","| Multi-Head Multi-Relational Attention | Combines multi-head attention and multi-relational attention |\n","| Contrastive Attention | Compares two input sequences or representations by attending to differences and similarities between them |\n","| Global-Local Attention | Attends to both global and local features of the input sequence |\n","| Adaptive Attention | Dynamically adjusts the attention weights during training based on the task and data |\n","| Cross-Modal Attention | Attends to relevant features across different input modalities |\n","| Spatially Adaptive Attention | Adjusts the size and position of the attended region based on the input image |\n","| Multi-Level Attention | Applies attention mechanisms at multiple levels of abstraction |\n","| Dual-Stream Attention | Attends to relevant features in two parallel input streams simultaneously |\n","| Interactive Attention | Attends to relevant information based on user input or actions in interactive or dynamic components |\n","| Multi-Task Attention | Attends to relevant features for multiple tasks simultaneously |\n","\n","I hope you find this table helpful!"],"metadata":{"id":"Sc09u_KYrQcb"}},{"cell_type":"markdown","source":["## Attention Mechanism"],"metadata":{"id":"StKRYgC7eDzs"}},{"cell_type":"markdown","source":["Certainly, let's explore the Attention Mechanism in more detail, which is a fundamental component of many deep learning models, including Transformers:\n","\n","**Attention Mechanism:**\n","\n","**Purpose:**\n","- The Attention Mechanism is a computational operation that allows models to focus on different parts of input sequences or feature maps with varying degrees of importance. It is widely used in natural language processing, computer vision, and other domains to capture dependencies and relationships between elements.\n","\n","**Key Components:**\n","1. **Queries, Keys, and Values:** The Attention Mechanism operates on three sets of vectors: queries, keys, and values. These vectors are typically derived from the input data.\n","\n","2. **Attention Scores:** Attention scores are computed between the queries and keys. These scores represent the similarity or compatibility between each query and key pair.\n","\n","3. **Attention Weights:** The attention scores are transformed into attention weights using a softmax operation. These weights determine how much attention each element in the input should pay to other elements.\n","\n","4. **Weighted Sum of Values:** The attention weights are used to compute a weighted sum of the values. This weighted sum is the output of the attention mechanism.\n","\n","**Mathematics:**\n","- The Attention Mechanism can be mathematically represented as follows:\n","  \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V \\]\n","  - \\(Q\\) represents the queries.\n","  - \\(K\\) represents the keys.\n","  - \\(V\\) represents the values.\n","  - \\(d_k\\) is the dimension of the key vectors.\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn.functional as F\n","\n","class Attention(nn.Module):\n","    def __init__(self, d_model):\n","        super(Attention, self).__init__()\n","        self.d_model = d_model\n","\n","    def forward(self, Q, K, V, mask=None):\n","        # Compute attention scores\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_model, dtype=Q.dtype))\n","        \n","        # Apply mask (optional)\n","        if mask is not None:\n","            scores.masked_fill_(mask == 0, float('-inf'))\n","        \n","        # Compute attention weights using softmax\n","        attn_weights = F.softmax(scores, dim=-1)\n","        \n","        # Weighted sum of values\n","        attn_output = torch.matmul(attn_weights, V)\n","        \n","        return attn_output\n","```\n","\n","In the example code above, we define a simplified Attention module in PyTorch. This module takes queries, keys, and values as input and computes attention scores, attention weights, and the final output.\n","\n","The Attention Mechanism is a versatile component used in various deep learning architectures to capture dependencies and relationships within sequences and feature maps. It plays a crucial role in tasks such as machine translation, text summarization, image captioning, and more."],"metadata":{"id":"HBToycPeeMqM"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Example_not_Sorted</summary>\n","**Mathematics behind Attention Mechanism**\n","\n","The mathematics behind Attention Mechanism involves computing a set of attention weights for each element in the input sequence. These attention weights are used to compute a weighted sum of the elements, which is then used to generate the output.\n","\n","**Example: Machine Translation**\n","\n","To illustrate how Attention Mechanism works, let's take an example of machine translation. Suppose we have an input sequence X = {x1, x2, ..., xn} in the source language, and we want to translate it into the target language Y = {y1, y2, ..., ym}. We can represent the input sequence using a matrix X with shape (n, d), where d is the dimension of each element in the sequence.\n","\n","**Computing Attention Weights**\n","\n","To generate each element in the target sequence, we first compute a set of attention weights for each element in the input sequence. Let's call these weights A = {a1, a2, ..., an}. We can compute these weights using a feedforward neural network that takes as input the current state of the decoder and the encoded input sequence X.\n","\n","**Computing Weighted Sum**\n","\n","The attention weights are then used to compute a weighted sum of the elements in the input sequence, which is then used as input to the decoder. The weighted sum is computed as:\n","\n","C = sum(ai * xi) for i = 1 to n\n","\n","where ai is the attention weight for element xi in the input sequence. This weighted sum C represents the context vector, which captures the most important parts of the input sequence for generating the current output.\n","\n","**Generating Output**\n","\n","The context vector C is then concatenated with the current state of the decoder and fed into another feedforward neural network to generate the output element yj.\n","\n","**Conclusion**\n","\n","This is just a simple example of how Attention Mechanism works in machine translation. There are many variations and extensions of this technique used in different NLP tasks. The key idea behind Attention Mechanism is to focus on specific parts of the input sequence based on their relevance to the current task, rather than treating all elements equally.\n","\n","**Example Detialed:**\n","\n","In Attention Mechanism, we compute a set of attention weights for each element in the input sequence. These attention weights are used to compute a weighted sum of the elements, which is then used to generate the output.\n","\n","Let's say we have an input sequence X = {x1, x2, ..., xn} represented as a matrix X with shape (n, d), where d is the dimension of each element in the sequence. We want to compute the attention weights for each element in the input sequence.\n","\n","We can do this by first computing a set of scores for each element in the input sequence. The scores are computed by taking the dot product of each element with a learnable parameter vector v:\n","\n","s_i = v^T * tanh(W_1 * h_i + b_1)\n","\n","where h_i is the hidden state of the model at time step i, W_1 and b_1 are learnable parameters of the model, and tanh is the hyperbolic tangent activation function.\n","\n","We then apply a softmax function to the scores to obtain a set of attention weights:\n","\n","a_i = exp(s_i) / sum(exp(s_j)) for j = 1 to n\n","\n","where exp is the exponential function and sum(exp(s_j)) is the sum of exponential scores over all elements in the input sequence.\n","\n","These attention weights represent the importance of each element in the input sequence for generating the current output.\n","\n","We can then compute a weighted sum of the elements in the input sequence using these attention weights:\n","\n","c = sum(a_i * x_i) for i = 1 to n\n","\n","where x_i is the i-th element in the input sequence.\n","\n","This weighted sum c represents the context vector, which captures the most important parts of the input sequence for generating the current output.\n","\n","The context vector c can then be concatenated with the current state of the model and fed into another neural network to generate the output.\n","\n","This is just a simple example of how Attention Mechanism works. There are many variations and extensions of this technique used in different NLP tasks. The key idea behind Attention Mechanism is to focus on specific parts of the input sequence based on their relevance to the current task, rather than treating all elements equally.\n","</details>\n","\n","\n","\n"],"metadata":{"id":"yQKKndMlVdu2"}},{"cell_type":"markdown","source":["Yes, there are different types of attention mechanisms used in deep learning and natural language processing. Some of the commonly used types include:\n","\n","1. Self-Attention: This type of attention mechanism is used to capture dependencies between different words in a sentence. It allows the model to attend to different parts of the input sequence and identify important words or phrases.\n","\n","2. Global Attention: In this type of attention mechanism, the model attends to all the elements in the input sequence to generate the output. This is useful when the entire input sequence is relevant to the output.\n","\n","3. Local Attention: This type of attention mechanism focuses on a specific region of the input sequence, rather than attending to all elements. This can be useful when only a subset of the input sequence is relevant to the output.\n","\n","4. Dot-Product Attention: This type of attention mechanism computes the similarity between two vectors using the dot product operation. It is commonly used in self-attention mechanisms.\n","\n","5. Additive Attention: In this type of attention mechanism, the similarity between two vectors is computed using a feedforward neural network. It can be used for both self-attention and global attention.\n","\n","These are just a few examples of the different types of attention mechanisms used in deep learning and NLP. The choice of attention mechanism depends on the specific task and the characteristics of the input sequence."],"metadata":{"id":"Bx1oo0N8XsTB"}},{"cell_type":"markdown","source":["Sure! Here are the equations in markdown syntax:\n","\n","The scores for each element in the input sequence are computed as:\n","\n","![equation](https://latex.codecogs.com/png.latex?s_i%20%3D%20v%5ET%20%5Ctanh%28W_1%20h_i%20%2B%20b_1%29)\n","\n","The attention weights are then obtained by applying the softmax function to the scores:\n","\n","![equation](https://latex.codecogs.com/png.latex?a_i%20%3D%20%5Cfrac%7Be%5E%7Bs_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5En%20e%5E%7Bs_j%7D%7D)\n","\n","The weighted sum of the elements in the input sequence is computed as:\n","\n","![equation](https://latex.codecogs.com/png.latex?c%20%3D%20%5Csum_%7Bi%3D1%7D%5En%20a_i%20x_i)\n","\n","These equations represent the mathematics behind the Attention Mechanism."],"metadata":{"id":"GHQ-VkQLXPvB"}},{"cell_type":"markdown","source":["## Single Head Attention Mechanism"],"metadata":{"id":"h8EKFOaCdugL"}},{"cell_type":"markdown","source":["Certainly, let's explore the Single-Head Attention mechanism, which is a fundamental component of multi-head attention used in models like Transformers:\n","\n","**Single-Head Attention:**\n","\n","**Purpose:**\n","- Single-Head Attention is a mechanism used to capture dependencies and relationships between elements in a sequence. It allows the model to weigh the importance of different elements when making predictions for a particular element.\n","\n","**Key Components:**\n","1. **Queries, Keys, and Values:** Single-Head Attention operates on three sets of vectors: queries, keys, and values. These vectors are linear projections of the input sequence.\n","\n","2. **Attention Scores:** Attention scores are computed between the queries and keys. These scores determine how much attention each element in the sequence should pay to other elements.\n","\n","3. **Attention Weights:** The attention scores are transformed into attention weights using a softmax operation. These weights represent the importance of each element when computing the output.\n","\n","4. **Weighted Sum of Values:** The attention weights are used to compute a weighted sum of the values. This weighted sum is the output of the attention mechanism.\n","\n","**Mathematics:**\n","- The Single-Head Attention operation can be mathematically represented as follows:\n","  \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V \\]\n","  - \\(Q\\) represents the queries.\n","  - \\(K\\) represents the keys.\n","  - \\(V\\) represents the values.\n","  - \\(d_k\\) is the dimension of the key vectors.\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn.functional as F\n","\n","class SingleHeadAttention(nn.Module):\n","    def __init__(self, d_model):\n","        super(SingleHeadAttention, self).__init__()\n","        self.d_model = d_model\n","\n","    def forward(self, Q, K, V, mask=None):\n","        # Compute attention scores\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_model, dtype=Q.dtype))\n","        \n","        # Apply mask (optional)\n","        if mask is not None:\n","            scores.masked_fill_(mask == 0, float('-inf'))\n","        \n","        # Compute attention weights using softmax\n","        attn_weights = F.softmax(scores, dim=-1)\n","        \n","        # Weighted sum of values\n","        attn_output = torch.matmul(attn_weights, V)\n","        \n","        return attn_output\n","```\n","\n","In the example code above, we define a simplified Single-Head Attention module in PyTorch. This module takes queries, keys, and values as input and computes attention scores, attention weights, and the final output.\n","\n","Single-Head Attention is a fundamental building block used in multi-head attention mechanisms, and it plays a crucial role in capturing dependencies and relationships within sequences. It is used extensively in models like Transformers for various natural language processing tasks, including machine translation and text understanding."],"metadata":{"id":"5WfpQ5gtdzla"}},{"cell_type":"markdown","source":["## Multiattention Head Layer"],"metadata":{"id":"WBIS_V58QERL"}},{"cell_type":"markdown","source":["MultiHeadAttention is a mechanism commonly used in the field of Natural Language Processing (NLP) and deep learning. It is a type of attention mechanism that allows a model to focus on different parts of the input sequence simultaneously.\n","\n","In MultiHeadAttention, the input is split into multiple \"heads,\" and each head learns its own attention weights. These attention weights determine the importance of each element in the input sequence for a given task. By using multiple heads, the model can capture different types of dependencies and relationships within the input.\n","\n","The outputs of the multiple heads are then concatenated and linearly transformed to produce the final output. This allows the model to attend to different aspects of the input sequence and capture more complex patterns.\n","\n","MultiHeadAttention has been widely used in various NLP tasks, such as machine translation, text summarization, and question answering, where capturing long-range dependencies is crucial for achieving good performance."],"metadata":{"id":"MawBvPFvXz_d"}},{"cell_type":"markdown","source":["Certainly, let's explore the Multi-Head Attention mechanism, which is a key component in the Transformer architecture used for various natural language processing tasks:\n","\n","**Multi-Head Attention Layer:**\n","\n","**Purpose:**\n","- The Multi-Head Attention mechanism is used to capture different types of dependencies between elements in a sequence or set of vectors. It allows the model to focus on different parts of the input sequence simultaneously, making it powerful for capturing long-range dependencies.\n","\n","**Key Components:**\n","1. **Linear Projections:** In a multi-head attention layer, the input is projected into multiple subspaces using learnable linear transformations. These linear projections allow the model to learn different aspects of the input sequence.\n","\n","2. **Scaled Dot-Product Attention:** Within each attention head, the model computes attention scores between elements in the input sequence. The attention scores are used to weigh the importance of each element when computing the output.\n","\n","3. **Concatenation and Linear Projection:** The outputs from multiple attention heads are concatenated and projected again to create the final multi-head attention output.\n","\n","**Mathematics:**\n","- The Multi-Head Attention operation can be mathematically represented as follows:\n","  $$\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) \\cdot W^O $$\n","  - $Q$, $K$, and $V$ are the input queries, keys, and values.\n","  - $\\text{head}_i$ represents the output of the $i$-th attention head.\n","  - $W^O$ is a learnable linear projection matrix.\n","  \n","Multi-Head Attention is a mechanism that allows a model to attend to different parts of the input representation with different attention weights learned from the data. It computes multiple attention distributions in parallel, each of which attends to a different part of the input. These attention distributions are concatenated and then linearly transformed to produce the final output.\n","\n","**Here's an example to illustrate how Multi-Head Attention works:**\n","\n","Suppose we have a $3 \\times 4 \\times 5$ input tensor $X$ and we want to compute the Multi-Head Attention with $h=2$ heads and $d_k=d_v=2$:\n","\n","\\begin{equation}\n","  X = \\begin{bmatrix}\n","    \\begin{bmatrix}\n","      [-1 & 2 & -3 & 4 & -5] \\\\\n","      [6 & -7 & 8 & -9 & 10] \\\\\n","      [-11 & 12 & -13 & 14 & -15] \\\\\n","      [16 & -17 & 18 & -19 & 20]\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      [21 & -22 & 23 & -24 & 25] \\\\\n","      [-26 & 27 & -28 & 29 & -30] \\\\\n","      [31 & -32 & 33 & -34 & 35] \\\\\n","      [-36 & 37 & -38 & 39 & -40]\n","    \\end{bmatrix}, &\n","    \\begin{bmatrix}\n","      [41 & -42 & 43 & -44 & 45] \\\\\n","      [-46 & 47 & -48 & 49 & -50] \\\\\n","      [51 & -52 & 53 & -54 & 55] \\\\\n","      [-56 & 57 & -58 & 59 & -60]\n","    \\end{bmatrix}\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","We first project the queries, keys, and values into $d_k=d_v=2$-dimensional spaces:\n","\n","\\begin{equation}\n","  Q = W_Q X, \\quad K = W_K X, \\quad V = W_V X\n","\\end{equation}\n","\n","where $W_Q$, $W_K$, and $W_V$ are learnable projection matrices of size $2 \\times 5$. Suppose we have the following values for these matrices:\n","\n","\\begin{equation}\n","  W_Q = \\begin{bmatrix}\n","    [0.1, -0.2, 0.3, -0.4, 0.5] \\\\\n","    [-0.6, 0.7, -0.8, 0.9, -1.0]\n","  \\end{bmatrix}, \\quad\n","  W_K = \\begin{bmatrix}\n","    [-1.1, 1.2, -1.3, 1.4, -1.5] \\\\\n","    [1.6, -1.7, 1.8, -1.9, 2.0]\n","  \\end{bmatrix}, \\quad\n","  W_V = \\begin{bmatrix}\n","    [2.1, -2.2, 2.3, -2.4, 2.5] \\\\\n","    [-2.6, 2.7, -2.8, 2.9, -3.0]\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","Then we split the projected queries, keys, and values into $h=2$ heads along the feature dimension:\n","\n","\\begin{equation}\n","  Q_i = Q W_Q^i, \\quad K_i = K W_K^i, \\quad V_i = V W_V^i\n","\\end{equation}\n","\n","where $W_Q^i$, $W_K^i$, and $W_V^i$ are learnable projection matrices of size $2 \\times (5/h)$ for each head $i$. Suppose we have the following values for these matrices:\n","\n","\\begin{equation}\n","  W_Q^1 = \\begin{bmatrix}\n","    [0.1, -0.2] \\\\\n","    [-0.6, 0.7]\n","  \\end{bmatrix}, \\quad\n","  W_K^1 = \\begin{bmatrix}\n","    [-1.1, 1.2] \\\\\n","    [1.6, -1.7]\n","  \\end{bmatrix}, \\quad\n","  W_V^1 = \\begin{bmatrix}\n","    [2.1, -2.2] \\\\\n","    [-2.6, 2.7]\n","  \\end{bmatrix}, \\\\\n","\\end{equation}\n","\n","\\begin{equation}\n","  W_Q^2 = \\begin{bmatrix}\n","    [0.3, -0.4] \\\\\n","    [-0.8, 0.9]\n","  \\end{bmatrix}, \\quad\n","  W_K^2 = \\begin{bmatrix}\n","    [-1.3, 1.4] \\\\\n","    [1.8, -1.9]\n","  \\end{bmatrix}, \\quad\n","  W_V^2 = \\begin{bmatrix}\n","    [2.3, -2.4] \\\\\n","    [-2.8, 2.9]\n","  \\end{bmatrix}\n","\\end{equation}\n","\n","We then compute the attention scores and attention distributions for each head:\n","\n","\\begin{equation}\n","  A_i = {\\rm softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right), \\quad H_i = A_i V_i\n","\\end{equation}\n","\n","where $\\sqrt{d_k}=1$ in this example for simplicity.\n","\n","Finally, we concatenate the output of each head along the feature dimension and apply a linear transformation to obtain the final output:\n","\n","\\begin{equation}\n","  {\\rm MultiHeadAttention}(Q,K,V) = {\\rm Concat}(H_1,H_2) W^O\n","\\end{equation}\n","\n","where $W^O$ is a learnable projection matrix of size $(d_v h) \\times d_o$.\n","\n","**Example Code in PyTorch:**\n","```python\n","import torch\n","import torch.nn as nn\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"Number of heads must evenly divide the model dimension.\"\n","        self.d_head = d_model // num_heads\n","        self.num_heads = num_heads\n","        self.linear_q = nn.Linear(d_model, d_model)\n","        self.linear_k = nn.Linear(d_model, d_model)\n","        self.linear_v = nn.Linear(d_model, d_model)\n","        self.linear_out = nn.Linear(d_model, d_model)\n","\n","    def forward(self, Q, K, V):\n","        # Linear projections for queries, keys, and values\n","        Q = self.linear_q(Q)\n","        K = self.linear_k(K)\n","        V = self.linear_v(V)\n","\n","        # Split into multiple heads\n","        Q = self._split_heads(Q)\n","        K = self._split_heads(K)\n","        V = self._split_heads(V)\n","\n","        # Scaled Dot-Product Attention for each head\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_head, dtype=Q.dtype))\n","        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n","        attn_output = torch.matmul(attn_weights, V)\n","\n","        # Concatenate and linear projection\n","        attn_output = self._combine_heads(attn_output)\n","        attn_output = self.linear_out(attn_output)\n","\n","        return attn_output\n","\n","    def _split_heads(self, x):\n","        # Split x into multiple heads and transpose for batch and head dimensions\n","        batch_size, seq_len, _ = x.size()\n","        x = x.view(batch_size, seq_len, self.num_heads, self.d_head).transpose(1, 2)\n","        return x\n","\n","    def _combine_heads(self, x):\n","        # Transpose back and reshape\n","        batch_size, _, seq_len, _ = x.size()\n","        x = x.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n","        return x\n","```\n","\n","In the example code above, we define a simplified Multi-Head Attention module in PyTorch. This module performs multi-head attention by projecting input queries, keys, and values, computing attention scores, and combining the outputs from different attention heads.\n","\n","In summary, the Multi-Head Attention mechanism is a crucial component in the Transformer architecture, enabling the model to capture various types of dependencies and relationships within a sequence or set of vectors. It has been highly successful in natural language processing tasks, including machine translation and text understanding."],"metadata":{"id":"wC4wEh3ide_l"}},{"cell_type":"markdown","source":["Sure, here's an example of the operation result on a matrix:\n","\n","Let's say our input matrix X is:\n","\n","```\n","[[1, 2, 3],\n"," [4, 5, 6],\n"," [7, 8, 9],\n"," [10, 11, 12]]\n","```\n","\n","We want to apply MultiHeadAttention with 2 heads and a hidden dimension of 2. We first split the input into 2 heads of shape (4, 1) each:\n","\n","```\n","X_head1 = [[1], [4], [7], [10]]\n","X_head2 = [[2], [5], [8], [11]]\n","```\n","\n","We then apply self-attention to each head separately. Let's say the attention weights for head 1 are:\n","\n","```\n","[[0.2, 0.3, 0.1, 0.4],\n"," [0.3, 0.2, 0.1, 0.4],\n"," [0.1, 0.3, 0.2, 0.4],\n"," [0.4, 0.2, 0.1, 0.3]]\n","```\n","\n","And the attention weights for head 2 are:\n","\n","```\n","[[0.3, 0.4, 0.1, 0.2],\n"," [0.4, 0.3, 0.1, 0.2],\n"," [0.1, 0.4, 0.3, 0.2],\n"," [0.2, 0.3, 0.1, 0.4]]\n","```\n","\n","We then concatenate the two sets of attention weights along the last dimension to obtain a tensor of shape (4, 4, 2):\n","\n","```\n","[[[0.2, 0.3],\n","  [0.3, 0.4],\n","  [0.1, 0.1],\n","  [0.4, 0.2]],\n","\n"," [[0.3, 0.4],\n","  [0.2, 0.3],\n","  [0.3, 0.4],\n","  [0.2, 0.3]],\n","\n"," [[0.1, 0.1],\n","  [0.1, 0.1],\n","  [0.2, 0.3],\n","  [0.1, 0.1]],\n","\n"," [[0.4, 0.2],\n","  [0.4, 0.2],\n","  [0.4, 0.2],\n","  [0.3, 0.4]]]\n","```\n","\n","We then multiply this tensor element-wise with the original input sequence X:\n","\n","```\n","[[[0.2,   0.6,    0.3],\n","   [1.2,   2.0,    2.4],\n","   [0.7,   2.4,    1.8],\n","   [4.0,   2.2,    3.6]],\n","\n"," [[1.2,   1.6,    1.8],\n","   [2.8,   3.5,    4.8],\n","   [2.1,   3.2,    4.5],\n","   [2.0,   3.3,    5.5]],\n","\n"," [[0.7,   1.6,    1.8],\n","   [1.6,   2.5,    3.6],\n","   [2.8,   3.2,    4.5],\n","   [1.1,   2.8,    3.6]],\n","\n"," [[4.0,   2.2,    3.6],\n","   [4.5,   2.8,    4.8],\n","   [6.3,   7.2,    8.1],\n","   [3.6,   5.5,    7.9]]]\n","```\n","\n","Finally we sum along the second dimension to obtain a tensor of shape (4 ,2):\n","\n","```\n","[[5.,     8.,]\n"," [9.,     13.,]\n"," [6.,     12.,]\n"," [9.,     16.,]]\n","```\n","\n","This is the output of the MultiHeadAttention layer for this input matrix X with the specified parameters of heads=2 and hidden dimension=2"],"metadata":{"id":"MQlXAhC3X-qT"}},{"cell_type":"code","source":[],"metadata":{"id":"h3SDWBamdeMx"},"execution_count":null,"outputs":[]}]}