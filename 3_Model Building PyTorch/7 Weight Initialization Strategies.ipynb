{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSy3c5kJr2NrbyktP/DnBa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#<font color='blue' size='5px'/> Weight Initialization Strategies<font/>\n","\n","\n","\n"],"metadata":{"id":"kiuuArcFosDQ"}},{"cell_type":"markdown","source":["Weight initialization strategies are critical in training deep neural networks. Properly initialized weights can accelerate convergence, mitigate vanishing or exploding gradient problems, and improve overall training stability. In this explanation, I'll describe several weight initialization strategies, provide mathematical equations, and explain their significance.\n","\n","### Random Initialization (Uniform or Normal Distribution):\n","\n","**Purpose:** Random initialization sets the initial weights with random values drawn from a specified distribution. Common distributions include the uniform and normal (Gaussian) distributions.\n","\n","**Mathematics (Uniform Distribution):**\n","In the uniform distribution, weights are initialized from a uniform distribution within a specified range:\n","\n","\\[\n","W_{ij} \\sim U(a, b)\n","\\]\n","\n","- \\(W_{ij}\\) represents the weight connecting neuron \\(i\\) in the current layer to neuron \\(j\\) in the next layer.\n","- \\(U(a, b)\\) denotes the uniform distribution in the interval \\([a, b]\\).\n","\n","**Mathematics (Normal Distribution):**\n","In the normal distribution, weights are initialized from a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n","\n","\\[\n","W_{ij} \\sim \\mathcal{N}(\\mu, \\sigma^2)\n","\\]\n","\n","- \\(W_{ij}\\) represents the weight connecting neuron \\(i\\) in the current layer to neuron \\(j\\) in the next layer.\n","- \\(\\mathcal{N}(\\mu, \\sigma^2)\\) denotes the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n","\n","### Xavier/Glorot Initialization:\n","\n","**Purpose:** Xavier/Glorot initialization is designed to address the vanishing/exploding gradient problem in deep networks. It sets the initial weights to values that help stabilize the training process.\n","\n","**Mathematics (Xavier Initialization for Sigmoid/Tanh Activation):**\n","For activation functions like sigmoid or hyperbolic tangent (tanh), Xavier initialization sets weights with mean \\(0\\) and variance \\(\\frac{1}{n}\\), where \\(n\\) is the number of input neurons to the layer:\n","\n","\\[\n","W_{ij} \\sim \\mathcal{N}(0, \\frac{1}{n})\n","\\]\n","\n","**Mathematics (Xavier Initialization for ReLU Activation):**\n","For ReLU (Rectified Linear Unit) activation, Xavier initialization sets weights with mean \\(0\\) and variance \\(\\frac{2}{n}\\) to account for the fact that ReLU neurons only activate for positive inputs:\n","\n","\\[\n","W_{ij} \\sim \\mathcal{N}(0, \\frac{2}{n})\n","\\]\n","\n","### He Initialization:\n","\n","**Purpose:** He initialization is designed for ReLU and its variants. It helps prevent dead neurons by initializing weights that maintain a higher variance.\n","\n","**Mathematics (He Initialization):**\n","For ReLU and similar activations, He initialization sets weights with mean \\(0\\) and variance \\(\\frac{2}{n}\\), where \\(n\\) is the number of input neurons to the layer:\n","\n","\\[\n","W_{ij} \\sim \\mathcal{N}(0, \\frac{2}{n})\n","\\]\n","\n","### LeCun Initialization:\n","\n","**Purpose:** LeCun initialization is designed for specific activation functions like Leaky ReLU. It accounts for the slope of the activation function to ensure stable training.\n","\n","**Mathematics (LeCun Initialization for Leaky ReLU):**\n","For Leaky ReLU activation with a negative slope \\(a\\), LeCun initialization sets weights with mean \\(0\\) and variance \\(\\frac{1}{n}\\), where \\(n\\) is the number of input neurons to the layer:\n","\n","\\[\n","W_{ij} \\sim \\mathcal{N}(0, \\frac{1}{n})\n","\\]\n","\n","These weight initialization strategies help initialize neural network weights effectively, improving the chances of successful training and convergence. The choice of initialization depends on the activation functions used and the specific problem. Proper weight initialization is a fundamental step in building and training deep neural networks."],"metadata":{"id":"kf7H6om3Yl1t"}},{"cell_type":"markdown","source":["3. **Weight Initialization:**\n","   - **Purpose:** Proper weight initialization helps prevent issues like vanishing or exploding gradients at the beginning of training.\n","   - **Mathematics:** Weight initialization methods set initial weights to small random values, typically drawn from a Gaussian or uniform distribution, with the variance or range adjusted based on the layer's activation function.\n","   - **Examples:** Xavier/Glorot initialization, He initialization.\n","\n","."],"metadata":{"id":"QrCl2r6eVSAp"}}]}