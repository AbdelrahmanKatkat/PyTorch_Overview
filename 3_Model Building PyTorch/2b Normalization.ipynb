{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+SetWNxGmQpt3SLNinP8P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#<font color='blue' size='5px'/> Normalization<font/>\n","\n","\n","\n"],"metadata":{"id":"kiuuArcFosDQ"}},{"cell_type":"markdown","source":["\n","5. **Batch Normalization:**\n","   - **Purpose:** Batch normalization helps stabilize training by normalizing the activations in each layer, reducing internal covariate shift.\n","   - **Mathematics:** Batch normalization normalizes the activations in a mini-batch, followed by scaling and shifting using learned parameters \\(\\gamma\\) and \\(\\beta\\).\n","   - **Mathematics of Batch Normalization:** Given activations \\(x\\) in a mini-batch, the batch normalization operation is:\n","     \\[ \\text{BN}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta \\]\n","     where\n","     - \\(\\text{BN}(x)\\) is the normalized output.\n","     - \\(\\gamma\\) and \\(\\beta\\) are learnable scale and shift parameters.\n","     - \\(\\mu\\) and \\(\\sigma^2\\) are the mini-batch mean and variance.\n","     - \\(\\epsilon\\) is a small constant for numerical stability."],"metadata":{"id":"De_4kFL1WpYi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbjX_UexkMrj"},"outputs":[],"source":[]}]}