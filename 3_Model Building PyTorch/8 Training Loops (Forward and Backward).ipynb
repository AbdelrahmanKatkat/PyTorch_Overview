{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMyiyylG9BMj8rcuTRwcIx2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#<font color='blue' size='5px'/> Training Loops (Forward and Backward)<font/>\n","\n","\n","\n"],"metadata":{"id":"kiuuArcFosDQ"}},{"cell_type":"markdown","source":["Hyperparameters are essentially any parameters that are set before training and are not learned during training. This includes things like the training loop (forward and backward passes), optimization techniques such as gradient descent, learning rates, weight initialization strategies, and regularization techniques"],"metadata":{"id":"OxZvMqBopHup"}},{"cell_type":"markdown","source":["## Introduction"],"metadata":{"id":"iXZ7Ju8Ytnkd"}},{"cell_type":"markdown","source":["Training loops in PyTorch involve two main phases: the forward pass and the backward pass (also known as backpropagation). These loops are crucial for training neural networks.\n","\n","**Training Loop Overview:**\n","\n","The training loop is the core of training neural networks. It consists of the following main steps:\n","\n","1. **Forward Pass:** In this phase, input data is passed through the network to compute predictions (forward propagation).\n","2. **Loss Calculation:** The predictions are compared to the ground truth to calculate a loss, which measures how far off the predictions are from the actual values.\n","3. **Backward Pass (Backpropagation):** The gradients of the loss with respect to the model's parameters are computed. These gradients are used to update the model's parameters to minimize the loss (gradient descent).\n","4. **Parameter Update:** The model's parameters are updated using an optimization algorithm (e.g., SGD, Adam) to reduce the loss.\n","\n","\n","\n","\n"],"metadata":{"id":"EuN64v_Ytp8U"}},{"cell_type":"markdown","source":["**Steps:**\n","1. **Data Loading:** Load a batch of training data.\n","2. **Model Prediction:** Pass the input data through the model to get predictions.\n","3. **Loss Calculation:** Calculate the loss by comparing predictions to ground truth.\n","4. **Backward Pass Preparation:** Initialize gradients to zero (for later backpropagation)."],"metadata":{"id":"xZFblW626X5o"}},{"cell_type":"markdown","source":["## Forward Pass"],"metadata":{"id":"7kIj8Ykd0JsP"}},{"cell_type":"markdown","source":["**Purpose:**\n","\n","  The forward pass, also known as forward propagation, is the phase in training a neural network where **input data is passed through the network to compute predictions**. It calculates the output of the model given the **current set of parameters (weights and biases)**.\n","\n","___\n","\n","\n","\n","**Mathematics of Forward Pass:**\n","\n","The forward pass is a critical component of neural network computation, **transforming input data through a series of layers to produce an output**.\n","  - Each layer in the network comprises a **linear transformation followed by an activation function**, with different types of layers used to process different types of data.\n","  - **Convolutional layers** are commonly used for image and video data,\n","  - **Recurrent layers** are used for sequential data such as text and speech.\n","\n","Fully connected layers are also frequently used in neural networks. The **forward pass involves computing pre-activation and activation values at each layer, which are then passed to the next layer**. The final output is obtained by computing the activation values at the output layer.\n","\n","\n","\n","\n","\n","\n","1. **Input Data:**\n","\n","  The input data is represented as a **vector X (or a batch of input vectors), where each element corresponds to a feature**. The input data can be represented as a matrix if there are multiple samples in a batch.\n","\n","\n","2. **Fully Connected Layers**\n","\n","  Fully connected layers are a common type of layer used in neural networks. In the forward pass of a fully connected layer, the **input data `x` is multiplied by a weight matrix `W` and a bias vector `b` is added** to produce an output `z`:\n","\n","  ```\n","  z = Wx + b\n","  ```\n","\n","  The output `z` is then passed through an **activation function `f` to introduce nonlinearity** into the model:\n","\n","  ```\n","  a = f(z)\n","  ```\n","\n","  where `a` is the output of the fully connected layer.\n","\n","3. **Convolutional Layers**\n","\n","  Convolutional layers are commonly used in convolutional neural networks for image and video analysis. In the forward pass of a convolutional layer, the input data `x` is **convolved with a set of filters `W` to produce an output feature map `z`:**\n","\n","  ```\n","  z = W * x\n","  ```\n","\n","  The **output feature map `z` is then passed through an activation function `f` to introduce nonlinearity** into the model:\n","\n","  ```\n","  a = f(z)\n","  ```\n","\n","  where `a` is the output of the convolutional layer.\n","\n","4. **Recurrent Layers**\n","\n","  Recurrent layers are commonly used in recurrent neural networks for sequential data analysis. In the forward pass of a recurrent layer, **the input at each time step `x_t` is combined with the previous hidden state `h_{t-1}` to produce an output `y_t` and a new hidden state `h_t`:**\n","\n","  ```\n","  y_t, h_t = f(x_t, h_{t-1})\n","  ```\n","\n","  **where `f` is a nonlinear function that combines the input and hidden state**.\n","\n","5. **Pooling Layers**\n","\n","  Pooling layers are commonly used in convolutional neural networks to reduce the spatial dimensions of feature maps. In the forward pass of a pooling layer, **the input feature map `x` is divided into non-overlapping regions and a pooling operation is applied to each region**. The pooling operation can be max pooling, average pooling, or other types:\n","\n","  ```\n","  y_{i,j} = pool(x_{i:i+k,j:j+k})\n","  ```\n","\n","  where `y_{i,j}` is the output of the pooling operation on the region `(i,j)` of size `(k,k)`.\n","\n","6. **Normalization Layers**\n","\n","  **Normalization layers are used to normalize the output of a layer to improve model performance**. In the forward pass of a normalization layer, the input data `x` is normalized using a normalization function:\n","\n","  ```\n","  y = norm(x)\n","  ```\n","\n","  where `y` is the normalized output.\n","\n","7. **Attention Layers**\n","\n","  Attention layers are used to selectively focus on certain parts of the input data. In the forward pass of an **attention layer, the input data `x` is multiplied by an attention vector `a` to produce a weighted sum**:\n","\n","  ```\n","  y = sum(a_i * x_i)\n","  ```\n","\n","  where `a_i` is the attention weight for input element `x_i`.\n","\n","\n","\n","\n","8. **Repeat for Subsequent Layers:** This process of linear transformation followed by activation is repeated for each layer in the neural network until we reach the output layer.\n","\n","9. **Final Output:** The final output of the network is typically obtained at the output layer. Depending on the problem type (classification, regression, etc.), different activation functions may be used at the output layer.\n","\n","   - **Output Calculation at Output Layer:**\n","     ```\n","     Output = A_output = Z_output\n","     ```\n","     - A_output represents the final activation values at the output layer.\n","     - Z_output represents the pre-activation values at the output layer.\n","\n","\n","___\n","\n","**Role of each Layer:**\n","\n","| Layer Type | Purpose |\n","|------------|---------|\n","| Input Layer | Receives input data and passes it to the first hidden layer |\n","| Hidden Layers | Perform computations on the input data, transforming it into a more useful representation |\n","| Convolutional Layers | Process image and video data by extracting features through convolution operations |\n","| Recurrent Layers | Process sequential data such as text and speech by maintaining a memory of previous inputs |\n","| Pooling Layers | Downsample the output of convolutional layers to reduce computation and extract dominant features |\n","| Dropout Layers | Prevent overfitting by randomly dropping out some nodes during training |\n","| Batch Normalization Layers | Improve training stability and reduce overfitting by normalizing the input to each layer |\n","| Activation Layers | Introduce non-linearity into the network by applying an activation function to the output of each layer |\n","| Output Layer | Produce the final output of the network, with the number and type of nodes dependent on the problem being solved |\n","\n","___\n","\n","\n","\n","\n"],"metadata":{"id":"IC7QsL9Dt5oj"}},{"cell_type":"markdown","source":["**Example:**\n","\n","In this example, we'll generate new images based on a sequence of input images. This scenario could be applicable in video frame prediction or generating new frames in animation.\n","\n","```python\n","import torch\n","import torch.nn as nn\n","\n","class ImageGenerationModel(nn.Module):\n","    def __init__(self):\n","        super(ImageGenerationModel, self).__init__()\n","        \n","        # LSTM layer to process the sequence of images\n","        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n","        \n","        # Upsampling layer to increase spatial dimensions\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","        \n","        # Batch normalization layer\n","        self.batch_norm = nn.BatchNorm2d(64)\n","        \n","        # Convolutional layer for image generation\n","        self.conv_gen = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1)\n","        \n","        # Sigmoid activation for pixel values between 0 and 1\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        # LSTM layer to process the sequence of images\n","        lstm_output, _ = self.lstm(x)\n","        \n","        # Upsampling layer\n","        upsampled_output = self.upsample(lstm_output)\n","        \n","        # Batch normalization\n","        batch_norm_output = self.batch_norm(upsampled_output)\n","        \n","        # Convolutional layer for image generation\n","        generated_image = self.conv_gen(batch_norm_output)\n","        \n","        # Apply sigmoid activation for pixel values between 0 and 1\n","        generated_image = self.sigmoid(generated_image)\n","        \n","        return generated_image\n","\n","# Create an instance of the image generation model\n","image_generator = ImageGenerationModel()\n","\n","# Generate a sequence of input images (batch_size=4, sequence_length=10, channels=64, height=32, width=32)\n","input_images = torch.randn(4, 10, 64, 32, 32)\n","\n","# Forward pass through the image generation model\n","generated_images = image_generator(input_images)\n","\n","# Print the shape of the generated images\n","print(\"Generated Images Shape:\", generated_images.shape)\n","```\n","\n","In this example, the `ImageGenerationModel` processes a sequence of input images (each with 64 channels, 32x32 spatial dimensions) using LSTM to capture temporal dependencies. The LSTM output is then upsampled, batch-normalized, and passed through a convolutional layer to generate new images with 3 channels (representing RGB colors). **The sigmoid activation ensures the pixel values are between 0 and 1, suitable for images**.\n","\n","This example illustrates a realistic scenario where multiple layers and operations are combined to process sequential input data and generate meaningful output, making it more representative of real-world applications."],"metadata":{"id":"Ago6qUIk1aeW"}},{"cell_type":"markdown","source":["## Backward Pass"],"metadata":{"id":"4ULnkFh80P9_"}},{"cell_type":"markdown","source":["**Backward Propagation:**\n","\n","Backward propagation, also known as backpropagation, is a fundamental algorithm for training neural networks.\n","  - It is the process of **computing gradients of a loss function with respect to the model's parameters**\n","  \n","  - Which are then used to update the model's parameters to minimize the Loss. Below,\n","\n","\n","\n","**Purpose:**\n","\n","Backward propagation is **used to compute the gradients of a loss function with respect to the model's parameters. These gradients indicate how the loss changes concerning changes in each parameter**. Gradients are crucial for updating the model's parameters during training through optimization algorithms like gradient descent.\n","\n","**Mathematics of Backward Propagation:**\n","\n","The backward propagation process can be broken down into several key steps:\n","\n","1. **Loss Function:**\n","\n","  - Start with a loss function (L) that quantifies **how far off the model's predictions (Y_pred) are from the ground truth (Y_true)**.\n","  \n","  - The goal is to **minimize this loss**.\n","\n","2. **Gradient Calculation:**\n","\n","  - Compute the gradients of the loss with respect to the **model's parameters (W and B)**.\n","  \n","  - These gradients **represent how the loss changes concerning changes in each parameter**. Mathematically, the gradient (∇L) is computed using the chain rule of calculus:\n","   \n","    ∇L = (∂L/∂Y_pred) * (∂Y_pred/∂Z) * (∂Z/∂W)\n","\n","   - (∂L/∂Y_pred) is the gradient of the **loss with respect to the model's predictions**.\n","   - (∂Y_pred/∂Z) is the gradient of the **predictions with respect to the pre-activation values (Z)**.\n","   - (∂Z/∂W) is the gradient of the **pre-activation values with respect to the model's weights (W)**.\n","\n","3. **Parameter Update:**\n","\n","  - Once the gradients are calculated, the model's parameters **(W and B) are updated in the direction that reduces the loss**.\n","  - This update is typically performed using an **optimization algorithm like gradient descent**:\n","   \n","     W ← W - α * ∇L\n","\n","   - W is the **model's weights.**\n","   - α is the **learning rate**, which controls the step size in parameter updates.\n","   - ∇L is the **gradient of the loss**.\n","\n","4. **Repeat:**\n","\n","  - Steps 1-3 are repeated for multiple iterations (epochs) over the training data to progressively reduce the loss and train the model.\n","\n","**Key Considerations:**\n","- Gradients are calculated using the chain rule, starting from the loss and propagating backward through the layers of the neural network.\n","- Gradients can be computed efficiently using automatic differentiation libraries like PyTorch and TensorFlow.\n","- Optimization algorithms like gradient descent adjust model parameters to minimize the loss.\n","\n"],"metadata":{"id":"8rdgwgnMDfjT"}},{"cell_type":"markdown","source":["**Example 1:**\n","\n","Let's consider a simple example with a single weight (W) and a loss function (L):\n","\n","  **L = (Y_pred - Y_true)^2**\n","\n","Here, we'd compute ∇L by taking the derivative of L with respect to Y_pred and then applying the chain rule to compute ∇L with respect to W. **So, we connected the loss change with weights change and bias change**, and then updated weights and biases to reduce loss by using Optimization algorithm and Learning rate.\n","\n","\n","**Example 2:**\n","\n","```markdown\n","Suppose we have a neural network with a single input feature\n","(x), a single hidden layer with one neuron, and a single output (y).\n","\n","The network is defined as follows:\n","\n","  Input layer: x\n","  Hidden layer: h = w1 * x + b1 (where w1 is the weight and b1 is the bias)\n","  Output layer: y = w2 * h + b2 (where w2 is the weight and b2 is the bias)\n","\n","  Let's assume the ground truth output (y_true) is 10, and the initial weights and biases are as follows:\n","  w1 = 0.5, b1 = 1.0, w2 = 2.0, b2 = 0.5\n","\n","  We will use the mean squared error (MSE) loss function to quantify the model's prediction error.\n","\n","1. Forward Propagation:\n","\n","  Given an input x = 2, we can compute the forward pass of the network as follows:\n","  h = 0.5 * 2 + 1.0 = 2.0\n","  y_pred = 2.0 * 2.0 + 0.5 = 4.5\n","\n","\n","2. Loss Calculation:\n","\n","  Using the MSE loss function, we can calculate the loss as:\n","  L = (y_pred - y_true)^2 = (4.5 - 10)^2 = 26.01\n","\n","\n","3. Backward Propagation:\n","\n","  Now, we need to calculate the gradients of the loss with respect to the weights and biases.\n","\n","  First, let's compute the gradient of the loss with respect to y_pred:\n","  dL_dy_pred = 2 * (y_pred - y_true) = 2 * (4.5 - 10) = -11\n","\n","\n","  Next, we calculate the gradients of the output layer:\n","  dL_dw2 = dL_dy_pred * h = -11 * 2.0 = -22.0\n","  dL_db2 = dL_dy_pred = -11\n","\n","\n","  Then, we calculate the gradients of the hidden layer:\n","  dL_dh = dL_dy_pred * w2 = -11 * 2.0 = -22.0\n","  dL_dw1 = dL_dh * x = -22.0 * 2 = -44.0\n","  dL_db1 = dL_dh = -22.0\n","\n","4. Parameter Update:\n","\n","  Finally, we update the weights and biases using the gradients and a learning rate (α) of 0.1:\n","  w1 = w1 - α * dL_dw1 = 0.5 - 0.1 * (-44.0) = 4.9\n","  b1 = b1 - α * dL_db1 = 1.0 - 0.1 * (-22.0) = 3.2\n","\n","This process of forward propagation, loss calculation,\n","backward propagation, and parameter update is repeated for\n","multiple iterations (epochs) to train the neural network and\n","minimize the loss.\n","```\n"],"metadata":{"id":"G_TG57MGE0A4"}},{"cell_type":"markdown","source":["**Example:**\n","\n","To complete the example, let's add the backward propagation step to the code. Backward propagation calculates gradients with respect to the model's parameters, which are then used to update the model during training. Here's how you can incorporate the backward pass into the code:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Define the image generation model with the same architecture\n","class ImageGenerationModel(nn.Module):\n","    def __init__(self):\n","        super(ImageGenerationModel, self).__init__()\n","        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n","        self.batch_norm = nn.BatchNorm2d(64)\n","        self.conv_gen = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        lstm_output, _ = self.lstm(x)\n","        upsampled_output = self.upsample(lstm_output)\n","        batch_norm_output = self.batch_norm(upsampled_output)\n","        generated_image = self.conv_gen(batch_norm_output)\n","        generated_image = self.sigmoid(generated_image)\n","        return generated_image\n","\n","# Create an instance of the image generation model\n","image_generator = ImageGenerationModel()\n","\n","# Generate a sequence of input images (batch_size=4, sequence_length=10, channels=64, height=32, width=32)\n","input_images = torch.randn(4, 10, 64, 32, 32)\n","\n","# Create ground truth images for demonstration (same shape as generated images)\n","ground_truth_images = torch.randn(4, 10, 3, 64, 64)\n","\n","# Define a loss function (e.g., Mean Squared Error) for image generation\n","criterion = nn.MSELoss()\n","\n","# Define an optimizer (e.g., Adam) to update the model's parameters\n","optimizer = optim.Adam(image_generator.parameters(), lr=0.001)\n","\n","# Forward pass through the model to generate images\n","generated_images = image_generator(input_images)\n","\n","# Calculate the loss by comparing generated images to ground truth\n","loss = criterion(generated_images, ground_truth_images)\n","\n","# Backward pass to compute gradients\n","optimizer.zero_grad()  # Zero out gradients\n","loss.backward()       # Compute gradients using backpropagation\n","\n","# Update model parameters using the optimizer\n","optimizer.step()\n","\n","# Now, the model's parameters have been updated based on the computed gradients\n","```\n","\n","In this code, we've added the following backward propagation steps:\n","\n","1. We define a loss function (`criterion`) that measures the difference between the generated images and ground truth images. In this case, we're using Mean Squared Error (MSE).\n","\n","2. We create an optimizer (`optimizer`) that will update the model's parameters (weights and biases) during training. We use the Adam optimizer with a learning rate of 0.001.\n","\n","3. After the forward pass, we calculate the loss by comparing the generated images to ground truth images using the defined loss function.\n","\n","4. The `loss.backward()` call computes gradients of the loss with respect to the model's parameters using backpropagation.\n","\n","5. Finally, the `optimizer.step()` updates the model's parameters based on the computed gradients.\n","\n"],"metadata":{"id":"0e7VSJ6A8ikv"}}]}