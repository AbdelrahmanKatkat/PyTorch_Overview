{"cells":[{"cell_type":"markdown","id":"88de6987","metadata":{"id":"88de6987"},"source":["#<font color='blue' size='5px'/> Model Building & Training with Pytorch Overview<font/>"]},{"cell_type":"markdown","source":["## 1 Model Building In PyTorch"],"metadata":{"id":"D_9aq-vNf8oh"},"id":"D_9aq-vNf8oh"},{"cell_type":"markdown","source":["### Introduction"],"metadata":{"id":"AJ3iBx1AX2GC"},"id":"AJ3iBx1AX2GC"},{"cell_type":"markdown","source":["1. **Introduction to PyTorch Model Building**\n","\n","  PyTorch is an open-source machine learning library that is widely used for building and training deep neural networks. PyTorch provides a flexible and intuitive approach to building models using dynamic computation graphs, making it easy to experiment with different architectures and training techniques.\n","\n","2. **Defining the Model Architecture**\n","\n","  The first step in building a PyTorch model is defining the architecture of the neural network. This involves specifying the **number of layers, the size of each layer, and the activation functions used in each layer**. PyTorch provides a wide range of pre-built layers and activation functions, as well as the ability to define custom layers and functions.\n","\n","3. **Implementing the Forward Pass**\n","\n","  Once the architecture of the model has been defined, the next step is to implement the **forward pass. The forward pass is the process of feeding input data through the neural network to produce an output**. In PyTorch, this is done by defining a **forward() method** for the model, which takes input data as input and returns the output of the model.\n","\n","4. **Defining the Loss Function**\n","\n","  In order to train a PyTorch model, it is necessary to define a loss function. The loss function measures how well the model is performing on a given task, and provides a way to **calculate the error between the predicted output and the true output**. PyTorch provides a wide range of **pre-built loss functions**, as well as the ability to define custom loss functions.\n","\n","5. **Backpropagation and Optimization**\n","\n","  Once the loss function has been defined, the next step is to **train the model using backpropagation and optimization**.\n","  \n","  Backpropagation is the process of **calculating the gradients of the loss function with respect to each parameter in the model**, and using these gradients to update the parameters using an optimization algorithm such as **stochastic gradient descent (SGD)**. PyTorch provides automatic differentiation, which makes it easy to calculate gradients using backpropagation.\n","\n"],"metadata":{"id":"rTYPZkMKXyfU"},"id":"rTYPZkMKXyfU"},{"cell_type":"markdown","source":["### Steps of Model Building"],"metadata":{"id":"wl0fEKuMWWqZ"},"id":"wl0fEKuMWWqZ"},{"cell_type":"markdown","source":["\n","**1. Import PyTorch Libraries:**\n","\n","Begin by importing the necessary PyTorch libraries:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","```\n","\n"],"metadata":{"id":"GZ1P1EPXMYkk"},"id":"GZ1P1EPXMYkk"},{"cell_type":"markdown","source":["**2. Define the Model Class:**\n","\n","In PyTorch, you create a model by defining a Python class that **inherits from `torch.nn.Module`**. This class will represent your neural network. Within this class, you define the layers and operations that make up your model in the constructor (`__init__`) method and specify the forward pass in the `forward` method. Here's a basic structure of a model class:\n","\n","```python\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        # Define layers and operations here\n","\n","    def forward(self, x):\n","        # Specify the forward pass\n","        # Use layers and operations defined in __init__\n","        return x\n","```\n","\n"],"metadata":{"id":"2G1TMHV2MiBZ"},"id":"2G1TMHV2MiBZ"},{"cell_type":"markdown","source":["**nn.Module:**\n","   - In PyTorch, you create a neural network model by defining a **Python class that inherits from `torch.nn.Module**. This class represents your neural network architecture.\n","\n","   - **nn.Module** is a **class in PyTorch** that provides a convenient way to organize and encapsulate all the learnable parameters of a neural network model. When a custom neural network model is defined in PyTorch, it usually inherits from the `nn.Module` class. This allows the model to **inherit all the functionalities of nn.Module**, such as tracking all its parameters, moving the model to GPU, and many more.\n","\n","   - **Within the constructor (`__init__`)** of the class, you define the layers and operations that make up your model.\n","    - For example, you might define linear layers, convolutional layers, activation functions, etc.\n","\n","**Forward Method:**\n","\n","  - **The `forward` method** is where you specify the **forward pass of your network**. You define how the data flows through the layers from input to output. This method computes the predictions or outputs of your model.\n","\n","  - In PyTorch, the `forward` method is a required method for any custom neural network model that inherits from the `nn.Module` class.\n","  - This method defines the computation that the **model performs on input data to produce its output**.\n","  - Specifically, given an input tensor, the `forward` method specifies how that tensor should be transformed as it passes through the layers of the model.\n","  - When a PyTorch model is called with an input tensor, the `forward` method is automatically executed to produce the output tensor.\n","\n","  - You can create complex models by composing various layers and modules in a modular and organized manner. This allows you to design architectures tailored to specific tasks.\n","\n"],"metadata":{"id":"o1D6WbDNR8Xn"},"id":"o1D6WbDNR8Xn"},{"cell_type":"markdown","source":["**3. Define Layers and Operations:**\n","\n","Inside the constructor (`__init__`) of your model class, define the layers and operations that you want to use in your model. PyTorch provides a wide range of built-in layers and modules to choose from. For example, you can define **linear (fully connected) layers, convolutional layers, activation functions, and more**.\n","\n","\n","- So, the main idea is to divide the layer into two stages unlike TensorFlow.\n","- Here the activation function & Computations that the model does in forward and Layer in __init__\n","- Here's an example of defining a simple feedforward neural network with two linear layers and a ReLU activation function:\n","\n","```python\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.fc1 = nn.Linear(in_features=64, out_features=128)\n","        self.fc2 = nn.Linear(in_features=128, out_features=10)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","```\n","\n"],"metadata":{"id":"HhiTn_JnMkjJ"},"id":"HhiTn_JnMkjJ"},{"cell_type":"markdown","source":["**In Python, `super` is a built-in function**\n","\n","- It is used to call a method in a parent class. In the context of PyTorch, `super` is used to call the `__init__` method of the parent class (`nn.Module`) in the `__init__` method of the custom neural network model.\n","\n","- This is necessary to properly** initialize the model's parameters** and other attributes that are defined in the parent class.\n","\n","- Here's an example of how `super` is used in the code:\n","\n","```python\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        # Define the rest of the model here\n","```\n","\n","- Here, `super(MyModel, self).__init__()` **calls the `__init__` method of the parent class (`nn.Module`) with the arguments `MyModel` (the current class) and `self` (the current instance of the class)**. This initializes the model's parameters and other attributes that are defined in the parent class."],"metadata":{"id":"Ra4rWw-8SEwo"},"id":"Ra4rWw-8SEwo"},{"cell_type":"markdown","source":["**4. Specify the Forward Pass:**\n","\n","In the `forward` method of your model class, specify how the data flows through the layers. This method defines the **computation that produces the model's predictions**. You can use the layers and operations defined in the constructor.\n","\n","In the example below, the forward pass applies a ReLU activation function after the first linear layer and returns the output of the second linear layer as the model's predictions.\n","\n","```\n","  def forward(self, x):\n","      x = torch.relu(self.fc1(x))\n","      x = self.fc2(x)\n","      return x\n","```\n"],"metadata":{"id":"xEeCgpQLMmoa"},"id":"xEeCgpQLMmoa"},{"cell_type":"markdown","source":["**5. Instantiate the Model:**\n","\n","To use your model, create an instance of it:\n","\n","```python\n","model = MyModel()\n","```\n","\n"],"metadata":{"id":"bKV0d-FjMqPi"},"id":"bKV0d-FjMqPi"},{"cell_type":"markdown","source":["**6. Model Summary and Parameters:**\n","\n","The `summary` function from the `torchsummary` package is used to summarize the PyTorch model by showing the number of parameters and the output shape of each layer in the model.\n","\n","  - You can check the **model's architecture and the number of trainable parameters using the `print`** function or dedicated libraries like `torchsummary`. For example:\n","\n","```python\n","from torchsummary import summary\n","\n","summary(model, (64,))  # Input shape, e.g., (batch_size, input_features)\n","```\n","\n","  - The **second parameter** of the `summary` function is a **tuple that specifies the input shape of the model**. In this case, `(64,)` represents a single input sample with 64 features. The `batch_size` is not specified here, so it is assumed to be 1.\n","\n","  - The **`summary` function uses this input shape to calculate the output shape of each layer** in the model, which is then displayed along with the number of parameters in each layer. This information can be useful for debugging and optimizing the model architecture.\n","\n","\n","**Full example**,\n","\n","`batch_size` is the number of input samples in each batch, `channels` is the number of color channels in the input data (e.g. 3 for RGB images), `height` is the height of the input data in pixels, and `width` is the width of the input data in pixels.\n","\n","\n","  ```python\n","  from torchsummary import summary\n","\n","  summary(model, (batch_size, channels, height, width))\n","  ```\n","\n","\n","  - For example, if you were working with 32x32 RGB images and a batch size of 16, you would use the following input shape:\n","\n","  ```python\n","  summary(model, (16, 3, 32, 32))\n","  ```\n","\n","\n"],"metadata":{"id":"C-YUJSavMsJx"},"id":"C-YUJSavMsJx"},{"cell_type":"markdown","source":["**7. Model to GPU (if available):**\n","\n","If you have a GPU available and want to accelerate training, move your model and data to the GPU using `model.to(device)` and `data.to(device)`.\n","\n","```python\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","```\n"],"metadata":{"id":"nzXRKC25MuHK"},"id":"nzXRKC25MuHK"},{"cell_type":"markdown","source":["**10. Model Serialization:**\n","\n","PyTorch allows you to save and load model weights for reuse or deployment. You can save the entire model or just its state_dict, which contains learned parameters. Here's an example of saving and loading model weights:\n","\n","```python\n","# Save model\n","torch.save(model.state_dict(), 'model_weights.pth')\n","\n","# Load model\n","model.load_state_dict(torch.load('model_weights.pth'))\n","```\n"],"metadata":{"id":"TcxfB7m8ahTe"},"id":"TcxfB7m8ahTe"},{"cell_type":"markdown","source":["**11. Model Architecture Patterns:**\n","\n","Depending on the task, you may follow specific architectural patterns:\n","   - **Sequential Models:** For linear stack-like networks, use `nn.Sequential` to create a sequence of layers.\n","   - **Residual Networks (ResNets):** Introduce shortcut connections to ease the training of very deep networks.\n","   - **Recurrent Neural Networks (RNNs):** Utilized for sequence data.\n","   - **Convolutional Neural Networks (CNNs):** Commonly used for image-related tasks.\n","   - **Transformer Models:** Suitable for sequence-to-sequence tasks with self-attention mechanisms.\n","\n"],"metadata":{"id":"apsfj9slamFo"},"id":"apsfj9slamFo"},{"cell_type":"markdown","source":["**list of model architecture patterns in PyTorch:**\n","\n","1. Feedforward Neural Networks (FFNN)\n","2. Convolutional Neural Networks (CNN)\n","3. Recurrent Neural Networks (RNN)\n","4. Long Short-Term Memory (LSTM) Networks\n","5. Gated Recurrent Units (GRU)\n","6. Autoencoders\n","7. Variational Autoencoders (VAE)\n","8. Generative Adversarial Networks (GAN)\n","9. Deep Belief Networks (DBN)\n","10. Restricted Boltzmann Machines (RBM)\n","11. Deep Convolutional GANs (DCGAN)\n","12. CycleGAN\n","13. Adversarial Autoencoders\n","14. Siamese Networks\n","15. Capsule Networks\n","16. Transformer Networks\n"],"metadata":{"id":"CIVdSTTE2bm4"},"id":"CIVdSTTE2bm4"},{"cell_type":"markdown","source":["**12. Transfer Learning:**\n","\n","You can leverage pre-trained models to solve similar tasks more efficiently. Fine-tuning a pre-trained model on your specific data can save significant training time and resources.\n","\n","```python\n","import torchvision.models as models\n","\n","# Load a pre-trained model (e.g., ResNet-18)\n","pretrained_model = models.resnet18(pretrained=True)\n","\n","# Replace the final classification layer with your custom layer\n","pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, num_classes)\n","```\n","\n","\n"],"metadata":{"id":"WFwKstpOao9e"},"id":"WFwKstpOao9e"},{"cell_type":"markdown","source":["**13. Debugging and Visualization:**\n","\n","During model building and training, it's essential to use tools for debugging and visualization. PyTorch provides tools like `print` statements, TensorBoard, and visualization libraries like Matplotlib to help you understand your model's behavior and identify issues.\n"],"metadata":{"id":"hJwMz_NwasvI"},"id":"hJwMz_NwasvI"},{"cell_type":"markdown","source":["**14. Loss Functions:**\n","\n","When building your model, you should also consider the choice of an appropriate loss function. The choice of loss function depends on the task you are solving. For example:\n","   - Cross-Entropy Loss (`nn.CrossEntropyLoss`) is commonly used for multi-class classification.\n","   - Mean Squared Error Loss (`nn.MSELoss`) is used for regression problems.\n","   - Custom loss functions can be defined for specialized tasks.\n","\n","You can specify the loss function in the training loop when calculating the loss between model predictions and ground truth.\n","\n","```python\n","criterion = nn.CrossEntropyLoss()\n","loss = criterion(predictions, ground_truth)\n","```\n","\n"],"metadata":{"id":"_zV4IYVhOm3x"},"id":"_zV4IYVhOm3x"},{"cell_type":"markdown","source":["**15. Model Evaluation:**\n","\n","During model building, it's essential to define evaluation metrics relevant to your task. For classification tasks, common evaluation metrics include accuracy, precision, recall, F1-score, and ROC AUC. For regression tasks, metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) are commonly used.\n","\n","You can use these metrics to evaluate the performance of your model on a validation or test dataset.\n","\n"],"metadata":{"id":"HhBCG7fNa2Q3"},"id":"HhBCG7fNa2Q3"},{"cell_type":"markdown","source":["**16. Hyperparameter Tuning:**\n","\n","Model building also involves hyperparameter tuning. Hyperparameters include learning rates, batch sizes, the number of hidden units or layers, dropout rates, weight decay, and more. You can experiment with different hyperparameter settings to find the best configuration for your model.\n","\n","\n"],"metadata":{"id":"y-q1aX9Ka61Y"},"id":"y-q1aX9Ka61Y"},{"cell_type":"markdown","source":["**17. Regularization Techniques:**\n","\n","To prevent overfitting during model training, you can employ regularization techniques like dropout and weight decay. Dropout layers randomly deactivate neurons during training to improve model generalization. Weight decay adds a regularization term to the loss to discourage large weights.\n"],"metadata":{"id":"jbjEICtk37cu"},"id":"jbjEICtk37cu"},{"cell_type":"markdown","source":["**18. Model Saving and Loading:**\n","\n","Once you have built and trained your model, you should save it to disk for future use or deployment. PyTorch allows you to save the model's state dictionary or the entire model architecture.\n","\n","```python\n","# Save model\n","torch.save(model.state_dict(), 'model_weights.pth')\n","\n","# Load model\n","model.load_state_dict(torch.load('model_weights.pth'))\n","```\n","\n","\n"],"metadata":{"id":"wi29tAEPa_wP"},"id":"wi29tAEPa_wP"},{"cell_type":"markdown","source":["## 2 Types of Layers that can be added"],"metadata":{"id":"tc4Evht3f_pT"},"id":"tc4Evht3f_pT"},{"cell_type":"markdown","source":["### Theory Guide"],"metadata":{"id":"S-YMPmVk0P9J"},"id":"S-YMPmVk0P9J"},{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_N9j0r2eLyrRos9mb7U89VO6Qq5PiJs1?usp=sharing)\n"],"metadata":{"id":"7P2pRsvfztAI"},"id":"7P2pRsvfztAI"},{"cell_type":"markdown","source":["## 3 Types of Activation Functions, Loss Functions, and Optimizers:"],"metadata":{"id":"1e2iJwdCgCLB"},"id":"1e2iJwdCgCLB"},{"cell_type":"markdown","source":["An activation function is a mathematical function that is applied to the output of a neural network layer or a single neuron. It helps to introduce non-linearity into the network, allowing it to learn complex patterns and relationships in the data.\n","\n","The activation function determines the output of a neuron or a layer, based on the weighted sum of inputs. It helps to decide whether the neuron should be activated or not by applying a certain threshold. The activation function can be linear or non-linear.\n","\n","Some commonly used activation functions include:\n","1. Sigmoid function: It maps the input to a value between 0 and 1, which is useful for binary classification problems.\n","2. Hyperbolic tangent (tanh) function: It maps the input to a value between -1 and 1, which is useful for classification problems.\n","3. Rectified Linear Unit (ReLU) function: It returns 0 for negative inputs and the input value for positive inputs, which is widely used in deep learning models.\n","4. Leaky ReLU function: It is similar to ReLU but allows a small gradient for negative inputs, preventing dead neurons.\n","5. Softmax function: It is used in multi-class classification problems to convert the output into probabilities.\n","."],"metadata":{"id":"fwUyCNU2WPWI"},"id":"fwUyCNU2WPWI"},{"cell_type":"markdown","source":["**3. Types of Activation Functions, Loss Functions, and Optimizers:**\n","   - Activation Functions:\n","     - ReLU (`nn.ReLU`) is widely used due to its effectiveness in combating vanishing gradients.\n","     - Sigmoid (`nn.Sigmoid`) and Tanh (`nn.Tanh`) are often used for specific tasks like binary classification.\n","   - Loss Functions:\n","     - Cross-Entropy Loss (`nn.CrossEntropyLoss`) is suitable for multi-class classification problems.\n","     - Mean Squared Error Loss (`nn.MSELoss`) is commonly used for regression tasks.\n","   - Optimizers:\n","     - Adam (`optim.Adam`) is a popular choice due to its adaptive learning rate capabilities.\n","     - Stochastic Gradient Descent (`optim.SGD`) is a classic optimization algorithm.\n","     - RMSprop (`optim.RMSprop`) adapts the learning rate based on the recent gradients.\n","\n","\n"],"metadata":{"id":"2fs3OBKALkhL"},"id":"2fs3OBKALkhL"},{"cell_type":"markdown","source":["### Activation Function"],"metadata":{"id":"ixnl8t0gHd1H"},"id":"ixnl8t0gHd1H"},{"cell_type":"markdown","source":["**Activation Functions:**\n","\n","Activation functions are mathematical functions applied to the output of a neuron in a neural network to introduce non-linearity into the model. The choice of activation function affects how the neuron responds to its input and, consequently, the network's ability to learn and represent complex patterns in the data. Here are some common types of activation functions used in neural networks:\n","\n","Here's the corrected version of the text:\n","\n","1. **Sigmoid Function (Logistic Activation):**\n","   - Formula:  $f(x) = \\frac{1}{1 + e^{-x}}$\n","   - Range: $(0, 1)$\n","   - Characteristics: Sigmoid functions squash input values to the range $(0, 1)$. They are useful in binary classification problems and can be interpreted as providing probabilities.\n","\n","2. **Hyperbolic Tangent Function (tanh):**\n","   - Formula: $f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n","   - Range: $(-1, 1)$\n","   - Characteristics: tanh functions are similar to sigmoid functions but squash input values to the range $(-1, 1)$. They are zero-centered, making optimization easier in some cases.\n","\n","3. **Rectified Linear Unit (ReLU):**\n","   - Formula: $f(x) = max(0, x)$\n","   - Range: $[0, \\infty)$\n","   - Characteristics: ReLU functions are piecewise linear and set negative inputs to zero. They are computationally efficient and have been widely adopted in deep learning models.\n","\n","4. **Leaky Rectified Linear Unit (Leaky ReLU):**\n","   - Formula: $f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{if } x \\leq 0 \\end{cases}$\n","   - Range: $(-\\infty, \\infty)$\n","   - Characteristics: Leaky ReLU is similar to ReLU but allows a small gradient for negative inputs ($\\alpha$ is typically a small positive constant). It helps mitigate the \"dying ReLU\" problem.\n","\n","5. **Parametric Rectified Linear Unit (PReLU):**\n","   - Formula: $f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ a_i x, & \\text{if } x \\leq 0 \\end{cases}$\n","   - Range: $(-\\infty, \\infty)$\n","   - Characteristics: PReLU is an extension of Leaky ReLU, where the slope of the negative part can be learned during training.\n","\n","6. **Exponential Linear Unit (ELU):**\n","   - Formula: $f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha(e^{x} - 1), & \\text{if } x \\leq 0 \\end{cases}$\n","   - Range: $(-\\infty, \\infty)$\n","   - Characteristics: ELU is similar to ReLU but smooth for negative inputs, with an exponential decay. It can help reduce the vanishing gradient problem.\n","\n","7. **Scaled Exponential Linear Unit (SELU):**\n","   - Formula: $f(x) = \\lambda \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha(e^{x} - 1), & \\text{if } x \\leq 0 \\end{cases}$\n","   - Range: $(-\\infty, \\infty)$\n","   - Characteristics: SELU is an extension of ELU with specific values for $\\alpha$ and $\\lambda$ that aim to make the activations maintain a stable mean and variance during training.\n","\n","8. **Softmax Function:**\n","   - Formula: $f(x)_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$ for each element $x_i$\n","   - Range: $(0, 1)$ with the sum of all elements equal to $1$\n","   - Characteristics: Softmax is used primarily in the output layer of multi-class classification models. It converts a vector of raw scores into a probability distribution over classes.\n","\n"],"metadata":{"id":"AA-RdNxiHlvT"},"id":"AA-RdNxiHlvT"},{"cell_type":"markdown","source":["### Why to use Activation Function"],"metadata":{"id":"x6_UkWbJYjKb"},"id":"x6_UkWbJYjKb"},{"cell_type":"markdown","source":["**Why to use Activation Function:**\n","\n","Adding nonlinearity through activation functions is a critical aspect of neural networks, and it serves several important purposes:\n","\n","  1. **Model Complex Relationships:** Without activation functions, neural networks would be limited to representing linear transformations of the input data. In many real-world problems, especially those with complex, non-linear patterns, linear models are insufficient. Activation functions introduce nonlinearity, allowing neural networks to capture and model intricate relationships within the data.\n","\n","  2. **Enable Representation Learning:** Neural networks are often referred to as universal function approximators, which means they have the capability to approximate any function. However, this capability is unlocked through the use of activation functions. Activation functions allow neural networks to learn and represent complex data patterns, making them highly adaptable to various tasks.\n","\n","  3. **Enable Deep Architectures:** Deep neural networks, consisting of multiple layers, have the potential to learn hierarchical features from raw data. Activation functions prevent the network from collapsing into a linear model as you stack more layers. Without nonlinearity, the composition of multiple linear transformations would still result in a linear transformation, limiting the network's expressiveness.\n","\n","  4. **Learn and Capture Local Patterns:** Activation functions introduce nonlinearity at the level of individual neurons. This nonlinearity enables neurons to capture and respond to local patterns and features in the input data. Neurons with activation functions can act as feature detectors, identifying specific characteristics in the data.\n","\n","  5. **Enable Nonlinear Decision Boundaries:** In classification tasks, activation functions allow neural networks to create nonlinear decision boundaries. This is crucial for distinguishing between complex classes that cannot be separated by simple linear boundaries. Activation functions enable neural networks to learn decision regions that are more flexible and expressive.\n","\n","  6. **Overcome the Vanishing Gradient Problem:** Certain activation functions, like the rectified linear unit (ReLU), help mitigate the vanishing gradient problem, which can hinder the training of deep networks. By allowing gradients to pass more freely during backpropagation, ReLU and similar functions enable the training of very deep architectures.\n","\n","  7. **Introduce Sparsity and Non-sparsity:** Some activation functions, like the sigmoid and hyperbolic tangent (tanh), introduce sparsity in the network's activations. This sparsity can have regularization effects, helping prevent overfitting. On the other hand, ReLU and its variants introduce non-sparsity, which can improve the representation capacity of the network.\n","\n","![image](https://www.simplilearn.com/ice9/free_resources_article_thumb/list-of-activation-functions-used-with-perceptron.jpg)"],"metadata":{"id":"RDZguT1yYiTU"},"id":"RDZguT1yYiTU"},{"cell_type":"markdown","source":["## 4 Training Loops (Forward and Backward)"],"metadata":{"id":"04i0Ccm75zJa"},"id":"04i0Ccm75zJa"},{"cell_type":"markdown","source":["**Training Loops (Forward and Backward):**\n","   - Training loops consist of two main phases:\n","     - Forward Pass: During this phase, input data is passed through the model to obtain predictions.\n","     - Backward Pass (Autograd): PyTorch's automatic differentiation engine (Autograd) automatically tracks operations during the forward pass. Gradients with respect to the loss are computed during the backward pass.\n","   - Training loops are organized with nested iterations. The inner loop typically iterates over batches of data, while the outer loop iterates over epochs, which are complete passes through the entire training dataset.\n","\n"],"metadata":{"id":"KJaDXJ2eLo8R"},"id":"KJaDXJ2eLo8R"},{"cell_type":"markdown","source":["## 5  Gradient Descent Optimization Techniques"],"metadata":{"id":"ug7KyqBe516p"},"id":"ug7KyqBe516p"},{"cell_type":"markdown","source":["- Gradient Descent Optimization:\n","  - Stochastic Gradient Descent (SGD) updates weights using gradients computed on mini-batches of data.\n","  - Mini-batch Gradient Descent balances computation efficiency and convergence speed."],"metadata":{"id":"aV99rUB4Y6oS"},"id":"aV99rUB4Y6oS"},{"cell_type":"markdown","source":["## 6 Learning Rates"],"metadata":{"id":"NachvJK459Ha"},"id":"NachvJK459Ha"},{"cell_type":"markdown","source":["\n","- Learning Rates:\n","  - Learning rate is a hyperparameter that controls the step size during weight updates. Choosing the right learning rate is crucial for training stability and convergence.\n","  - Learning rate schedulers adjust the learning rate during training to improve convergence.\n","\n","\n"],"metadata":{"id":"tqbSeGV_LrrJ"},"id":"tqbSeGV_LrrJ"},{"cell_type":"markdown","source":["## 7 Weight Initialization Strategies"],"metadata":{"id":"2znTziNk6AUZ"},"id":"2znTziNk6AUZ"},{"cell_type":"markdown","source":["- Weight Initialization:\n","  - Proper weight initialization helps prevent vanishing/exploding gradients and accelerates convergence.\n","  - Xavier/Glorot initialization and He initialization are commonly used strategies."],"metadata":{"id":"UUwYm3J6ZE-K"},"id":"UUwYm3J6ZE-K"},{"cell_type":"markdown","source":["## 8 Regularization Techniques"],"metadata":{"id":"zrcOT20t6EMF"},"id":"zrcOT20t6EMF"},{"cell_type":"markdown","source":["**Regularization Techniques:**\n","   - Regularization techniques are used to prevent overfitting:\n","     - Dropout randomly drops neurons during training, forcing the network to be more robust.\n","     - Weight Decay (L2 Regularization) adds a penalty term to the loss to discourage large weight values.\n","     - Batch Normalization normalizes activations within a layer to reduce internal covariate shift and accelerate training."],"metadata":{"id":"rOZ5sVggLt2h"},"id":"rOZ5sVggLt2h"},{"cell_type":"code","source":[],"metadata":{"id":"d6ojEvbVZHyK"},"id":"d6ojEvbVZHyK","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"collapsed_sections":["D_9aq-vNf8oh","qnE0NuoXF10X","VwptiVJAKubW","KXLt6XblN3uY","59Ik_p7yRkfy","T-q5IjNOUemb","4kpCcFUIWKqh","1e2iJwdCgCLB","ixnl8t0gHd1H","04i0Ccm75zJa","NachvJK459Ha","zrcOT20t6EMF","ICVBE1MUya6l"],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}