{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMIN9UGo/YRCH7iu5voKSRV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#<font color='blue' size='5px'/>Gradient Optimization Techniques<font/>"],"metadata":{"id":"kiuuArcFosDQ"}},{"cell_type":"markdown","source":["# 1 Introduction"],"metadata":{"id":"7GiO7LUqbyzy"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Introduction</summary>\n","\n","Gradient optimization techniques are **algorithms used to find the optimal set of parameters (weights and biases) of a machine learning model by iteratively minimizing a loss or cost function.** These techniques rely on the gradients of the loss function with respect to the parameters to determine how to update the parameters at each iteration.\n","\n","Gradient optimization techniques are a family of methods used in machine learning and deep learning to iteratively **update the model parameters in order to minimize a loss function.**\n","  - These techniques adjust the parameters based on the **gradients of the loss function with respect to the parameters**.\n","  - Let's dive into several gradient optimization techniques, provide mathematical equations, and describe how they work.\n","</details>\n"],"metadata":{"id":"4ysEVD_yTTWq"}},{"cell_type":"markdown","source":["\n","<details>\n","  <summary>Gradient Optimization Techniques in Machine Learning</summary>\n","\n","This table provides an overview of various gradient optimization techniques used in machine learning and deep learning, including their applications, advantages, and disadvantages.\n","\n","| Optimization Technique     | Application                                  | Advantages                                     | Disadvantages                                 |\n","|----------------------------|----------------------------------------------|------------------------------------------------|-----------------------------------------------|\n","| Gradient Descent           | General optimization of convex functions, model training | - Simplicity                                    | - Sensitive to learning rate choice            |\n","| Stochastic Gradient Descent | Large-scale and online learning, deep neural network training | - Faster convergence                            | - Noisy updates                                |\n","| Mini-Batch Gradient Descent | General optimization, faster convergence, deep neural network training | - Balance between GD and SGD                   | - Requires tuning batch size                   |\n","| Momentum                   | Faster convergence, dampens oscillations, deep neural network training | - Faster convergence, reduced oscillations     | - Sensitive to momentum coefficient            |\n","| Adagrad                    | Sparse data, adaptive learning rates, deep neural network training | - Adaptive learning rates                      | - Accumulates squared gradients, may slow down |\n","| RMSprop                    | Adaptive learning rates, deep neural network training | - Adaptive learning rates                      | - Accumulates squared gradients, similar to Adagrad |\n","| Adam                       | General optimization, combines momentum and adaptive learning rates, deep neural network training | - Efficient, often works well out of the box | - Requires tuning hyperparameters               |\n","| Nesterov Accelerated Gradient (NAG) | Faster convergence, deep neural network training | - Faster convergence                            | - Requires tuning momentum coefficient         |\n","| Adadelta                   | Adaptive learning rates, deep neural network training | - Adaptive learning rates                      | - Requires tuning hyperparameters               |\n","| Rprop                      | Neural network training with noisy gradients | - Convergence in the presence of noisy gradients | - Sensitive to initial weight choices         |\n","| Nadam                      | General optimization, combines Nesterov momentum and Adam, deep neural network training | - Efficient, faster convergence                 | - Requires tuning hyperparameters               |\n","| L-BFGS                     | Small to medium-sized optimization problems, non-convex functions | - Well-suited for small problems, accurate      | - Memory-intensive, slow for large problems    |\n","| L-BFGS-B                   | Constrained optimization problems, optimization with bounds | - Handles constrained optimization              | - Memory-intensive, slow for large problems    |\n","\n","</details>"],"metadata":{"id":"XW2sYbE8VFU7"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Additional Gradient Optimization Techniques in Machine Learning</summary>\n","\n","This table extends the list of gradient optimization techniques used in machine learning and deep learning, providing insights into their applications, advantages, and disadvantages.\n","\n","| Optimization Technique     | Application                                  | Advantages                                     | Disadvantages                                 |\n","|----------------------------|----------------------------------------------|------------------------------------------------|-----------------------------------------------|\n","| L-BFGS-BF                  | Constrained optimization problems with non-linear constraints | - Handles non-linear constraints                | - Complex to set up and tune                   |\n","| Conjugate Gradient         | Linear and quadratic optimization, model training | - Suitable for linear optimization problems    | - Limited to convex problems                  |\n","| FTRL Proximal              | Online learning, regularization, large-scale optimization | - Efficient for large-scale problems            | - Requires careful hyperparameter tuning       |\n","| LBFGS-MF                   | Large-scale optimization with matrix factorization | - Efficient, handles factorization problems     | - May be less suitable for deep learning      |\n","| CG-FR                      | General optimization, model training         | - Works well for linear and convex problems     | - Limited to certain types of optimization problems |\n","| Rprop+                     | Optimization with enhanced Rprop             | - Enhanced convergence properties              | - Similar sensitivity to initial weights as Rprop |\n","\n","These additional optimization techniques offer specialized solutions for various optimization scenarios. When choosing an optimizer, consider the specific requirements and challenges of your task, as well as the characteristics of your dataset and neural network architecture. Careful experimentation and hyperparameter tuning are essential for achieving optimal training results.\n","\n","</details>"],"metadata":{"id":"L04x1qdbVcnN"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Mathematics General</summary>\n","\n","<details>\n","  <summary>Gradient Descent</summary>\n","Gradient Descent is the fundamental optimization algorithm in machine learning. It aims to find the minimum of the loss function by iteratively adjusting model parameters.\n","\n","**Steps:**\n","1. Initialize parameters randomly.\n","2. Compute the **gradient of the loss function with respect to the parameters**.\n","3. Update parameters in the opposite direction of the gradient:\n","   $$\\text{new_parameter} = \\text{old_parameter} - \\text{learning_rate} \\times \\nabla(\\text{loss})$$\n","\n","</details>\n","\n","\n","\n","\n","\n","<details>\n","  <summary>Stochastic Gradient Descent (SGD)</summary>\n","\n","SGD is an extension of Gradient Descent that updates parameters using a single random training example at each iteration.\n","\n","**Advantages:**\n","- Faster convergence, less computational cost per iteration.\n","\n","**Steps:**\n","1. Initialize parameters randomly.\n","2. Shuffle training data.\n","3. For each mini-batch:\n","   - Compute the gradient of the loss function with respect to the parameters using the mini-batch.\n","   - Update parameters using the gradient.\n","\n","\n","</details>\n","\n","\n","<details>\n","  <summary>Mini-Batch Gradient Descent</summary>\n","\n","Mini-Batch Gradient Descent combines the benefits of batch GD and SGD by updating parameters using a small random subset (mini-batch) of the training data at each iteration.\n","\n","**Advantages:**\n","- Better generalization than SGD.\n","- Efficient use of computational resources.\n","\n","**Steps:**\n","1. Initialize parameters randomly.\n","2. Shuffle training data.\n","3. For each mini-batch:\n","   - Compute the gradient of the loss function with respect to the parameters using the mini-batch.\n","   - Update parameters using the gradient.\n","\n","</details>\n","\n","\n","\n","\n","\n","\n","<details>\n","  <summary>Momentum</summary>\n","Momentum helps accelerate convergence by adding a fraction of the previous update to the current update. It dampens oscillations in the parameter space.\n","\n","**Steps:**\n","1. Initialize velocity to zero.\n","2. For each iteration:\n","   - Compute the gradient of the loss function with respect to the parameters.\n","   - Update velocity:\n","     $$\\text{velocity} = \\beta \\times \\text{velocity} + \\text{learning\\_rate} \\times \\nabla(\\text{loss})$$\n","   - Update parameters:\n","     $$\\text{new\\_parameter} = \\text{old\\_parameter} - \\text{velocity}$$\n","\n","</details>\n","\n","\n","\n","\n","\n","<details>\n","  <summary>Adagrad (Adaptive Gradient Algorithm)</summary>\n","Adagrad adapts the learning rate individually for each parameter based on the historical sum of squared gradients.\n","\n","**Advantages:**\n","- Automatically adapts learning rates.\n","- Effective for sparse data.\n","\n","**Steps:**\n","1. Initialize squared gradient accumulator to zero for each parameter.\n","2. For each iteration:\n","   - Compute the gradient of the loss function with respect to the parameters.\n","   - Update squared gradient accumulator:\n","     $$\\text{squared\\_gradient\\_accumulator} += (\\nabla(\\text{loss}))^2$$\n","   - Update parameters:\n","     $$\\text{new\\_parameter} = \\text{old\\_parameter} - \\frac{\\text{learning\\_rate}}{\\sqrt{\\text{squared\\_gradient\\_accumulator} + \\epsilon}} \\times \\nabla(\\text{loss})$$\n","\n","</details>\n","\n","\n","\n","\n","\n","\n","<details>\n","  <summary>RMSprop (Root Mean Square Propagation)</summary>\n","RMSprop adapts the learning rate by using a moving average of squared gradients.\n","\n","**Advantages:**\n","- Adaptive learning rates.\n","- Mitigates issues of rapidly decreasing learning rates in Adagrad.\n","\n","**Steps:**\n","1. Initialize squared gradient accumulator to zero for each parameter.\n","2. For each iteration:\n","   - Compute the gradient of the loss function with respect to the parameters.\n","   - Update squared gradient accumulator:\n","     $$\\text{squared\\_gradient\\_accumulator} = \\rho \\times \\text{squared\\_gradient\\_accumulator} + (1 - \\rho) \\times (\\nabla(\\text{loss}))^2$$\n","   - Update parameters:\n","     $$\\text{new\\_parameter} = \\text{old\\_parameter} - \\frac{\\text{learning\\_rate}}{\\sqrt{\\text{squared\\_gradient\\_accumulator} + \\epsilon}} \\times \\nabla(\\text{loss})$$\n","\n","</details>\n","\n","\n","\n","\n","\n","<details>\n","  <summary>Adam (Adaptive Moment Estimation)</summary>\n","\n","Adam combines the ideas of momentum and Adagrad/RMSprop. It uses moving averages of both gradients and squared gradients.\n","\n","**Steps:**\n","1. Initialize moving averages for the first moment (mean) and second moment (uncentered variance) to zero for each parameter.\n","2. For each iteration:\n","   - Compute the gradient of the loss function with respect to the parameters.\n","   - Update the first moment:\n","     $$\\text{first\\_moment} = \\beta_1 \\times \\text{first\\_moment} + (1 - \\beta_1) \\times \\nabla(\\text{loss})$$\n","   - Update the second moment:\n","     $$\\text{second\\_moment} = \\beta_2 \\times \\text{second\\_moment} + (1 - \\beta_2) \\times (\\nabla(\\text{loss}))^2$$\n","   - Correct bias in moving averages:\n","     $$\\text{corrected\\_first\\_moment} = \\frac{\\text{first\\_moment}}{1 - \\beta_1^t}$$\n","     $$\\text{corrected\\_second\\_moment} = \\frac{\\text{second\\_moment}}{1 - \\beta_2^t}$$\n","     where \\(t\\) is the current iteration.\n","   - Update parameters:\n","     $$\\text{new\\_parameter} = \\text{old\\_parameter} - \\frac{\\text{learning\\_rate}}{\\sqrt{\\text{corrected\\_second\\_moment} + \\epsilon}} \\times \\text{corrected\\_first\\_moment}$$\n","\n","</details>\n","\n","\n","\n","\n","\n","\n","<details>\n","  <summary>Nesterov Accelerated Gradient (NAG)</summary>\n","\n","Nesterov Accelerated Gradient is a modification of the standard momentum method. It adjusts the parameter updates by considering the gradient at a point ahead in the direction of the momentum.\n","\n","**Steps:**\n","1. Initialize velocity to zero.\n","2. For each iteration:\n","   - Predict the future parameter update by moving in the direction of the current velocity:\n","     $$\\text{future\\_parameter} = \\text{old\\_parameter} - \\text{momentum} \\times \\text{velocity}$$\n","   - Compute the gradient of the loss function with respect to the future parameter:\n","     $$\\text{future\\_gradient} = \\nabla(\\text{loss})(\\text{future\\_parameter})$$\n","   - Update velocity:\n","     $$\\text{velocity} = \\text{momentum} \\times \\text{velocity} + \\text{learning\\_rate} \\times \\text{future\\_gradient}$$\n","   - Update parameters:\n","     $$\\text{new\\_parameter} = \\text{old\\_parameter} - \\text{velocity}$$\n","\n","</details>\n","\n","\n","\n","\n","<details>\n","  <summary>Adadelta</summary>\n","Adadelta is an extension of RMSprop that dynamically adapts learning rates without requiring a fixed learning rate.\n","\n","**Steps:**\n","1. Initialize two accumulators, one for the running average of squared gradients and another for the running average of squared parameter updates.\n","2. For each iteration:\n","   - Compute the gradient of the loss function with respect to the parameters.\n","   - Update the running average of squared gradients:\n","     $$\\text{running\\_avg\\_squared\\_gradient} = \\rho \\times \\text{running\\_avg\\_squared\\_gradient} + (1 - \\rho) \\times (\\nabla(\\text{loss}))^2$$\n","   - Compute the effective learning rate:\n","     $$\\text{effective\\_learning\\_rate} = \\frac{\\sqrt{\\text{running\\_avg\\_squared\\_update} + \\epsilon}}{\\sqrt{\\text{running\\_avg\\_squared\\_gradient} + \\epsilon}}$$\n","   - Update the running average of squared parameter updates:\n","     $$\\text{running\\_avg\\_squared\\_update} = \\rho \\times \\text{running\\_avg\\_squared\\_update} + (1 - \\rho) \\times (\\text{parameter\\_update})^2$$\n","   - Update parameters:\n","     $$\\text{new\\_parameter} = \\text{old\\_parameter} - \\text{effective\\_learning\\_rate} \\times \\nabla(\\text{loss})$$\n","\n","</details>\n","\n","\n","\n","\n","\n","\n","<details>\n","  <summary>L-BFGS - Limited-memor Broyden Fletcher Goldfarb Shanno</summary>\n","\n","L-BFGS is a quasi-Newton optimization method often used for small to medium-sized problems where the Hessian matrix is approximated without explicit computation.\n","\n","**Advantages:**\n","- Well-suited for optimization with a small number of parameters.\n","- Requires less hyperparameter tuning.\n","\n","**Steps:**\n","1. Initialize parameters randomly.\n","2. For each iteration:\n","   - Approximate the Hessian matrix using limited memory.\n","   - Update parameters using the quasi-Newton method.\n","\n","</details>\n","\n","\n","\n","\n","\n","These gradient optimization techniques play a vital role in training deep neural networks effectively by controlling how model parameters are updated during training. The choice of an optimizer depends on the specific problem, architecture, and available computational resources. Experimentation and tuning are often required to find the most suitable optimizer for a given task.\n","\n","</details>"],"metadata":{"id":"JuvgiXOIPPse"}},{"cell_type":"markdown","source":["<details>\n","<summary>Guidelines on when to use the optimization techniques</summary>\n","\n","1. Gradient Descent: Use when working with small datasets and simple models. It is simple to implement and can work well when the learning rate is chosen appropriately.\n","\n","2. Stochastic Gradient Descent: Use when working with large datasets or online learning scenarios. It converges faster than Gradient Descent but can be noisy due to updates being calculated on a single example.\n","\n","3. Mini-Batch Gradient Descent: Use when looking for a balance between Gradient Descent and Stochastic Gradient Descent. It can converge faster than Gradient Descent and be less noisy than Stochastic Gradient Descent, but requires tuning the batch size.\n","\n","4. Momentum: Use when working with deep neural networks and looking for faster convergence while reducing oscillations. It works by adding a fraction of the previous update to the current update, which helps overcome local minima.\n","\n","5. Adagrad: Use when working with sparse data or when looking for adaptive learning rates. It accumulates squared gradients to adjust the learning rate for each parameter separately, which can be useful when some parameters have a much larger gradient than others.\n","\n","6. RMSprop: Use when looking for adaptive learning rates in deep neural network training. It is similar to Adagrad but accumulates a moving average of squared gradients instead of accumulating all past squared gradients.\n","\n","7. Adam: Use when looking for an efficient optimization technique that combines momentum and adaptive learning rates. It often works well out of the box but requires tuning hyperparameters.\n","</details>"],"metadata":{"id":"Nk5c8dZev81A"}},{"cell_type":"markdown","source":["# 2 Gradient Descent"],"metadata":{"id":"J2itozbgUdgc"}},{"cell_type":"markdown","source":["## Theory"],"metadata":{"id":"iQuXP3dXdSTP"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Theory</summary>\n","\n","**Gradient Descent** is a fundamental optimization algorithm used for minimizing a cost or loss function in machine learning and deep learning. It's particularly useful for finding the optimal parameters (weights and biases) of a model. The basic idea is to iteratively adjust the parameters in a direction that decreases the value of the loss function. Here's a detailed explanation, equations, and a numerical example.\n","\n","**Gradient Descent Process:**\n","\n","  1. **Initialization:** Start with initial parameter values (weights and biases) randomly or with predefined values.\n","\n","  2. **Forward Pass:** Use the current parameters to make predictions on the training data.\n","\n","  3. **Loss Calculation:** Calculate the value of the loss function (often a non-negative value that quantifies the error between predictions and actual targets).\n","\n","  4. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters. The gradient represents the direction of steepest increase in the loss.\n","\n","  5. **Parameter Update:** Adjust the parameters in the opposite direction of the gradient to minimize the loss. The update is determined by the learning rate, which controls the step size.\n","\n","  6. **Iteration:** Repeat steps 2-5 for a specified number of iterations or until convergence criteria are met.\n","\n","**Mathematical Formulation:**\n","\n","- **Parameters:** Let $\\theta$ represent the parameters of the model (weights and biases).\n","\n","- **Loss Function:** The loss function is typically denoted as $J(\\theta)$, and it measures the difference **between predicted and actual values**.\n","\n","- **Gradient:** The gradient of the loss function with respect to the parameters is denoted as $\\nabla J(\\theta)$. It is a vector that points in the direction of the steepest ascent of the loss.\n","\n","- **Learning Rate:** The learning rate, denoted as $\\alpha$, controls the size of the step taken in each iteration. It is a hyperparameter that you can tune.\n","\n","- **Update Rule:** The parameters are updated using the following rule:\n","  $$ \\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta) $$\n","\n","**Numerical Example:**\n","\n","Let's consider a simple linear regression problem, where we want to fit a line to a set of data points. The loss function for linear regression is often the Mean Squared Error (MSE):\n","\n","$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n","\n","Where:\n","- $\\theta$ are the model parameters (slope and intercept of the line).\n","- $h_\\theta(x^{(i)})$ is the model's prediction for data point $i$.\n","- $y^{(i)}$ is the actual target value for data point $i$.\n","- $m$ is the number of data points.\n","\n","The gradient of the MSE loss with respect to $\\theta$ is calculated as follows:\n","\n","$$ \\nabla J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} $$\n","\n","Now, let's perform one iteration of gradient descent:\n","\n","- Initial parameters: $\\theta = [2, 3]$\n","- Learning rate: $\\alpha = 0.01$\n","\n","1. **Forward Pass:** Calculate the model's prediction $h_\\theta(x^{(i)})$ for all data points using the current $\\theta$.\n","\n","2. **Loss Calculation:** Calculate the MSE loss using the predictions and actual target values.\n","\n","3. **Gradient Calculation:** Calculate the gradient $\\nabla J(\\theta)$ using the formula.\n","\n","4. **Parameter Update:** Update the parameters using the learning rate:\n","   $$ \\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta) $$\n","\n","Repeat these steps for multiple iterations until the loss converges to a minimum. The parameters $\\theta$ will be adjusted in each iteration to minimize the loss and find the best-fitting line.\n","\n","This process can be implemented in code using your preferred programming language and machine learning library, such as Python with NumPy for the numerical calculations.\n","\n","![image](https://miro.medium.com/max/1400/1*16uR2JPzCtu2qSCGPe7kbw.jpeg)\n","\n","</details>"],"metadata":{"id":"UP5I934LdWGq"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Numerical Example</summary>\n","\n","\n","Let's consider a simple linear regression problem, where we want to fit a line to a set of data points. Suppose we have the following data:\n","\n","| x | y |\n","|---|---|\n","| 1 | 2 |\n","| 2 | 4 |\n","| 3 | 6 |\n","| 4 | 8 |\n","| 5 | 10 |\n","\n","Our goal is to find the line that best fits this data. We can use gradient descent to find the optimal parameters (slope and intercept) of the line.\n","\n","The loss function for linear regression is often the Mean Squared Error (MSE):\n","\n","$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n","\n","Where:\n","- $\\theta$ are the model parameters (slope and intercept of the line).\n","- $h_\\theta(x^{(i)})$ is the model's prediction for data point $i$.\n","- $y^{(i)}$ is the actual target value for data point $i$.\n","- $m$ is the number of data points.\n","\n","For linear regression, we can use the hypothesis function:\n","\n","$$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n","\n","**Our goal is to find the values of $\\theta_0$ and $\\theta_1$ that minimize the MSE loss.**\n","\n","We can start by initializing our parameters with random values. Let's say we choose $\\theta_0 = 1$ and $\\theta_1 = 1$. We also need to choose a learning rate $\\alpha$. Let's use $\\alpha = 0.01$.\n","\n","Now we can perform one iteration of gradient descent:\n","\n","1. **Forward Pass:** Calculate the model's prediction $h_\\theta(x^{(i)})$ for all data points using the current $\\theta$.\n","\n","   For example, when $x=1$, $h_\\theta(x) = \\theta_0 + \\theta_1 x = 1 + 1(1) = 2$. Similarly, we can calculate the predictions for all other data points.\n","\n","2. **Loss Calculation:** Calculate the MSE loss using the predictions and actual target values.\n","\n","   $$ J(\\theta) = \\frac{1}{10} [(2-2)^2 + (4-4)^2 + (6-6)^2 + (8-8)^2 + (10-10)^2] = 0 $$\n","\n","3. **Gradient Calculation:** Calculate the gradient $\\nabla J(\\theta)$ using the formula.\n","\n","   $$ \\nabla J(\\theta) = \\begin{bmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\ \\frac{\\partial J}{\\partial \\theta_1} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\\\ \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} \\end{bmatrix} $$\n","\n","   Plugging in our values, we get:\n","\n","   $$ \\nabla J(\\theta) = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix} $$\n","\n","4. **Parameter Update:** Update the parameters using the learning rate:\n","\n","   $$ \\theta_0 \\leftarrow \\theta_0 - \\alpha \\frac{\\partial J}{\\partial \\theta_0} = 1 - 0.01(0) = 1 $$\n","\n","   $$ \\theta_1 \\leftarrow \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1} = 1 - 0.01(0) = 1 $$\n","\n","After one iteration, our parameters have not changed since the gradient was zero. We can continue to run more iterations until we reach convergence criteria or a maximum number of iterations.\n","\n","\n","\n","\n","</details>"],"metadata":{"id":"HyCrLqdLffoo"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"ahLPaqhfdUCZ"}},{"cell_type":"markdown","source":["<details>\n","  <summary>code</summary>\n","\n","To write code for Gradient Descent in PyTorch, you need to define a model, loss function, optimizer, and perform the training loop. Here's a step-by-step guide:\n","\n","1. Import the necessary libraries:\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","```\n","\n","2. Define your model using the `nn.Module` class. You can create a simple linear regression model as an example:\n","```python\n","class LinearRegression(nn.Module):\n","    def __init__(self):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","```\n","\n","3. Define your training data and labels:\n","```python\n","x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n","y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n","```\n","\n","4. Initialize an instance of your model:\n","```python\n","model = LinearRegression()\n","```\n","\n","5. Define your loss function:\n","```python\n","criterion = nn.MSELoss()\n","```\n","\n","6. Define your optimizer (e.g., Stochastic Gradient Descent):\n","```python\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","```\n","\n","7. Perform the training loop:\n","```python\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    # Forward pass\n","    outputs = model(x_train)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print progress\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","```\n","\n","In this example, we iterate through a fixed number of epochs. In each epoch, we perform a forward pass to get the predicted outputs, calculate the loss between the predicted outputs and the true labels, perform backpropagation to compute the gradients, and update the model's parameters using the optimizer.\n","\n","Note: Make sure to convert your data to tensors and move them to the appropriate device (e.g., GPU) if available.\n","\n","</details>"],"metadata":{"id":"xZZhZJjLjSP5"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Code</summary>\n","\n","```\n","import torch\n","\n","# Define the target function to optimize (quadratic function)\n","def target_function(x):\n","    return x**2 + 3 * x + 2\n","\n","# Initialize a random starting point\n","x = torch.tensor(5.0, requires_grad=True)\n","\n","# Hyperparameters\n","learning_rate = 0.1\n","num_epochs = 100\n","\n","# Gradient Descent loop\n","for epoch in range(num_epochs):\n","    # Forward pass: Compute the loss (value of the target function)\n","    loss = target_function(x)\n","    \n","    # Backward pass: Compute the gradient of the loss with respect to x\n","    loss.backward()\n","    \n","    # Update x in the opposite direction of the gradient\n","    x.data = x - learning_rate * x.grad\n","    \n","    # Zero the gradients for the next iteration\n","    x.grad.zero_()\n","\n","# Print the optimized value of x\n","print(\"Optimized x:\", x.item())\n","\n","```\n","</details>"],"metadata":{"id":"BKkp_nrAl38p"}},{"cell_type":"markdown","source":["# 3 Stochastic Gradient Descent (SGD)"],"metadata":{"id":"YSLiDi53UiJr"}},{"cell_type":"markdown","source":["## Theory"],"metadata":{"id":"7zIo8LfKgbjQ"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Theory</summary>\n","\n","**Stochastic Gradient Descent (SGD)** is a variation of the gradient descent optimization algorithm. While traditional gradient descent computes the gradient of the loss function with respect to the entire dataset, SGD estimates the gradient using only a single randomly selected data point (or a small batch of data points) in each iteration. **This randomness can lead to faster convergence and is particularly useful when dealing with large datasets**. Here's a detailed explanation, equations, and a numerical example.\n","\n","**Stochastic Gradient Descent Process:**\n","\n","1. **Initialization:** Start with initial parameter values (weights and biases) randomly or with predefined values.\n","\n","2. **Shuffling Data:** Randomly shuffle the training dataset to ensure that each data point has an equal chance of being selected.\n","\n","3. **Iteration:** For each iteration, **select one data point (or a small batch of data points) randomly**.\n","\n","4. **Forward Pass:** Use the selected data point to make predictions on the training data.\n","\n","5. **Loss Calculation:** Calculate the loss on the selected data point. The loss is often a non-negative value that quantifies the error between the prediction and the actual target.\n","\n","6. **Gradient Calculation:** Compute the gradient of the loss function with respect to the parameters using the selected data point.\n","\n","7. **Parameter Update:** Adjust the parameters in the opposite direction of the gradient using the learning rate. Repeat steps 3-6 for a specified number of iterations or until convergence criteria are met.\n","\n","**Mathematical Formulation:**\n","\n","- **Parameters:** Let $\\theta$ represent the parameters of the model (weights and biases).\n","\n","- **Loss Function:** The loss function is typically denoted as $J(\\theta)$, and it measures the difference between predicted and actual values.\n","\n","- **Gradient:** The gradient of the loss function with respect to the parameters is denoted as $\\nabla J(\\theta)$. It is a vector that points in the direction of steepest ascent of the loss for the selected data point.\n","\n","- **Learning Rate:** The learning rate, denoted as $\\alpha$, controls the size of the step taken in each iteration. It is a hyperparameter that you can tune.\n","\n","- **Update Rule:** The parameters are updated using the following rule:\n","  $$ \\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta) $$\n","\n","**Numerical Example:**\n","\n","Let's consider a simple linear regression problem similar to the one in the previous example. The loss function for linear regression is the Mean Squared Error (MSE):\n","\n","$$ J(\\theta) = \\frac{1}{2} (h_\\theta(x) - y)^2 $$\n","\n","Where:\n","- $\\theta$ are the model parameters (slope and intercept of the line).\n","- $h_\\theta(x)$ is the model's prediction for a selected data point.\n","- $y$ is the actual target value for the selected data point.\n","\n","The gradient of the MSE loss with respect to $\\theta$ for a selected data point is calculated as follows:\n","\n","$$ \\nabla J(\\theta) = (h_\\theta(x) - y)x $$\n","\n","Now, let's perform one iteration of stochastic gradient descent:\n","\n","- Initial parameters: $\\theta = [2, 3]$\n","- Learning rate: $\\alpha = 0.01$\n","\n","1. **Shuffling Data:** Randomly shuffle the training dataset.\n","\n","2. **Iteration:** Select one data point randomly.\n","\n","3. **Forward Pass:** Calculate the model's prediction $h_\\theta(x)$ for the selected data point using the current $\\theta$.\n","\n","4. **Loss Calculation:** Calculate the MSE loss using the prediction and the actual target value.\n","\n","5. **Gradient Calculation:** Calculate the gradient $\\nabla J(\\theta)$ using the formula.\n","\n","6. **Parameter Update:** Update the parameters using the learning rate:\n","   $$ \\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta) $$\n","\n","Repeat these steps for multiple iterations, randomly selecting different data points each time. The randomness of SGD can lead to faster convergence and is particularly useful when working with large datasets where computing the gradient on the entire dataset would be computationally expensive.\n","\n","This process can be implemented in code using your preferred programming language and machine learning library, such as Python with NumPy for the numerical calculations.\n","\n","![image](https://cdn-images-1.medium.com/max/1200/1*7LbtloKtsBZW1P0DmR4UDA.png)\n","\n","![image](https://www.researchgate.net/publication/331758559/figure/download/fig3/AS:736526556200964@1552613023975/Schematic-of-classical-gradient-descent-in-1D-left-panel-and-stochastic-gradient.png)\n","\n","</details>"],"metadata":{"id":"orohWi8Hged6"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Numerical Example</summary>\n","\n","\n","Let's consider a simple linear regression problem, where we want to fit a line to a set of data points. Suppose we have the following data:\n","\n","| x | y |\n","|---|---|\n","| 1 | 2 |\n","| 2 | 4 |\n","| 3 | 6 |\n","| 4 | 8 |\n","| 5 | 10 |\n","\n","Our goal is to find the line that best fits this data using stochastic gradient descent. We can use the same loss function and hypothesis function as in the previous example:\n","\n","- **Loss Function:** The Mean Squared Error (MSE) is often used as the loss function for linear regression:\n","\n","  $$ J(\\theta) = \\frac{1}{2} (h_\\theta(x) - y)^2 $$\n","\n","- **Hypothesis Function:** For linear regression, we can use the hypothesis function:\n","\n","  $$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n","\n","We can start by initializing our parameters with random values. Let's say we choose $\\theta_0 = 1$ and $\\theta_1 = 1$. We also need to choose a learning rate $\\alpha$. Let's use $\\alpha = 0.01$.\n","\n","Now we can perform one iteration of stochastic gradient descent:\n","\n","1. **Shuffling Data:** Randomly shuffle the training dataset.\n","\n","2. **Iteration:** For the first iteration, select the first data point: $x=1$, $y=2$.\n","\n","3. **Forward Pass:** Calculate the model's prediction $h_\\theta(x)$ using the current $\\theta$ and the selected data point.\n","\n","   $$ h_\\theta(x) = \\theta_0 + \\theta_1 x = 1 + 1(1) = 2 $$\n","\n","4. **Loss Calculation:** Calculate the loss on the selected data point.\n","\n","   $$ J(\\theta) = \\frac{1}{2} (h_\\theta(x) - y)^2 = \\frac{1}{2} (2 - 2)^2 = 0 $$\n","\n","5. **Gradient Calculation:** Compute the gradient of the loss function with respect to the parameters using the selected data point.\n","\n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = (h_\\theta(x) - y) = 0 $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = (h_\\theta(x) - y) x = 0 $$\n","\n","6. **Parameter Update:** Adjust the parameters in the opposite direction of the gradient using the learning rate.\n","\n","   $$ \\theta_0 \\leftarrow \\theta_0 - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = 1 $$\n","   \n","   $$ \\theta_1 \\leftarrow \\theta_1 - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = 1 $$\n","\n","We have completed one iteration of stochastic gradient descent using a single data point. We can repeat steps 2-6 for a specified number of iterations or until convergence criteria are met.\n","</details>"],"metadata":{"id":"c2gMYV-lgmB5"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"BSiyhOMKgdP5"}},{"cell_type":"markdown","source":["<details>\n","  <summary>code</summary>\n","\n","To write code for Stochastic Gradient Descent (SGD) in PyTorch, you can use the `optim.SGD` optimizer class. Here's an example of how to implement SGD in PyTorch:\n","\n","1. Import the necessary libraries:\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","```\n","\n","2. Define your model using the `nn.Module` class. You can create a simple linear regression model as an example:\n","```python\n","class LinearRegression(nn.Module):\n","    def __init__(self):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","```\n","\n","3. Define your training data and labels:\n","```python\n","x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n","y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n","```\n","\n","4. Initialize an instance of your model:\n","```python\n","model = LinearRegression()\n","```\n","\n","5. Define your loss function:\n","```python\n","criterion = nn.MSELoss()\n","```\n","\n","6. Define your optimizer (SGD):\n","```python\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","```\n","\n","7. Perform the training loop:\n","```python\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    # Shuffle the training data\n","    indices = torch.randperm(x_train.size(0))\n","    x_train_shuffled = x_train[indices]\n","    y_train_shuffled = y_train[indices]\n","\n","    # Iterate over mini-batches\n","    batch_size = 2\n","    for i in range(0, x_train.size(0), batch_size):\n","        # Forward pass\n","        outputs = model(x_train_shuffled[i:i+batch_size])\n","        loss = criterion(outputs, y_train_shuffled[i:i+batch_size])\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Print progress\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","```\n","\n","In this example, we iterate through a fixed number of epochs. In each epoch, we shuffle the training data, then iterate over mini-batches of the shuffled data. For each mini-batch, we perform a forward pass to get the predicted outputs, calculate the loss between the predicted outputs and the true labels, perform backpropagation to compute the gradients, and update the model's parameters using the optimizer.\n","\n","Note that the batch size is set to 2 in this example, but you can adjust it according to your specific needs.\n","\n","\n","</details>"],"metadata":{"id":"u-CeCVtAj4LG"}},{"cell_type":"markdown","source":["# 4 Mini-Batch Gradient Descent"],"metadata":{"id":"IkL29tqDUlH7"}},{"cell_type":"markdown","source":["## Theory"],"metadata":{"id":"jsCymdwCgpUe"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Theory</summary>\n","\n","**Mini-Batch Gradient Descent** is a variation of the gradient descent optimization algorithm that falls between the traditional gradient descent and stochastic gradient descent.\n","- Instead of computing the gradient using the entire dataset (batch gradient descent) or a single data point (stochastic gradient descent), mini-batch gradient descent uses a small random subset of the data (a mini-batch) in each iteration.\n","- This approach offers a balance between the computational efficiency of stochastic gradient descent and the robustness of batch gradient descent. Here's a detailed explanation, equations, and a numerical example.\n","\n","**Mini-Batch Gradient Descent Process:**\n","\n","1. **Initialization:** Start with initial parameter values (weights and biases) randomly or with predefined values.\n","\n","2. **Shuffling Data:** Randomly shuffle the training dataset to ensure that each mini-batch is composed of different data points.\n","\n","3. **Iteration:** For each iteration, select a mini-batch of data points randomly.\n","\n","4. **Forward Pass:** Use the selected mini-batch to make predictions on the training data.\n","\n","5. **Loss Calculation:** Calculate the loss on the selected mini-batch. The loss is often a non-negative value that quantifies the error between the predictions and the actual targets.\n","\n","6. **Gradient Calculation:** Compute the gradient of the loss function with respect to the parameters using the selected mini-batch.\n","\n","7. **Parameter Update:** Adjust the parameters in the opposite direction of the gradient using the learning rate. Repeat steps 3-6 for a specified number of iterations or until convergence criteria are met.\n","\n","**Mathematical Formulation:**\n","\n","- **Parameters:** Let $\\theta$ represent the parameters of the model (weights and biases).\n","\n","- **Loss Function:** The loss function is typically denoted as $J(\\theta)$, and it measures the difference between predicted and actual values.\n","\n","- **Gradient:** The gradient of the loss function with respect to the parameters is denoted as $\\nabla J(\\theta)$. It is a vector that points in the direction of steepest ascent of the loss for the selected mini-batch.\n","\n","- **Learning Rate:** The learning rate, denoted as $\\alpha$, controls the size of the step taken in each iteration. It is a hyperparameter that you can tune.\n","\n","- **Update Rule:** The parameters are updated using the following rule:\n","  $$ \\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta) $$\n","\n","**Numerical Example:**\n","\n","Let's use the same linear regression problem as in the previous examples. The loss function is the Mean Squared Error (MSE):\n","\n","$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n","\n","Where:\n","- $\\theta$ are the model parameters (slope and intercept of the line).\n","- $h_\\theta(x^{(i)})$ is the model's prediction for data point $i$.\n","- $y^{(i)}$ is the actual target value for data point $i$.\n","- $m$ is the number of data points.\n","\n","The gradient of the MSE loss with respect to $\\theta$ for a mini-batch of data points is calculated as follows:\n","\n","$$ \\nabla J(\\theta) = \\frac{1}{\\text{mini-batch size}} \\sum_{i=1}^{\\text{mini-batch size}} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} $$\n","\n","Now, let's perform one iteration of mini-batch gradient descent:\n","\n","- Initial parameters: $\\theta = [2, 3]$\n","- Learning rate: $\\alpha = 0.01$\n","- Mini-batch size: 10\n","\n","1. **Shuffling Data:** Randomly shuffle the training dataset.\n","\n","2. **Iteration:** Select a mini-batch of 10 data points randomly.\n","\n","3. **Forward Pass:** Calculate the model's predictions $h_\\theta(x^{(i)})$ for the data points in the mini-batch using the current $\\theta$.\n","\n","4. **Loss Calculation:** Calculate the MSE loss on the selected mini-batch using the predictions and actual target values.\n","\n","5. **Gradient Calculation:** Calculate the gradient $\\nabla J(\\theta)$ using the formula.\n","\n","6. **Parameter Update:** Update the parameters using the learning rate:\n","   $$ \\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta) $$\n","\n","Repeat these steps for multiple iterations, randomly selecting different mini-batches each time. Mini-batch gradient descent combines the computational efficiency of batch gradient descent with the benefits of stochastic gradient descent, providing a good balance between the two approaches.\n","\n","This process can be implemented in code using your preferred programming language and machine learning library, such as Python with NumPy for the numerical calculations.\n","\n","![image](https://miro.medium.com/max/875/1*948oZ_spc1GMqQ-FYQddMw.png)\n","\n","\n","</details>"],"metadata":{"id":"JwCpzpgkgpUe"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Numerical Example</summary>\n","\n","Sure, here's a numerical example:\n","\n","Let's consider the same simple linear regression problem as before, where we want to fit a line to a set of data points. Suppose we have the same data:\n","\n","| x | y |\n","|---|---|\n","| 1 | 2 |\n","| 2 | 4 |\n","| 3 | 6 |\n","| 4 | 8 |\n","| 5 | 10 |\n","\n","Our goal is to find the line that best fits this data using mini-batch gradient descent. We can use the same loss function and hypothesis function as in the previous examples:\n","\n","- **Loss Function:** The Mean Squared Error (MSE) is often used as the loss function for linear regression:\n","\n","  $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n","\n","- **Hypothesis Function:** For linear regression, we can use the hypothesis function:\n","\n","  $$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n","\n","We can start by initializing our parameters with random values. Let's say we choose $\\theta_0 = 1$ and $\\theta_1 = 1$. We also need to choose a learning rate $\\alpha$. Let's use $\\alpha = 0.01$.\n","\n","Now we can perform one iteration of mini-batch gradient descent:\n","\n","1. **Shuffling Data:** Randomly shuffle the training dataset.\n","\n","2. **Iteration:** For the first iteration, select a mini-batch of size 2 randomly: $(x_1,y_1) = (2,4)$ and $(x_2,y_2) = (3,6)$.\n","\n","3. **Forward Pass:** Calculate the model's predictions $h_\\theta(x_1)$ and $h_\\theta(x_2)$ using the current $\\theta$ and the selected mini-batch.\n","\n","   $$ h_\\theta(x_1) = \\theta_0 + \\theta_1 x_1 = 1 + 1(2) = 3 $$\n","   \n","   $$ h_\\theta(x_2) = \\theta_0 + \\theta_1 x_2 = 1 + 1(3) = 4 $$\n","\n","4. **Loss Calculation:** Calculate the loss on the selected mini-batch.\n","\n","   $$ J(\\theta) = \\frac{1}{4} [(h_\\theta(x_1) - y_1)^2 + (h_\\theta(x_2) - y_2)^2] $$\n","   \n","   $$ J(\\theta) = \\frac{1}{4} [(3 - 4)^2 + (4 - 6)^2] $$\n","   \n","   $$ J(\\theta) = \\frac{1}{4} [1 + 4] $$\n","   \n","   $$ J(\\theta) = \\frac{5}{4} $$\n","\n","5. **Gradient Calculation:** Compute the gradient of the loss function with respect to the parameters using the selected mini-batch.\n","\n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} $$\n","   \n","   For our mini-batch of size 2, we have:\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{2} [(h_\\theta(x_1) - y_1) + (h_\\theta(x_2) - y_2)] $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{2} [(h_\\theta(x_1) - y_1) x_1 + (h_\\theta(x_2) - y_2) x_2] $$\n","   \n","   Plugging in our values, we get:\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{2} [(3 - 4) + (4 - 6)] $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{2} [(3 - 4) (2) + (4 - 6) (3)] $$\n","   \n","   Simplifying, we get:\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = -1 $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = -3 $$\n","   \n","6. **Parameter Update:** Adjust the parameters in the opposite direction of the gradient using the learning rate.\n","\n","   $$ \\theta_0 \\leftarrow \\theta_0 - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_0} $$\n","   \n","   $$ \\theta_1 \\leftarrow \\theta_1 - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_1} $$\n","   \n","   Plugging in our values, we get:\n","   \n","   $$ \\theta_0 \\leftarrow 1 - 0.01 (-1) = 1.01 $$\n","   \n","   $$ \\theta_1 \\leftarrow 1 - 0.01 (-3) = 1.03 $$\n","   \n","After this iteration, we can repeat steps 2-6 for a specified number of iterations or until convergence criteria are met.\n","\n","\n","</details>"],"metadata":{"id":"MSGH3O1fgpUe"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"svc2iPeygpUf"}},{"cell_type":"markdown","source":["<details>\n","  <summary>code</details\n","\n","\n","</details>"],"metadata":{"id":"zvXSupKDj9XQ"}},{"cell_type":"markdown","source":["# 5 Momentum"],"metadata":{"id":"FdpOCi-yUnLp"}},{"cell_type":"markdown","source":["## Theory"],"metadata":{"id":"PmqKevWcgp7B"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Theory</summary>\n","\n","\n","**Momentum** is an optimization algorithm that enhances the traditional gradient descent by **introducing the concept of inertia or momentum**.\n","- It helps accelerate the convergence of the optimization process, particularly in cases where the cost function has many narrow and steep valleys.\n","- Momentum is **often employed to mitigate the oscillations** typically observed in gradient descent. Below is a detailed explanation of momentum, including equations and a numerical example.\n","\n","**Momentum Process:**\n","\n","1. **Initialization:** Start with initial parameter values (weights and biases) randomly or with predefined values.\n","\n","2. **Initialization of Velocity:** Introduce a variable, often called velocity $((v))$, which is **initialized as a vector of zeros with the same dimensions as the parameters**.\n","\n","3. **Forward Pass:** Use the current parameters to make predictions on the training data.\n","\n","4. **Loss Calculation:** Calculate the value of the loss function (often a non-negative value that quantifies the error between predictions and actual targets).\n","\n","5. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters. The gradient represents the direction of steepest increase in the loss.\n","\n","6. **Velocity Update:** Update the velocity using the gradient and a momentum coefficient $((\\beta)$):\n","   $$ v \\leftarrow \\beta v + \\nabla J(\\theta) $$\n","\n","   Here, $$(\\nabla J(\\theta))$$ is the gradient and $(\\beta)$ is a **hyperparameter that controls the effect of momentum. Typical values for $(\\beta)$ are in the range of 0.9 to 0.99.**\n","\n","7. **Parameter Update:** Adjust the parameters in the direction of the velocity to minimize the loss. The update is determined by the learning rate $((\\alpha))$:\n","   $$ \\theta \\leftarrow \\theta - \\alpha v $$\n","\n","8. **Iteration:** Repeat steps 3-7 for a specified number of iterations or until convergence criteria are met.\n","\n","**Mathematical Formulation:**\n","\n","- **Parameters:** Let $(\\theta)$ represent the parameters of the model (weights and biases).\n","\n","- **Loss Function:** The loss function is typically denoted as $(J(\\theta))$, and it measures the difference between predicted and actual values.\n","\n","- **Gradient:** The gradient of the loss function with respect to the parameters is denoted as $(\\nabla J(\\theta))$. It is a vector that points in the direction of steepest ascent of the loss.\n","\n","- **Velocity:** The velocity vector is often denoted as $(v)$. It has the same dimensions as the parameters and is used to track the accumulated gradient information over time.\n","\n","- **Learning Rate:** The learning rate, denoted as $(\\alpha)$, controls the size of the step taken in each iteration. It is a hyperparameter that you can tune.\n","\n","- **Momentum Coefficient:** The momentum coefficient, denoted as $(\\beta)$, controls the effect of momentum on the optimization process.\n","\n","**Numerical Example:**\n","\n","Let's consider a simple convex optimization problem with a quadratic cost function:\n","\n","$$ J(\\theta) = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y) $$\n","\n","Where:\n","- $( \\theta $) is a parameter vector to be optimized.\n","- $( X $) is a design matrix of features.\n","- $( y $) is the target vector.\n","\n","The gradient of the cost function is:\n","\n","$$ \\nabla J(\\theta) = X^T(X\\theta - y) $$\n","\n","Now, let's perform one iteration of momentum optimization:\n","\n","- Initial parameters: $( \\theta = [2, 3] $)\n","- Learning rate: $( \\alpha = 0.01 $)\n","- Momentum coefficient: $( \\beta = 0.9 $)\n","\n","1. **Forward Pass:** Calculate the model's predictions $( X\\theta )$ using the current $(\\theta)$.\n","\n","2. **Loss Calculation:** Calculate the cost function $( J(\\theta) )$ using the predictions and actual targets.\n","\n","3. **Gradient Calculation:** Calculate the gradient $(\\nabla J(\\theta))$ using the formula.\n","\n","4. **Velocity Update:** Update the velocity $( v )$ using the gradient and momentum coefficient:\n","   $$ v \\leftarrow 0.9v + X^T(X\\theta - y) $$\n","\n","5. **Parameter Update:** Update the parameters using the velocity and learning rate:\n","   $$ \\theta \\leftarrow \\theta - 0.01v $$\n","\n","Repeat these steps for multiple iterations. The momentum coefficient helps smooth the optimization trajectory, and the velocity term accumulates past gradients to provide inertia, making the optimization process more stable and less susceptible to oscillations.\n","\n","This process can be implemented in code using your preferred programming language and machine learning library, such as Python with NumPy for the numerical calculations.\n","\n","![image](https://i.stack.imgur.com/epW89.jpg)\n","\n","</details>"],"metadata":{"id":"pQLh275rgp7C"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Numerical Example</summary>\n","Sure, here's a numerical example:\n","\n","Let's consider the same simple linear regression problem as before, where we want to fit a line to a set of data points. Suppose we have the same data:\n","\n","| x | y |\n","|---|---|\n","| 1 | 2 |\n","| 2 | 4 |\n","| 3 | 6 |\n","| 4 | 8 |\n","| 5 | 10 |\n","\n","Our goal is to find the line that best fits this data using momentum optimization. We can use the same loss function and hypothesis function as in the previous examples:\n","\n","- **Loss Function:** The Mean Squared Error (MSE) is often used as the loss function for linear regression:\n","\n","  $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n","\n","- **Hypothesis Function:** For linear regression, we can use the hypothesis function:\n","\n","  $$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n","\n","We can start by initializing our parameters with random values. Let's say we choose $\\theta_0 = 1$ and $\\theta_1 = 1$. We also need to choose a learning rate $\\alpha$. Let's use $\\alpha = 0.01$ and a momentum coefficient $\\beta$ of 0.9.\n","\n","Now we can perform one iteration of momentum optimization:\n","\n","1. **Initialization of Velocity:** Initialize the velocity vector \\(v\\) to zero.\n","\n","2. **Forward Pass:** Calculate the model's predictions using the current \\(\\theta\\).\n","\n","   $$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n","\n","   $$ h_\\theta(1) = 1 + 1(1) = 2 $$\n","   \n","   $$ h_\\theta(2) = 1 + 1(2) = 3 $$\n","   \n","   $$ h_\\theta(3) = 1 + 1(3) = 4 $$\n","   \n","   $$ h_\\theta(4) = 1 + 1(4) = 5 $$\n","   \n","   $$ h_\\theta(5) = 1 + 1(5) = 6 $$\n","\n","3. **Loss Calculation:** Calculate the loss on the entire dataset.\n","\n","   $$ J(\\theta) = \\frac{1}{10} [(2-2)^2 + (3-4)^2 + (4-6)^2 + (5-8)^2 + (6-10)^2] = \\frac{55}{10} = 5.5 $$\n","\n","4. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters using the entire dataset.\n","\n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) = \\frac{1}{5} [(2-2) + (3-4) + (4-6) + (5-8) + (6-10)] = -3 $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} = \\frac{1}{5} [(2-2)(1) + (3-4)(2) + (4-6)(3) + (5-8)(4) + (6-10)(5)] = -7 $$\n","\n","5. **Velocity Update:** Update the velocity using the gradient and a momentum coefficient of \\(\\beta\\) = 0.9:\n","\n","   $$ v_0 \\leftarrow 0.9 v_0 - 0.01 (-3) = 0.03 $$\n","   \n","   $$ v_1 \\leftarrow 0.9 v_1 - 0.01 (-7) = 0.07 $$\n","\n","6. **Parameter Update:** Adjust the parameters in the direction of the velocity to minimize the loss.\n","\n","   $$ \\theta_0 \\leftarrow \\theta_0 + v_0 = 1 + 0.03 = 1.03 $$\n","   \n","   $$ \\theta_1 \\leftarrow \\theta_1 + v_1 = 1 + 0.07 = 1.07 $$\n","\n","7. **Iteration:** Repeat steps 2-6 for a specified number of iterations or until convergence criteria are met.\n","\n","We can continue iterating until convergence criteria are met, such as when the change in loss between iterations falls below a certain threshold.\n","\n","</details>"],"metadata":{"id":"u-a30m_Fgp7C"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"201hGtPMgp7D"}},{"cell_type":"markdown","source":["<details>\n","  <summary>code</summary>\n","\n","To implement the Momentum optimization algorithm in PyTorch, you can use the `optim.SGD` optimizer class and set the `momentum` parameter. Here's an example of how to write code for Momentum in PyTorch:\n","\n","1. Import the necessary libraries:\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","```\n","\n","2. Define your model using the `nn.Module` class. You can create a simple linear regression model as an example:\n","```python\n","class LinearRegression(nn.Module):\n","    def __init__(self):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","```\n","\n","3. Define your training data and labels:\n","```python\n","x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n","y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n","```\n","\n","4. Initialize an instance of your model:\n","```python\n","model = LinearRegression()\n","```\n","\n","5. Define your loss function:\n","```python\n","criterion = nn.MSELoss()\n","```\n","\n","6. Define your optimizer with Momentum:\n","```python\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","```\n","\n","7. Perform the training loop:\n","```python\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    # Forward pass\n","    outputs = model(x_train)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print progress\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","```\n","\n","In this example, we iterate through a fixed number of epochs. In each epoch, we perform a forward pass to get the predicted outputs, calculate the loss between the predicted outputs and the true labels, perform backpropagation to compute the gradients, and update the model's parameters using the optimizer with Momentum.\n","\n","Note: Make sure to convert your data to tensors and move them to the appropriate device (e.g., GPU) if available.\n","\n","</details>"],"metadata":{"id":"N9jRFPtqj98k"}},{"cell_type":"markdown","source":["# 6 Adagrad (Adaptive Gradient Algorithm)"],"metadata":{"id":"aWZzo1T8UoMD"}},{"cell_type":"markdown","source":["## Theory"],"metadata":{"id":"uIfKc5JNgqcK"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Theory</summary>\n","\n","\n","**Adagrad (Adaptive Gradient Algorithm)** is an optimization algorithm that adapts the learning rate for each parameter during the training process.\n","- It is particularly useful when dealing with sparse data or problems where certain parameters require smaller learning rates.\n","- Adagrad **maintains a per-parameter learning rate that scales with the historical gradient information for each parameter**. Here's a detailed explanation of Adagrad, including equations and a numerical example.\n","\n","**Adagrad Process:**\n","\n","1. **Initialization:** Start with initial parameter values (weights and biases) randomly or with predefined values.\n","\n","2. **Initialization of Accumulated Squared Gradient:** Introduce a variable, often called $( G $), which is initialized as a vector of zeros with the same dimensions as the parameters.\n","\n","3. **Forward Pass:** Use the current parameters to make predictions on the training data.\n","\n","4. **Loss Calculation:** Calculate the value of the loss function (often a non-negative value that quantifies the error between predictions and actual targets).\n","\n","5. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters. The gradient represents the direction of steepest increase in the loss.\n","\n","6. **Accumulated Squared Gradient Update:** Update the accumulated squared gradient using the square of the gradient:\n","   $$ G \\leftarrow G + (\\nabla J(\\theta))^2 $$\n","\n","7. **Parameter Update:** Adjust the parameters using the accumulated squared gradient to adapt the learning rate. The update is determined by the learning rate ($(\\alpha$)):\n","   $$ \\theta \\leftarrow \\theta - \\frac{\\alpha}{\\sqrt{G + \\epsilon}} \\nabla J(\\theta) $$\n","\n","   Here, $(\\nabla J(\\theta)$) is the gradient, $(\\alpha$) is the learning rate, and $(\\epsilon$) is a small constant (e.g., $(10^{-8}$)) added for numerical stability.\n","\n","8. **Iteration:** Repeat steps 3-7 for a specified number of iterations or until convergence criteria are met.\n","\n","**Mathematical Formulation:**\n","\n","- **Parameters:** Let $(\\theta$) represent the parameters of the model (weights and biases).\n","\n","- **Loss Function:** The loss function is typically denoted as $(J(\\theta)$), and it measures the difference between predicted and actual values.\n","\n","- **Gradient:** The gradient of the loss function with respect to the parameters is denoted as $(\\nabla J(\\theta)$). It is a vector that points in the direction of steepest ascent of the loss.\n","\n","- **Accumulated Squared Gradient:** The accumulated squared gradient is often denoted as $(G$). It has the same dimensions as the parameters and is used to track the historical gradient information.\n","\n","- **Learning Rate:** The learning rate, denoted as $(\\alpha$), controls the size of the step taken in each iteration. It is a hyperparameter that you can tune.\n","\n","- **Numerical Stability Term:** $(\\epsilon$) is a small constant added to the denominator for numerical stability.\n","\n","**Numerical Example:**\n","\n","Let's consider a simple convex optimization problem with a quadratic cost function:\n","\n","$$ J(\\theta) = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y) $$\n","\n","Where:\n","- $( \\theta $) is a parameter vector to be optimized.\n","- $( X $) is a design matrix of features.\n","- $( y $) is the target vector.\n","\n","The gradient of the cost function is:\n","\n","$$ \\nabla J(\\theta) = X^T(X\\theta - y) $$\n","\n","Now, let's perform one iteration of Adagrad:\n","\n","- Initial parameters: $( \\theta = [2, 3] $)\n","- Learning rate: $( \\alpha = 0.01 $)\n","- Numerical stability term: $( \\epsilon = 10^{-8} $)\n","\n","1. **Forward Pass:** Calculate the model's predictions $( X\\theta $) using the current $(\\theta$).\n","\n","2. **Loss Calculation:** Calculate the cost function $( J(\\theta) $) using the predictions and actual targets.\n","\n","3. **Gradient Calculation:** Calculate the gradient \\(\\nabla J(\\theta)\\) using the formula.\n","\n","4. **Accumulated Squared Gradient Update:** Update the accumulated squared gradient $( G $) using the square of the gradient:\n","   $$ G \\leftarrow G + (X^T(X\\theta - y))^2 $$\n","\n","5. **Parameter Update:** Update the parameters using the accumulated squared gradient and learning rate:\n","   $$ \\theta \\leftarrow \\theta - \\frac{0.01}{\\sqrt{G + 10^{-8}}} X^T(X\\theta - y) $$\n","\n","Repeat these steps for multiple iterations. Adagrad adapts the learning rate for each parameter based on the historical gradient information, which can be particularly beneficial in situations where different parameters require different learning rates for convergence.\n","\n","![image](https://dmitrijskass.netlify.app/2021/04/15/adagrad-adaptive-gradient-algorithm/images/algo.png)\n","\n","![\n","![image](https://image.slidesharecdn.com/anoverviewofgradientdescentoptimizationalgorithms-170414055411/95/an-overview-of-gradient-descent-optimization-algorithms-45-638.jpg?cb=1492149859)\n","\n","</details>"],"metadata":{"id":"1gLuLY5ogqcL"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Numerical Example</summary>\n","Certainly! Here's a numerical example to illustrate how Adagrad works:\n","\n","Let's consider a simple linear regression problem where we want to fit a line to a set of data points. Suppose we have the following data:\n","\n","| x | y |\n","|---|---|\n","| 1 | 2 |\n","| 2 | 4 |\n","| 3 | 6 |\n","| 4 | 8 |\n","| 5 | 10 |\n","\n","Our goal is to find the line that best fits this data using Adagrad optimization. We can use the same loss function and hypothesis function as in the previous examples:\n","\n","- **Loss Function:** The Mean Squared Error (MSE) is often used as the loss function for linear regression:\n","\n","  $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n","\n","- **Hypothesis Function:** For linear regression, we can use the hypothesis function:\n","\n","  $$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n","\n","We can start by initializing our parameters with random values. Let's say we choose \\( \\theta_0 = 1 \\) and \\( \\theta_1 = 1 \\). We also need to choose a learning rate \\( \\alpha \\). Let's use \\( \\alpha = 0.1 \\).\n","\n","Now we can perform one iteration of Adagrad optimization:\n","\n","1. **Initialization of Accumulated Squared Gradient:** Initialize the accumulated squared gradient vector \\( G \\) to zero.\n","\n","2. **Forward Pass:** Calculate the model's predictions using the current \\( \\theta \\).\n","\n","   $$ h_\\theta(x) = \\theta_0 + \\theta_1 x $$\n","\n","   $$ h_\\theta(1) = 1 + 1(1) = 2 $$\n","   \n","   $$ h_\\theta(2) = 1 + 1(2) = 3 $$\n","   \n","   $$ h_\\theta(3) = 1 + 1(3) = 4 $$\n","   \n","   $$ h_\\theta(4) = 1 + 1(4) = 5 $$\n","   \n","   $$ h_\\theta(5) = 1 + 1(5) = 6 $$\n","\n","3. **Loss Calculation:** Calculate the loss using the MSE formula:\n","\n","   $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 $$\n","\n","   $$ J(\\theta) = \\frac{1}{10} [(2-2)^2 + (3-4)^2 + (4-6)^2 + (5-8)^2 + (6-10)^2] $$\n","   \n","   $$ J(\\theta) = \\frac{1}{10} [0 + 1 + 4 + 9 + 16] $$\n","   \n","   $$ J(\\theta) = \\frac{30}{10} = 3 $$\n","\n","4. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters \\( \\theta_0 \\) and \\( \\theta_1 \\).\n","\n","   To simplify the calculation, we can use the partial derivatives:\n","\n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x^{(i)} $$\n","\n","   Plugging in the values:\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{5} [(2-2) + (3-4) + (4-6) + (5-8) + (6-10)] = -\\frac{7}{5} $$\n","   \n","   $$ \\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{1}{5} [(2-2)(1) + (3-4)(2) + (4-6)(3) + (5-8)(4) + (6-10)(5)] = -\\frac{19}{5} $$\n","\n","5. **Accumulated Squared Gradient Update:** Update the accumulated squared gradient using the square of the gradient:\n","\n","   $$ G_{\\text{new}} = G_{\\text{old}} + (\\nabla J(\\theta))^2 $$\n","   \n","   For simplicity, let's assume \\( G_{\\text{old}} = [0, 0] \\).\n","   \n","   $$ G_{\\text{new}} = [0, 0] + \\left[ \\left(-\\frac{7}{5}\\right)^2, \\left(-\\frac{19}{5}\\right)^2 \\right] $$\n","   \n","   $$ G_{\\text{new}} = \\left[ \\frac{49}{25}, \\frac{361}{25} \\right] $$\n","\n","6. **Parameter Update:** Adjust the parameters using the accumulated squared gradient to adapt the learning rate.\n","\n","   $$ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\frac{\\alpha}{\\sqrt{G_{\\text{new}} + \\epsilon}} \\nabla J(\\theta) $$\n","   \n","   Let's assume \\( \\epsilon = 10^{-8} \\).\n","   \n","   $$ \\theta_{\\text{new}} = [1, 1] - \\frac{0.1}{\\sqrt{\\left[ \\frac{49}{25}, \\frac{361}{25} \\right] + 10^{-8}}} [-\\frac{7}{5}, -\\frac{19}{5}] $$\n","   \n","   After performing the calculations, we get:\n","   \n","   $$ \\theta_{\\text{new}} = [0.998, 0.996] $$\n","\n","7. **Iteration:** Repeat steps 3-6 for a specified number of iterations or until convergence criteria are met.\n","\n","This process is repeated for multiple iterations until the parameters converge to their optimal values that minimize the loss function.\n","\n","</details>"],"metadata":{"id":"kpF0lrChgqcL"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"WLYSaiY5gqcL"}},{"cell_type":"markdown","source":["<details>\n","  <summary>code</summary>\n","To implement the Adagrad optimization algorithm in PyTorch, you can use the `optim.Adagrad` optimizer class. Here's an example of how to write code for Adagrad in PyTorch:\n","\n","1. Import the necessary libraries:\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","```\n","\n","2. Define your model using the `nn.Module` class. You can create a simple linear regression model as an example:\n","```python\n","class LinearRegression(nn.Module):\n","    def __init__(self):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","```\n","\n","3. Define your training data and labels:\n","```python\n","x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n","y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n","```\n","\n","4. Initialize an instance of your model:\n","```python\n","model = LinearRegression()\n","```\n","\n","5. Define your loss function:\n","```python\n","criterion = nn.MSELoss()\n","```\n","\n","6. Define your optimizer with Adagrad:\n","```python\n","optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n","```\n","\n","7. Perform the training loop:\n","```python\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    # Forward pass\n","    outputs = model(x_train)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print progress\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","```\n","\n","In this example, we iterate through a fixed number of epochs. In each epoch, we perform a forward pass to get the predicted outputs, calculate the loss between the predicted outputs and the true labels, perform backpropagation to compute the gradients, and update the model's parameters using the optimizer with Adagrad.\n","\n","Note: Make sure to convert your data to tensors and move them to the appropriate device (e.g., GPU) if available.\n","\n","</details>"],"metadata":{"id":"K2PYXiCSj-xs"}},{"cell_type":"markdown","source":["# 7 RMSprop (Root Mean Square Propagation)"],"metadata":{"id":"V9hoQ7_gUtoz"}},{"cell_type":"markdown","source":["## Theory"],"metadata":{"id":"0wUxFM4Qgq7i"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Theory</summary>\n","\n","**RMSprop (Root Mean Square Propagation)** is an optimization algorithm that adapts the learning rate for each parameter during training. It addresses some of the limitations of Adagrad by introducing an exponentially moving average of the squared gradients, which helps stabilize the learning rate and avoid diminishing it too quickly. Here's a detailed explanation of RMSprop, including equations and a numerical example.\n","\n","### RMSprop Process:\n","\n","1. **Initialization:** Start with initial parameter values (weights and biases) randomly or with predefined values.\n","\n","2. **Initialization of Moving Average of Squared Gradient:** Introduce a variable, often called $E[g^2]$, which is initialized as a vector of zeros with the same dimensions as the parameters.\n","\n","3. **Forward Pass:** Use the current parameters to make predictions on the training data.\n","\n","4. **Loss Calculation:** Calculate the value of the loss function (often a non-negative value that quantifies the error between predictions and actual targets).\n","\n","5. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters. The gradient represents the direction of steepest increase in the loss.\n","\n","6. **Moving Average Update:** Update the moving average of the squared gradient using a decay term ($\\rho$):\n","   $$ E[g^2] \\leftarrow \\rho E[g^2] + (1 - \\rho)(\\nabla J(\\theta))^2 $$\n","\n","   Here, $\\nabla J(\\theta)$ is the gradient, and $\\rho$ is a decay parameter (typically close to 1, e.g., 0.9).\n","\n","7. **Parameter Update:** Adjust the parameters using the moving average of the squared gradient to adapt the learning rate. The update is determined by the learning rate ($\\alpha$) and an added small constant for numerical stability ($\\epsilon$):\n","   $$ \\theta \\leftarrow \\theta - \\frac{\\alpha}{\\sqrt{E[g^2] + \\epsilon}} \\nabla J(\\theta) $$\n","\n","8. **Iteration:** Repeat steps 3-7 for a specified number of iterations or until convergence criteria are met.\n","\n","### Mathematical Formulation:\n","\n","- **Parameters:** Let $\\theta$ represent the parameters of the model (weights and biases).\n","\n","- **Loss Function:** The loss function is typically denoted as $J(\\theta)$, and it measures the difference between predicted and actual values.\n","\n","- **Gradient:** The gradient of the loss function with respect to the parameters is denoted as $\\nabla J(\\theta)$. It is a vector that points in the direction of steepest ascent of the loss.\n","\n","- **Moving Average of Squared Gradient:** The moving average of the squared gradient is often denoted as $E[g^2]$. It has the same dimensions as the parameters and is used to adapt the learning rate.\n","\n","- **Learning Rate:** The learning rate, denoted as $\\alpha$, controls the size of the step taken in each iteration. It is a hyperparameter that you can tune.\n","\n","- **Decay Parameter:** $\\rho$ is a decay parameter that controls the rate at which the moving average is updated.\n","\n","- **Numerical Stability Term:** $\\epsilon$ is a small constant added to the denominator for numerical stability.\n","\n","### Numerical Example:\n","\n","Let's consider a simple convex optimization problem with a quadratic cost function, similar to the previous examples:\n","\n","$$ J(\\theta) = \\frac{1}{2}(X\\theta - y)^T(X\\theta - y) $$\n","\n","Where:\n","- $\\theta$ is a parameter vector to be optimized.\n","- $X$ is a design matrix of features.\n","- $y$ is the target vector.\n","\n","The gradient of the cost function is:\n","\n","$$ \\nabla J(\\theta) = X^T(X\\theta - y) $$\n","\n","Now, let's perform one iteration of RMSprop:\n","\n","- Initial parameters: $\\theta = [2, 3]$\n","- Learning rate: $\\alpha = 0.01$\n","- Decay parameter: $\\rho = 0.9$\n","- Numerical stability term: $\\epsilon = 10^{-8}$\n","\n","1. **Forward Pass:** Calculate the model's predictions $X\\theta$ using the current $\\theta$.\n","\n","2. **Loss Calculation:** Calculate the cost function $J(\\theta)$ using the predictions and actual targets.\n","\n","3. **Gradient Calculation:** Calculate the gradient $\\nabla J(\\theta)$ using the formula.\n","\n","4. **Moving Average Update:** Update the moving average of the squared gradient $E[g^2]$ using the decay parameter:\n","   $$ E[g^2] \\leftarrow 0.9E[g^2] + 0.1(X^T(X\\theta - y))^2 $$\n","\n","5. **Parameter Update:** Update the parameters using the moving average of the squared gradient, learning rate, and numerical stability term:\n","   $$ \\theta \\leftarrow \\theta - \\frac{0.01}{\\sqrt{0.9E[g^2] + 10^{-8}}} X^T(X\\theta - y) $$\n","\n","Repeat these steps for multiple iterations. RMSprop adapts the learning rate for each parameter based on the historical gradient information, providing more stability and preventing the learning rate from decreasing too rapidly. This makes it particularly useful in training deep neural networks.\n","\n","image](https://cdn-images-1.medium.com/max/800/1*7hr6VK6jSmT9fD2W50XkrA.png)\n","\n","</detials>"],"metadata":{"id":"oPQXaLe5gq7i"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Numerical Example</summary>\n","\n","Certainly! Here's a numerical example to illustrate how RMSprop works:\n","\n","Let's consider a simple linear regression problem where we want to fit a line to a set of data points. Suppose we have the following data:\n","\n","| x | y |\n","|---|---|\n","| 1 | 2 |\n","| 2 | 4 |\n","| 3 | 6 |\n","| 4 | 8 |\n","| 5 | 10 |\n","\n","Our goal is to find the line that best fits this data using RMSprop optimization. We can use the same loss function and hypothesis function as in the previous examples:\n","\n","- **Loss Function:** The Mean Squared Error (MSE) is often used as the loss function for linear regression:\n","\n","  \\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]\n","\n","- **Hypothesis Function:** For linear regression, we can use the hypothesis function:\n","\n","  \\[ h_\\theta(x) = \\theta_0 + \\theta_1 x \\]\n","\n","We can start by initializing our parameters with random values. Let's say we choose \\( \\theta_0 = 1 \\) and \\( \\theta_1 = 1 \\). We also need to choose a learning rate \\( \\alpha \\) and a decay parameter \\( \\rho \\). Let's use \\( \\alpha = 0.1 \\) and \\( \\rho = 0.9 \\).\n","\n","Now we can perform one iteration of RMSprop optimization:\n","\n","1. **Initialization of Moving Average of Squared Gradient:** Initialize the variable \\( E[g^2] \\) to zero.\n","\n","2. **Forward Pass:** Calculate the model's predictions using the current \\( \\theta \\).\n","\n","   \\[ h_\\theta(x) = \\theta_0 + \\theta_1 x \\]\n","\n","   \\[ h_\\theta(1) = 1 + 1(1) = 2 \\]\n","   \n","   \\[ h_\\theta(2) = 1 + 1(2) = 3 \\]\n","   \n","   \\[ h_\\theta(3) = 1 + 1(3) = 4 \\]\n","   \n","   \\[ h_\\theta(4) = 1 + 1(4) = 5 \\]\n","   \n","   \\[ h_\\theta(5) = 1 + 1(5) = 6 \\]\n","\n","3. **Loss Calculation:** Calculate the value of the loss function.\n","\n","   \\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]\n","   \n","   Substituting the values:\n","   \n","   \\[ J(\\theta) = \\frac{1}{10} [(2-2)^2 + (3-4)^2 + (4-6)^2 + (5-8)^2 + (6-10)^2] = 7.5 \\]\n","\n","4. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters.\n","\n","   The gradient for \\( \\theta_0 \\) is:\n","   \n","   \\[ \\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) = -3.6 \\]\n","   \n","   The gradient for \\( \\theta_1 \\) is:\n","   \n","   \\[ \\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)} = -7.6 \\]\n","\n","5. **Moving Average Update:** Update the moving average of the squared gradient using the decay parameter \\( \\rho \\).\n","\n","   For \\( E[g^2]_{\\theta_0} \\):\n","   \n","   \\[ E[g^2]_{\\theta_0} = 0.9 E[g^2]_{\\theta_0} + 0.1 (\\frac{\\partial J}{\\partial \\theta_0})^2 = (0.9)(0) + (0.1)(-3.6)^2 = 1.296 \\]\n","   \n","   For \\( E[g^2]_{\\theta_1} \\):\n","   \n","   \\[ E[g^2]_{\\theta_1} = 0.9 E[g^2]_{\\theta_1} + 0.1 (\\frac{\\partial J}{\\partial \\theta_1})^2 = (0.9)(0) + (0.1)(-7.6)^2 = 5.776 \\]\n","\n","6. **Parameter Update:** Adjust the parameters using the moving average of the squared gradient, learning rate \\( \\alpha \\), and a small constant \\( \\epsilon \\) for numerical stability.\n","\n","   For \\( \\theta_0 \\):\n","   \n","   \\[ \\theta_0 = \\theta_0 - \\frac{\\alpha}{\\sqrt{E[g^2]_{\\theta_0} + \\epsilon}} (\\frac{\\partial J}{\\partial \\theta_0}) = 1 - \\frac{0.1}{\\sqrt{1.296 + 10^{-8}}} (-3.6) = 1.0002778 \\]\n","   \n","   For \\( \\theta_1 \\):\n","   \n","   \\[ \\theta_1 = \\theta_1 - \\frac{\\alpha}{\\sqrt{E[g^2]_{\\theta_1} + \\epsilon}} (\\frac{\\partial J}{\\partial [omitted]\n","\n","\n","</details>"],"metadata":{"id":"74RpjRK2gq7t"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"--01UHV7gq7t"}},{"cell_type":"markdown","source":["<details>\n","  <summary>code</summary>\n","  \n","To implement the RMSprop optimization algorithm in PyTorch, you can use the `optim.RMSprop` optimizer class. Here's an example of how to write code for RMSprop in PyTorch:\n","\n","1. Import the necessary libraries:\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","```\n","\n","2. Define your model using the `nn.Module` class. You can create a simple linear regression model as an example:\n","```python\n","class LinearRegression(nn.Module):\n","    def __init__(self):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","```\n","\n","3. Define your training data and labels:\n","```python\n","x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n","y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n","```\n","\n","4. Initialize an instance of your model:\n","```python\n","model = LinearRegression()\n","```\n","\n","5. Define your loss function:\n","```python\n","criterion = nn.MSELoss()\n","```\n","\n","6. Define your optimizer with RMSprop:\n","```python\n","optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n","```\n","\n","7. Perform the training loop:\n","```python\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    # Forward pass\n","    outputs = model(x_train)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print progress\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","```\n","\n","In this example, we iterate through a fixed number of epochs. In each epoch, we perform a forward pass to get the predicted outputs, calculate the loss between the predicted outputs and the true labels, perform backpropagation to compute the gradients, and update the model's parameters using the optimizer with RMSprop.\n","\n","Note: Make sure to convert your data to tensors and move them to the appropriate device (e.g., GPU) if available.\n","\n","</details>"],"metadata":{"id":"FUUAurNSj_tE"}},{"cell_type":"markdown","source":["# 8 Adam (Adaptive Moment Estimation)"],"metadata":{"id":"94czgmCRUwri"}},{"cell_type":"markdown","source":["## Theory"],"metadata":{"id":"P1PEW0wKgrpC"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Theory</summary>\n","\n","**Adam (Adaptive Moment Estimation)** is an optimization algorithm that combines the benefits of both RMSprop and momentum. It adapts the learning rate for each parameter and includes two moving averages, one for the gradient and another for the squared gradient. Adam is particularly effective in optimizing complex, non-convex functions like those encountered in deep learning. Here's a detailed explanation of Adam, including equations and a numerical example.\n","\n","**Adam Process:**\n","\n","1. **Initialization:** Start with initial parameter values (weights and biases) randomly or with predefined values.\n","\n","2. **Initialization of First Moment and Second Moment Variables:** Introduce two variables, often called $m$ and $v$, which are initialized as vectors of zeros with the same dimensions as the parameters.\n","\n","3. **Initialization of Time Step Variable:** Introduce a variable $t$ that starts at 1 and is used to keep track of the iterations.\n","\n","4. **Forward Pass:** Use the current parameters to make predictions on the training data.\n","\n","5. **Loss Calculation:** Calculate the value of the loss function (often a non-negative value that quantifies the error between predictions and actual targets).\n","\n","6. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters. The gradient represents the direction of steepest increase in the loss.\n","\n","7. **First Moment Update:** Update the first moment ($m$) using momentum, similar to the momentum algorithm:\n","   $$m \\leftarrow \\beta_1 \\cdot m + (1 - \\beta_1) \\cdot \\nabla J(\\theta)$$\n","\n","   Here, $\\nabla J(\\theta)$ is the gradient, and $\\beta_1$ is a hyperparameter typically close to 1 (e.g., 0.9).\n","\n","8. **Second Moment Update:** Update the second moment ($v$) using RMSprop-like adaptation:\n","   $$v \\leftarrow \\beta_2 \\cdot v + (1 - \\beta_2) \\cdot (\\nabla J(\\theta))^2$$\n","\n","   Here, $(\\nabla J(\\theta))^2$ is the element-wise square of the gradient, and $\\beta_2$ is a hyperparameter typically close to 1 (e.g., 0.999).\n","\n","9. **Bias Correction:** Correct the bias of the first and second moment estimates:\n","   $$\\hat{m} = \\frac{m}{1 - \\beta_1^t}$$\n","   $$\\hat{v} = \\frac{v}{1 - \\beta_2^t}$$\n","\n","   Here, $t$ is the time step.\n","\n","10. **Parameter Update:** Adjust the parameters using the corrected first and second moments to adapt the learning rate. The update is determined by the learning rate ($\\alpha$) and an added small constant for numerical stability ($\\epsilon$):\n","    $$\\theta \\leftarrow \\theta - \\frac{\\alpha}{\\sqrt{\\hat{v} + \\epsilon}} \\hat{m}$$\n","\n","11. **Iteration:** Increment the time step ($t$) and repeat steps 4-10 for a specified number of iterations or until convergence criteria are met.\n","\n","**Mathematical Formulation:**\n","\n","- **Parameters:** Let $\\theta$ represent the parameters of the model (weights and biases).\n","\n","- **Loss Function:** The loss function is typically denoted as $J(\\theta)$, and it measures the difference between predicted and actual values.\n","\n","- **Gradient:** The gradient of the loss function with respect to the parameters is denoted as $\\nabla J(\\theta)$. It is a vector that points in the direction of steepest ascent of the loss.\n","\n","- **First Moment ($m$):** $m$ is a moving average of the gradients.\n","\n","- **Second Moment ($v$):** $v$ is a moving average of the squared gradients.\n","\n","- **Learning Rate:** The learning rate, denoted as $\\alpha$, controls the size of the step taken in each iteration. It is a hyperparameter that you can tune.\n","\n","- **Decay Parameters:** $\\beta_1$ and $\\beta_2$ are decay parameters that control the rate at which the moving averages are updated.\n","\n","- **Numerical Stability Term:** $\\epsilon$ is a small constant added to the denominator for numerical stability.\n","\n","**Numerical Example:**\n","\n","Let's perform one iteration of Adam with a simple convex optimization problem and a quadratic cost function, similar to the previous examples:\n","\n","- Initial parameters: $\\theta = [2, 3]$\n","- Learning rate: $\\alpha = 0.01$\n","- First moment decay ($\\beta_1$): 0.9\n","- Second moment decay ($\\beta_2$): 0.999\n","- Numerical stability term ($\\epsilon$): $10^{-8}$\n","\n","1. **Forward Pass:** Calculate the model's predictions $X\\theta$ using the current $\\theta$.\n","\n","2. **Loss Calculation:** Calculate the cost function $J(\\theta)$ using the predictions and actual targets.\n","\n","3. **Gradient Calculation:** Calculate the gradient $\\nabla J(\\theta)$ using the formula.\n","\n","4. **First Moment Update:** Update the first moment $m$ using momentum:\n","   $$m \\leftarrow 0.9 \\cdot m + 0.1 \\cdot \\nabla J(\\theta)$$\n","\n","5. **Second Moment Update:** Update the second moment $v$ using RMSprop-like adaptation:\n","   $$v \\leftarrow 0.999 \\cdot v + 0.001 \\cdot (\\nabla J(\\theta))^2$$\n","\n","6. **Bias Correction:** Correct the bias of the first and second moment estimates:\n","   $$\\hat{m} = \\frac{m}{1 - 0.9^t}$$\n","   $$\\hat{v} = \\frac{v}{1 - 0.999^t}$$\n","\n","7. **Parameter Update:** Update the parameters using the corrected first and second moments, learning rate, and numerical stability term:\n","   $$\\theta \\leftarrow \\theta - \\frac{0.01}{\\sqrt{10^{-8} + \\hat{v}}} \\hat{m}$$\n","\n","Repeat these steps for multiple iterations. Adam adapts the learning rate for each parameter based on both the first and second moments, providing effective and stable optimization in a wide range of optimization tasks, including training deep neural networks.\n","\n","\n","</details>"],"metadata":{"id":"KM5-5zVUgrpD"}},{"cell_type":"markdown","source":["<details>\n","  <summary>Numerical Example</summary>\n","\n","\n","Suppose we have a neural network with 2 weights, $w_1$ and $w_2$, and a bias term $b$. We initialize the weights and bias to the following values:\n","\n","$$w_1 = 0.5, w_2 = -0.8, b = 0.2$$\n","\n","We also set the hyperparameters as follows:\n","\n","$$\\alpha = 0.01, \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$$\n","\n","Let's assume that we have a training example with input $x = [1, -1]$ and target output $y = 1$. We use the mean squared error (MSE) loss function:\n","\n","$$J(w_1, w_2, b) = \\frac{1}{2}(y - f(x))^2$$\n","\n","where $f(x)$ is the output of the neural network for input $x$.\n","\n","We can now perform the Adam algorithm as follows:\n","\n","1. Initialize $m_1 = m_2 = v_1 = v_2 = v_b = 0$ and $t = 0$.\n","2. Compute the gradient of the loss with respect to the parameters using backpropagation:\n","   $$\\nabla J(w_1, w_2, b) = [(y - f(x))f'(x)]\\begin{bmatrix}x_1 \\\\ x_2 \\\\ 1\\end{bmatrix}$$\n","   where $f'(x)$ is the derivative of the activation function with respect to its input.\n","   For simplicity, let's assume that $f(x) = w_1x_1 + w_2x_2 + b$ and $f'(x) = 1$.\n","   Plugging in the values, we get:\n","   $$\\nabla J(w_1, w_2, b) = (y - (0.5 - 0.8) + 0.2)\\begin{bmatrix}1 \\\\ -1 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}0.3 \\\\ -0.3 \\\\ 0.3\\end{bmatrix}$$\n","3. Increment the time step: $t \\leftarrow t + 1$.\n","4. Update the first moment estimates:\n","   \\begin{align*}\n","   m_1 &\\leftarrow \\beta_1 m_1 + (1 - \\beta_1) \\nabla J(w_1, w_2, b)_1 \\\\\n","       &= 0.9 \\cdot 0 + 0.1 \\cdot 0.3 \\\\\n","       &= 0.03 \\\\\n","   m_2 &\\leftarrow \\beta_1 m_2 + (1 - \\beta_1) \\nabla J(w_1, w_2, b)_2 \\\\\n","       &= 0.9 \\cdot 0 + 0.1 \\cdot (-0.3) \\\\\n","       &= -0.03 \\\\\n","   m_b &\\leftarrow \\beta_1 m_b + (1 - \\beta_1) \\nabla J(w_1, w_2, b)_3 \\\\\n","       &= 0.9 \\cdot 0 + 0.1 \\cdot 0.3 \\\\\n","       &= 0.03\n","   \\end{align*}\n","5. Update the second moment estimates:\n","   \\begin{align*}\n","   v_1 &\\leftarrow \\beta_2 v_1 + (1 - \\beta_2) (\\nabla J(w_1, w_2, b)_1)^2 \\\\\n","       &= 0.999 \\cdot 0 + 0.001 \\cdot (0.3)^2 \\\\\n","       &= 9 \\times 10^{-5} \\\\\n","   v_2 &\\leftarrow \\beta_2 v_2 + (1 - \\beta_2) (\\nabla J(w_1, w_2, b)_2)^2 \\\\\n","       &= 0.999 \\cdot 0 + 0.001 \\cdot (-0.3)^2 \\\\\n","       &= 9 \\times 10^{-5} \\\\\n","   v_b &\\leftarrow \\beta_2 v_b + (1 - \\beta_2) (\\nabla J(w_1, w_2, b)_3)^2 \\\\\n","       &= 0.999 \\cdot 0 + 0.001 \\cdot (0.3)^2 \\\\\n","       &= 9 \\times 10^{-5}\n","   \\end{align*}\n","6. Compute the bias-corrected first and second moment estimates:\n","   \\begin{align*}\n","   \\hat{m}_1 &= \\frac{m_1}{1 - \\beta_1^t} = \\frac{0.03}{1 - 0.9^1} = 0.3 \\\\\n","   \\hat{m}_2 &= \\frac{m_2}{1 - \\beta_1^t} = \\frac{-0.03}{1 - 0.9^1} = -0.3 \\\\\n","   \\hat{m}_b &= \\frac{m_b}{1 - \\beta_1^t} = \\frac{0.03}{1 - 0.9^1} = 0.3 \\\\\n","   \\hat{v}_1 &= \\frac{v_1}{1 - \\beta_2^t} = \\frac{9\\times10^{-5}}{1 - 0.999^1} = 4.5\\times10^{-3} \\\\\n","   \\hat{v}_2 &= \\frac{v_2}{1 - \\beta_2^t} = \\frac{9\\times10^{-5}}{1 - 0.999^1} = 4.5\\times10^{-3} \\\\\n","   \\hat{v}_b &= \\frac{v_b}{1 - \\beta_2^t} = \\frac{9\\times10^{-5}}{1 - 0.999^1} = 4.5\\times10^{-3}\n","   \\end{align*}\n","7. Update the parameters:\n","   $$w_1 \\leftarrow w_1 - \\alpha\\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} = 0.5 - (0.01)(\\frac{0.3}{\\sqrt{4.5\\times10^{-3}} + 10^{-8}}) = 0.498$$\n","   \n","   $$w_2 \\leftarrow w_2 - \\alpha\\frac{\\hat{m}_2}{\\sqrt{\\hat{v}_2} + \\epsilon} = -0.8 - (0.01)(\\frac{-0.3}{\\sqrt{4.5\\times10^{-3}} + 10^{-8}}) = -0.802$$\n","   \n","   $$b \\leftarrow b - \\alpha\\frac{\\hat{m}_b}{\\sqrt{\\hat{v}_b} + \\epsilon} = 0.2 - (0.01)(\\frac{0.3}{\\sqrt{4.5\\times10^{-3}} + 10^{-8}}) = 0.198$$\n","8. Repeat steps 2-7 for additional training examples or until convergence criteria are met.\n","\n","This is just one iteration of the Adam algorithm for a single training example, but it gives an idea of how the algorithm works to update the parameters based on the gradients and moving averages of the gradients and squared gradients over time.\n","\n","</details>"],"metadata":{"id":"MQlnqErvgrpE"}},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"XP3G2kLkgrpF"}},{"cell_type":"markdown","source":["<details>\n","  <summary>code</summary>\n","\n","To implement the Adam optimization algorithm in PyTorch, you can use the `optim.Adam` optimizer class. Here's an example of how to write code for Adam in PyTorch:\n","\n","1. Import the necessary libraries:\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","```\n","\n","2. Define your model using the `nn.Module` class. You can create a simple linear regression model as an example:\n","```python\n","class LinearRegression(nn.Module):\n","    def __init__(self):\n","        super(LinearRegression, self).__init__()\n","        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output feature\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","```\n","\n","3. Define your training data and labels:\n","```python\n","x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n","y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n","```\n","\n","4. Initialize an instance of your model:\n","```python\n","model = LinearRegression()\n","```\n","\n","5. Define your loss function:\n","```python\n","criterion = nn.MSELoss()\n","```\n","\n","6. Define your optimizer with Adam:\n","```python\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","```\n","\n","7. Perform the training loop:\n","```python\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    # Forward pass\n","    outputs = model(x_train)\n","    loss = criterion(outputs, y_train)\n","\n","    # Backward and optimize\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print progress\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","```\n","\n","In this example, we iterate through a fixed number of epochs. In each epoch, we perform a forward pass to get the predicted outputs, calculate the loss between the predicted outputs and the true labels, perform backpropagation to compute the gradients, and update the model's parameters using the optimizer with Adam.\n","\n","Note: Make sure to convert your data to tensors and move them to the appropriate device (e.g., GPU) if available.\n","\n","</details>"],"metadata":{"id":"ayjdoZmnkAhS"}},{"cell_type":"code","source":[],"metadata":{"id":"VQUWU9hBUjL8"},"execution_count":null,"outputs":[]}]}